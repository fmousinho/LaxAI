# Cloud Run Job Dockerfile for GPU training worker
# Based on NVIDIA's PyTorch container with CUDA support
# Aligned with existing LaxAI Dockerfile patterns

ARG BASE_IMAGE=pytorch/pytorch:2.8.0-cuda12.8-cudnn9-runtime
ARG GOOGLE_CLOUD_PROJECT=""
ARG ENTRY_POINT="python src/cloud/worker.py"


FROM ${BASE_IMAGE}

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1
ENV GOOGLE_CLOUD_PROJECT=${GOOGLE_CLOUD_PROJECT}
ENV ENTRY_POINT=${ENTRY_POINT}
ENV TRAINING_JOBS_SUBSCRIPTION="training-jobs-sub"
ENV FIRESTORE_ENABLED="true"
ENV WORKER_TIMEOUT="3600"
ENV MAX_CONCURRENT_JOBS="1"

WORKDIR /app

# Copy requirements files and install directly in this image
ARG REQS=requirements-gpu.txt
COPY requirements/ requirements/

RUN apt-get update && apt-get install -y --no-install-recommends \
    python3-pip \
    git \
    wget \
    curl \
    unzip \
    libglib2.0-0 \
    libsm6 \
    libxrender1 \
    libxext6 \
    libgl1 \
    && rm -rf /var/lib/apt/lists/*

# Install project requirements directly (use the chosen requirements file)
RUN python -m pip install --upgrade pip \
 && python -m pip install --no-cache-dir "transformers[torch] @ git+https://github.com/huggingface/transformers.git@main" \
 && python -m pip install --no-cache-dir -r requirements/${REQS}

# Copy source code
COPY documentation /app/documentation
COPY config.toml /app/
COPY pyproject.toml .
COPY README.md .
# Copy source into /app/src so pip can find the package when building from /app
COPY src/ src/

RUN python -m pip install --no-cache-dir . --no-deps

RUN mkdir -p /tmp/models /tmp/artifacts

CMD ["sh", "-c", "exec $ENTRY_POINT"]