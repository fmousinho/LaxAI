# Cloud Run Job Dockerfile for GPU training worker
# Based on NVIDIA's PyTorch container with CUDA support
# Aligned with existing LaxAI Dockerfile patterns

ARG BASE_IMAGE=pytorch/pytorch:2.8.0-cuda12.8-cudnn9-runtime
ARG DEPS_IMAGE=fmousinho/laxai-deps:latest
ARG GOOGLE_CLOUD_PROJECT=""
ARG ENTRY_POINT="python src/cloud/worker.py"


# Use the same dependency approach as main Dockerfile
FROM ${DEPS_IMAGE} AS deps

# Main worker image with GPU support
FROM ${BASE_IMAGE}

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1
ENV GOOGLE_CLOUD_PROJECT=${GOOGLE_CLOUD_PROJECT}
ENV ENTRY_POINT=${ENTRY_POINT}
ENV TRAINING_JOBS_SUBSCRIPTION="training-jobs-sub"
ENV FIRESTORE_ENABLED="true"
ENV WORKER_TIMEOUT="3600"
ENV MAX_CONCURRENT_JOBS="1"


# Set working directory
WORKDIR /app

# Copy wheels from deps image (if available). Include the directory so
# downstream builds can rely on /wheels being present in the deps image.
# NOTE: some older deps images may not expose /wheels at runtime. Keep a
# defensive fallback during pip install so builds still work.
COPY --from=deps /wheels /wheels

# Copy requirements and install (prefer /wheels if present at runtime)
ARG REQS=requirements-gpu.txt
COPY requirements/ requirements/
RUN if [ -d /wheels ] && [ "$(ls -A /wheels)" ]; then \
            echo "Using local wheel cache at /wheels"; \
            pip install --no-cache-dir --find-links /wheels -r requirements/${REQS}; \
        else \
            echo "/wheels not present or empty, installing from PyPI"; \
            pip install --no-cache-dir -r requirements/${REQS}; \
        fi

# Copy source code
COPY documentation /app/documentation
COPY config.toml /app/
COPY src/ src/
COPY pyproject.toml .
COPY README.md .

# Install the package in development mode
RUN pip install -e .

# Create directory for model outputs
RUN mkdir -p /tmp/models /tmp/artifacts

# Set the entrypoint
CMD ["sh", "-c", "exec $ENTRY_POINT"]
