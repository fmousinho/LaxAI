{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdac947",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dirname() missing 1 required positional argument: 'p'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m current_dir = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m      6\u001b[39m project_root = os.path.dirname(os.path.dirname(current_dir))\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m project_root \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m sys.path:\n",
      "\u001b[31mTypeError\u001b[39m: dirname() missing 1 required positional argument: 'p'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "End-to-end workflow script for the LaxAI project.\n",
    "\n",
    "This script automates the following process:\n",
    "1. Finds all video files in a specified tenant's 'raw' directory in GCS.\n",
    "2. For each video, it runs the Data Preparation Pipeline.\n",
    "3. Upon successful data preparation, it identifies the generated training datasets.\n",
    "4. For each training dataset, it runs the Model Training Pipeline.\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "# --- Path Setup ---\n",
    "# Add the project root to the Python path to allow for absolute imports\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.insert(0, current_dir)\n",
    "\n",
    "from core.common.google_storage import get_storage\n",
    "from core.train.dataprep_pipeline import DataPrepPipeline\n",
    "from core.train.train_pipeline import TrainPipeline\n",
    "from config.all_config import detection_config, training_config\n",
    "\n",
    "# --- Configure Logging ---\n",
    "# Note: This script assumes logging is configured elsewhere (e.g., in config)\n",
    "# If not, uncomment the following lines for basic logging.\n",
    "# from config import logging_config\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def main(tenant_id: str, frames_per_video: int, verbose: bool, save_intermediate: bool):\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the data prep and training workflows.\n",
    "\n",
    "    Args:\n",
    "        tenant_id: The tenant ID for GCS operations.\n",
    "        frames_per_video: Number of frames to extract per video in the data prep pipeline.\n",
    "        verbose: Enable verbose logging for pipelines.\n",
    "        save_intermediate: Save intermediate pipeline results to GCS.\n",
    "    \"\"\"\n",
    "    logger.info(f\"--- Starting End-to-End Workflow for Tenant: {tenant_id} ---\")\n",
    "\n",
    "    # 1. Find all videos in the raw directory\n",
    "    try:\n",
    "        tenant_storage = get_storage(f\"{tenant_id}/user\")\n",
    "        raw_blobs = tenant_storage.list_blobs(prefix=\"raw/\")\n",
    "        video_files = [\n",
    "            blob.split('/')[-1] for blob in raw_blobs\n",
    "            if blob.lower().endswith(('.mp4', '.mov', '.avi')) and not blob.endswith('/')\n",
    "        ]\n",
    "        if not video_files:\n",
    "            logger.warning(\"No video files found in 'raw/' directory. Exiting.\")\n",
    "            return\n",
    "        logger.info(f\"Found {len(video_files)} videos to process: {video_files}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to list videos from GCS. Ensure credentials are correct. Error: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. Run DataPrepPipeline for each video\n",
    "    for video_file in video_files:\n",
    "        logger.info(f\"\\n--- Processing Video: {video_file} ---\")\n",
    "        dataprep_pipeline = DataPrepPipeline(\n",
    "            config=detection_config,\n",
    "            tenant_id=tenant_id,\n",
    "            verbose=verbose,\n",
    "            save_intermediate=save_intermediate\n",
    "        )\n",
    "        dataprep_results = dataprep_pipeline.run(video_path=video_file)\n",
    "\n",
    "        if dataprep_results.get(\"status\") != \"completed\":\n",
    "            logger.error(f\"Data prep pipeline failed for {video_file}. Skipping training.\")\n",
    "            logger.error(f\"Details: {json.dumps(dataprep_results.get('errors'), indent=2)}\")\n",
    "            continue\n",
    "\n",
    "        # 3. Run TrainPipeline for each resulting dataset\n",
    "        datasets_folder = dataprep_results.get(\"context\", {}).get(\"datasets_folder\")\n",
    "        if not datasets_folder:\n",
    "            logger.error(f\"Could not find 'datasets_folder' in dataprep results for {video_file}. Skipping training.\")\n",
    "            continue\n",
    "\n",
    "        logger.info(f\"Data prep successful. Starting training for datasets in: {datasets_folder}\")\n",
    "        train_pipeline = TrainPipeline(tenant_id=tenant_id, verbose=verbose, save_intermediate=save_intermediate)\n",
    "        # Assuming the train pipeline needs to run on the 'train' subdirectory of the datasets folder.\n",
    "        # This part may need adjustment based on the exact structure and requirements.\n",
    "        train_dataset_path = os.path.join(datasets_folder, \"train\") # This assumes a single train/val split.\n",
    "        train_results = train_pipeline.run(dataset_path=train_dataset_path)\n",
    "\n",
    "        if train_results.get(\"status\") == \"completed\":\n",
    "            logger.info(f\"Successfully completed training for dataset from video {video_file}.\")\n",
    "        else:\n",
    "            logger.error(f\"Training pipeline failed for dataset from video {video_file}.\")\n",
    "            logger.error(f\"Details: {json.dumps(train_results.get('errors'), indent=2)}\")\n",
    "\n",
    "    logger.info(\"--- End-to-End Workflow Finished ---\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Run the full LaxAI Data Prep and Training Workflow.\")\n",
    "    parser.add_argument(\"--tenant_id\", type=str, default=\"tenant1\", help=\"The tenant ID for GCS.\")\n",
    "    parser.add_argument(\"--frames\", type=int, default=20, help=\"Number of frames to extract per video.\")\n",
    "    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Enable verbose pipeline logging.\")\n",
    "    parser.add_argument(\"--save_intermediate\", action=\"store_true\", help=\"Save intermediate pipeline step results to GCS.\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # A basic logging config is needed if not configured globally\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    main(\n",
    "        tenant_id=args.tenant_id,\n",
    "        frames_per_video=args.frames,\n",
    "        verbose=args.verbose,\n",
    "        save_intermediate=args.save_intermediate\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
