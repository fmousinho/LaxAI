{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb319810",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9fe7448",
   "metadata": {},
   "source": [
    "# Building NN to Identify Individual Players #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069e7c85",
   "metadata": {},
   "source": [
    "## Motivation ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7696e8",
   "metadata": {},
   "source": [
    "Player performance analysis requires a strong tracking mechanism. Position-based trackers such as ByteTrack can mistake identities when multiple detections overlap. Pre-trained identification algorithms like SigLip also perform very poorly, in part due to the very low resultion of the detection crops (which can be as small as 10 x 30 pixels).\n",
    "\n",
    "We will attempt at training a model specific for our purposes, leveraging  tracklet-based self-supervision to create a triplet ([A]nchor, [P]ositive and [N]egative) data set to be fed into a siemese NN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71601c54",
   "metadata": {},
   "source": [
    "## Common Elements ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb1e33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0708 20:38:07.787000 94918 .venv312/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Set this to the absolute path of your project root\n",
    "project_root = \"/Users/fernandomousinho/Documents/Learning_to_Code/LaxAI\"\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "import torch\n",
    "import supervision as sv\n",
    "from tqdm import tqdm\n",
    "from collections import deque, defaultdict\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from PIL import Image\n",
    "import umap\n",
    "import json\n",
    "\n",
    "from modules.detection import DetectionModel\n",
    "from tools.store_driver import Store\n",
    "from modules.custom_tracker import AffineAwareByteTrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34f8f382",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video = \"/Users/fernandomousinho/Library/CloudStorage/GoogleDrive-fmousinho76@gmail.com/My Drive/Colab_Notebooks/FCA_Upstate_NY_003.mp4\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "store = Store()\n",
    "debug_max_frames = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb51187",
   "metadata": {},
   "source": [
    "## Curate Training Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "459f1294",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_JSON_FILE_PATH = \"detections.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789d7c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_info = sv.VideoInfo.from_video_path(video_path=input_video)\n",
    "generator_params = {\n",
    "    \"source_path\": input_video,\n",
    "    \"end\": debug_max_frames if debug_max_frames else video_info.total_frames,\n",
    "}\n",
    "frames_generator = sv.get_video_frames_generator(**generator_params)\n",
    "model = DetectionModel(store=store, device=device)\n",
    "\n",
    "tracker = AffineAwareByteTrack(id_type='external', maintain_separate_track_obj=False)\n",
    "\n",
    "frame_target = debug_max_frames if debug_max_frames else video_info.total_frames\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(stride=1, **generator_params)\n",
    "\n",
    "json_sink = sv.JSONSink(RESULT_JSON_FILE_PATH)\n",
    "\n",
    "previous_frame: Optional[np.ndarray] = None\n",
    "frame_id = 0\n",
    "\n",
    "with json_sink as sink:\n",
    "    for frame in tqdm(frame_generator, desc=\"Processing frames\", total=frame_target):\n",
    "        all_detections = model.generate_detections(frame)\n",
    "        all_detections = all_detections.with_nms(threshold=0.4, class_agnostic=False)\n",
    "\n",
    "        if previous_frame is not None:\n",
    "            affine_matrix = tracker.calculate_affine_transform(previous_frame, frame)\n",
    "        else:\n",
    "            affine_matrix = tracker.get_identity_affine_matrix()\n",
    "        previous_frame = frame.copy()\n",
    "\n",
    "        all_detections = tracker.update_with_transform(\n",
    "            detections=all_detections,\n",
    "            frame=frame,\n",
    "            affine_matrix=affine_matrix\n",
    "        )\n",
    "        sink.append(all_detections, custom_data={\"frame_id\": frame_id})\n",
    "        frame_id += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12edbd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_detections(json_file: str) -> List[sv.Detections]:\n",
    "    rows_by_frame_number = defaultdict(list)\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    for row in data:\n",
    "        frame_number = int(row[\"frame_id\"])\n",
    "        rows_by_frame_number[frame_number].append(row)\n",
    "\n",
    "    detections_list = []\n",
    "    for frame_number, rows in rows_by_frame_number.items():\n",
    "        xyxy = []\n",
    "        class_id = []\n",
    "        confidence = []\n",
    "        tracker_id = []\n",
    "        custom_data = defaultdict(list)\n",
    "\n",
    "        for row in rows:\n",
    "            xyxy.append([row[key] for key in [\"x_min\", \"y_min\", \"x_max\", \"y_max\"]])\n",
    "            class_id.append(row[\"class_id\"])\n",
    "            confidence.append(row[\"confidence\"])\n",
    "            tracker_id.append(row[\"tracker_id\"])\n",
    "\n",
    "            for custom_key in row.keys():\n",
    "                if custom_key in [\"x_min\", \"y_min\", \"x_max\", \"y_max\", \"class_id\", \"confidence\", \"tracker_id\"]:\n",
    "                    continue\n",
    "                custom_data[custom_key].append(row[custom_key])\n",
    "\n",
    "        if all([val == \"\" for val in class_id]):\n",
    "            class_id = None\n",
    "        if all([val == \"\" for val in confidence]):\n",
    "            confidence = None\n",
    "        if all([val == \"\" for val in tracker_id]):\n",
    "            tracker_id = None\n",
    "\n",
    "        detections_list.append(\n",
    "            sv.Detections(\n",
    "                xyxy=np.array(xyxy, dtype=np.float32),\n",
    "                class_id=np.array(class_id, dtype=int),\n",
    "                confidence=np.array(confidence, dtype=np.float32),\n",
    "                tracker_id=np.array(tracker_id, dtype=int),\n",
    "                data=dict(custom_data)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    return detections_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f09b561",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_detections = json_to_detections(RESULT_JSON_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60e1545a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11, 4), (6,), (6,), (6,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_detections[100].xyxy.shape, all_detections[0].class_id.shape, all_detections[0].confidence.shape, all_detections[0].tracker_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4339c3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import random\n",
    "import os\n",
    "\n",
    "random.seed(42)  # For reproducibility\n",
    "\n",
    "src_data_dir = \"data\"\n",
    "train_dir = \"data/train\"\n",
    "val_dir = \"data/val\"\n",
    "\n",
    "# Create train and val directories\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# For each tracker_id directory in data/\n",
    "for track_id in os.listdir(src_data_dir):\n",
    "    track_path = os.path.join(src_data_dir, track_id)\n",
    "    if not os.path.isdir(track_path):\n",
    "        continue\n",
    "\n",
    "    # List all crop files for this track\n",
    "    crop_files = [f for f in os.listdir(track_path) if f.endswith('.jpg')]\n",
    "    random.shuffle(crop_files)\n",
    "\n",
    "    split_idx = int(0.8 * len(crop_files))\n",
    "    train_files = crop_files[:split_idx]\n",
    "    val_files = crop_files[split_idx:]\n",
    "\n",
    "    # Create per-track folders in train/ and val/\n",
    "    train_track_dir = os.path.join(train_dir, track_id)\n",
    "    val_track_dir = os.path.join(val_dir, track_id)\n",
    "    os.makedirs(train_track_dir, exist_ok=True)\n",
    "    os.makedirs(val_track_dir, exist_ok=True)\n",
    "\n",
    "    # Copy files\n",
    "    for fname in train_files:\n",
    "        src = os.path.join(track_path, fname)\n",
    "        dst = os.path.join(train_track_dir, fname)\n",
    "        shutil.copy2(src, dst)\n",
    "\n",
    "    for fname in val_files:\n",
    "        src = os.path.join(track_path, fname)\n",
    "        dst = os.path.join(val_track_dir, fname)\n",
    "        shutil.copy2(src, dst)\n",
    "\n",
    "print(f\"Done! Crops split into '{train_dir}' and '{val_dir}' with per-track structure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170e8053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from collections import deque\n",
    "\n",
    "# Create the main data directory\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Build a mapping from track_id to list of (frame_id, bbox, confidence)\n",
    "track_detections = {}\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(stride=1, source_path=input_video)\n",
    "frame_idx = 0\n",
    "next_detected_frame = all_detections[0].data[\"frame_id\"][0] if all_detections else 0\n",
    "all_detections_dq = deque(all_detections)\n",
    "\n",
    "for frame in tqdm(frame_generator, desc=\"Processing frames for crop extraction\"):\n",
    "    if frame_idx != next_detected_frame:\n",
    "        frame_idx += 1\n",
    "        continue\n",
    "    \n",
    "    detections = all_detections_dq.popleft()\n",
    "    \n",
    "    # Extract crops for each detection in this frame\n",
    "    for i in range(len(detections.xyxy)):\n",
    "        frame_id = detections.data[\"frame_id\"][i]\n",
    "        bbox = detections.xyxy[i]  # [x1, y1, x2, y2]\n",
    "        tracker_id = detections.tracker_id[i]\n",
    "        confidence = detections.confidence[i]\n",
    "        \n",
    "        if tracker_id is None:\n",
    "            continue\n",
    "            \n",
    "        # Create folder for this tracker_id if it doesn't exist\n",
    "        track_folder = os.path.join(\"data\", str(tracker_id))\n",
    "        os.makedirs(track_folder, exist_ok=True)\n",
    "        \n",
    "        # Extract crop from frame\n",
    "        x1, y1, x2, y2 = map(int, bbox)\n",
    "        crop = frame[y1:y2, x1:x2]\n",
    "        \n",
    "        # Save crop with filename: frame_id_confidence.jpg\n",
    "        crop_filename = f\"{frame_id}_{confidence:.3f}.jpg\"\n",
    "        crop_path = os.path.join(track_folder, crop_filename)\n",
    "        cv2.imwrite(crop_path, crop)\n",
    "    \n",
    "    # Update for next frame\n",
    "    if len(all_detections_dq) > 0:\n",
    "        next_detected_frame = all_detections_dq[0].data[\"frame_id\"][0]\n",
    "    else:\n",
    "        break\n",
    "    frame_idx += 1\n",
    "\n",
    "print(f\"Crop extraction complete! Check the 'data' directory for organized crops.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f28bbbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class LacrossePlayerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for loading lacrosse player crops for triplet loss.\n",
    "    Each player's crops are expected to be in a separate folder.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_dir, transform=None, min_images_per_player=3):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.min_images_per_player = min_images_per_player\n",
    "        \n",
    "        # Get all player directories\n",
    "        all_players = [d for d in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, d))]\n",
    "        \n",
    "        # Filter players with sufficient images\n",
    "        self.players = []\n",
    "        self.player_to_images = {}\n",
    "        \n",
    "        for player in all_players:\n",
    "            player_images = [os.path.join(image_dir, player, img) \n",
    "                           for img in os.listdir(os.path.join(image_dir, player))\n",
    "                           if img.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "            \n",
    "            # Only include players with enough images for triplet sampling\n",
    "            if len(player_images) >= self.min_images_per_player:\n",
    "                self.players.append(player)\n",
    "                self.player_to_images[player] = player_images\n",
    "        \n",
    "        if len(self.players) < 2:\n",
    "            raise ValueError(f\"Need at least 2 players with {min_images_per_player}+ images each. Found {len(self.players)} valid players.\")\n",
    "        \n",
    "        # Create list of all valid images\n",
    "        self.all_images = []\n",
    "        for player in self.players:\n",
    "            self.all_images.extend(self.player_to_images[player])\n",
    "        \n",
    "        self.player_indices = {player: i for i, player in enumerate(self.players)}\n",
    "        \n",
    "        print(f\"Dataset initialized with {len(self.players)} players and {len(self.all_images)} total images\")\n",
    "        for player in self.players:\n",
    "            print(f\"  Player {player}: {len(self.player_to_images[player])} images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Anchor image\n",
    "        anchor_path = self.all_images[index]\n",
    "        anchor_player = os.path.basename(os.path.dirname(anchor_path))\n",
    "        anchor_label = self.player_indices[anchor_player]\n",
    "        \n",
    "        try:\n",
    "            anchor_img = Image.open(anchor_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading anchor image {anchor_path}: {e}\")\n",
    "            # Return the first valid image as fallback\n",
    "            anchor_path = self.all_images[0]\n",
    "            anchor_player = os.path.basename(os.path.dirname(anchor_path))\n",
    "            anchor_label = self.player_indices[anchor_player]\n",
    "            anchor_img = Image.open(anchor_path).convert('RGB')\n",
    "\n",
    "        # Select a positive image (different image of the same player)\n",
    "        positive_list = self.player_to_images[anchor_player]\n",
    "        if len(positive_list) < 2:\n",
    "            # If only one image, use the same image (shouldn't happen due to filtering)\n",
    "            positive_path = anchor_path\n",
    "        else:\n",
    "            # Ensure positive is different from anchor\n",
    "            positive_candidates = [p for p in positive_list if p != anchor_path]\n",
    "            if positive_candidates:\n",
    "                positive_path = random.choice(positive_candidates)\n",
    "            else:\n",
    "                positive_path = random.choice(positive_list)  # Fallback\n",
    "        \n",
    "        try:\n",
    "            positive_img = Image.open(positive_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading positive image {positive_path}: {e}\")\n",
    "            positive_img = anchor_img  # Use anchor as fallback\n",
    "        \n",
    "        # Select a negative image (image of a different player)\n",
    "        negative_candidates = [p for p in self.players if p != anchor_player]\n",
    "        if not negative_candidates:\n",
    "            # This shouldn't happen if we have at least 2 players\n",
    "            negative_player = anchor_player\n",
    "        else:\n",
    "            negative_player = random.choice(negative_candidates)\n",
    "        \n",
    "        negative_path = random.choice(self.player_to_images[negative_player])\n",
    "        \n",
    "        try:\n",
    "            negative_img = Image.open(negative_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading negative image {negative_path}: {e}\")\n",
    "            negative_img = anchor_img  # Use anchor as fallback\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            try:\n",
    "                anchor_img = self.transform(anchor_img)\n",
    "                positive_img = self.transform(positive_img)\n",
    "                negative_img = self.transform(negative_img)\n",
    "            except Exception as e:\n",
    "                print(f\"Error applying transforms: {e}\")\n",
    "                # Return tensors without transforms as fallback\n",
    "                anchor_img = transforms.ToTensor()(anchor_img)\n",
    "                positive_img = transforms.ToTensor()(positive_img)\n",
    "                negative_img = transforms.ToTensor()(negative_img)\n",
    "\n",
    "        return anchor_img, positive_img, negative_img, torch.tensor(anchor_label)\n",
    "\n",
    "# Define data augmentations\n",
    "# Note: The input size (80, 40) matches your model expectations\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((80, 40)), # Height, Width\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c34319a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class SiameseNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A Siamese network that uses a pre-trained ResNet as a backbone\n",
    "    to generate feature embeddings for player crops.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim=128):\n",
    "        super(SiameseNet, self).__init__()\n",
    "        # Use a pre-trained ResNet, but remove its final classification layer\n",
    "        self.backbone = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        \n",
    "        # Modify the first conv layer to be more suitable for small images if needed\n",
    "        # For example, smaller kernel and stride\n",
    "        self.backbone.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        \n",
    "        # Get the number of features from the backbone's output\n",
    "        num_ftrs = self.backbone.fc.in_features\n",
    "        \n",
    "        # Replace the final layer with our embedding layer\n",
    "        self.backbone.fc = nn.Linear(num_ftrs, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The forward pass returns the embedding vector\n",
    "        embedding = self.backbone(x)\n",
    "        # L2-normalize the embedding\n",
    "        embedding = nn.functional.normalize(embedding, p=2, dim=1)\n",
    "        return embedding\n",
    "\n",
    "    def forward_triplet(self, anchor, positive, negative):\n",
    "        # Helper function to compute embeddings for a triplet\n",
    "        emb_anchor = self.forward(anchor)\n",
    "        emb_positive = self.forward(positive)\n",
    "        emb_negative = self.forward(negative)\n",
    "        return emb_anchor, emb_positive, emb_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d60cd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 61 player folders in data/train\n",
      "Dataset initialized with 59 players and 10072 total images\n",
      "  Player 59: 4 images\n",
      "  Player 50: 108 images\n",
      "  Player 57: 48 images\n",
      "  Player 32: 66 images\n",
      "  Player 35: 323 images\n",
      "  Player 56: 55 images\n",
      "  Player 51: 54 images\n",
      "  Player 58: 50 images\n",
      "  Player 34: 302 images\n",
      "  Player 33: 240 images\n",
      "  Player 20: 590 images\n",
      "  Player 18: 122 images\n",
      "  Player 27: 225 images\n",
      "  Player 9: 20 images\n",
      "  Player 11: 25 images\n",
      "  Player 7: 180 images\n",
      "  Player 29: 66 images\n",
      "  Player 16: 103 images\n",
      "  Player 42: 44 images\n",
      "  Player 45: 71 images\n",
      "  Player 6: 578 images\n",
      "  Player 28: 58 images\n",
      "  Player 17: 29 images\n",
      "  Player 1: 232 images\n",
      "  Player 10: 185 images\n",
      "  Player 19: 81 images\n",
      "  Player 26: 278 images\n",
      "  Player 8: 156 images\n",
      "  Player 21: 48 images\n",
      "  Player 44: 85 images\n",
      "  Player 43: 35 images\n",
      "  Player 38: 125 images\n",
      "  Player 36: 110 images\n",
      "  Player 31: 325 images\n",
      "  Player 54: 102 images\n",
      "  Player 53: 139 images\n",
      "  Player 30: 329 images\n",
      "  Player 37: 112 images\n",
      "  Player 39: 16 images\n",
      "  Player 52: 142 images\n",
      "  Player 55: 39 images\n",
      "  Player 46: 144 images\n",
      "  Player 41: 74 images\n",
      "  Player 48: 131 images\n",
      "  Player 24: 740 images\n",
      "  Player 23: 9 images\n",
      "  Player 4: 136 images\n",
      "  Player 15: 581 images\n",
      "  Player 3: 238 images\n",
      "  Player 12: 176 images\n",
      "  Player 49: 209 images\n",
      "  Player 40: 34 images\n",
      "  Player 47: 221 images\n",
      "  Player 2: 132 images\n",
      "  Player 13: 952 images\n",
      "  Player 5: 8 images\n",
      "  Player 14: 98 images\n",
      "  Player 22: 8 images\n",
      "  Player 25: 281 images\n",
      "Dataset size: 10072 images\n",
      "Number of batches: 630\n",
      "Epoch [1/10], Step [10/630], Loss: 0.2390\n",
      "Epoch [1/10], Step [10/630], Loss: 0.2390\n",
      "Epoch [1/10], Step [20/630], Loss: 0.3060\n",
      "Epoch [1/10], Step [20/630], Loss: 0.3060\n",
      "Epoch [1/10], Step [30/630], Loss: 0.1508\n",
      "Epoch [1/10], Step [30/630], Loss: 0.1508\n",
      "Epoch [1/10], Step [40/630], Loss: 0.4574\n",
      "Epoch [1/10], Step [40/630], Loss: 0.4574\n",
      "Epoch [1/10], Step [50/630], Loss: 0.4172\n",
      "Epoch [1/10], Step [50/630], Loss: 0.4172\n",
      "Epoch [1/10], Step [60/630], Loss: 0.1530\n",
      "Epoch [1/10], Step [60/630], Loss: 0.1530\n",
      "Epoch [1/10], Step [70/630], Loss: 0.1054\n",
      "Epoch [1/10], Step [70/630], Loss: 0.1054\n",
      "Epoch [1/10], Step [80/630], Loss: 0.3056\n",
      "Epoch [1/10], Step [80/630], Loss: 0.3056\n",
      "Epoch [1/10], Step [90/630], Loss: 0.2727\n",
      "Epoch [1/10], Step [90/630], Loss: 0.2727\n",
      "Epoch [1/10], Step [100/630], Loss: 0.3311\n",
      "Epoch [1/10], Step [100/630], Loss: 0.3311\n",
      "Epoch [1/10], Step [110/630], Loss: 0.3382\n",
      "Epoch [1/10], Step [110/630], Loss: 0.3382\n",
      "Epoch [1/10], Step [120/630], Loss: 0.3870\n",
      "Epoch [1/10], Step [120/630], Loss: 0.3870\n",
      "Epoch [1/10], Step [130/630], Loss: 0.1959\n",
      "Epoch [1/10], Step [130/630], Loss: 0.1959\n",
      "Epoch [1/10], Step [140/630], Loss: 0.4820\n",
      "Epoch [1/10], Step [140/630], Loss: 0.4820\n",
      "Epoch [1/10], Step [150/630], Loss: 0.2016\n",
      "Epoch [1/10], Step [150/630], Loss: 0.2016\n",
      "Epoch [1/10], Step [160/630], Loss: 0.2242\n",
      "Epoch [1/10], Step [160/630], Loss: 0.2242\n",
      "Epoch [1/10], Step [170/630], Loss: 0.2527\n",
      "Epoch [1/10], Step [170/630], Loss: 0.2527\n",
      "Epoch [1/10], Step [180/630], Loss: 0.1007\n",
      "Epoch [1/10], Step [180/630], Loss: 0.1007\n",
      "Epoch [1/10], Step [190/630], Loss: 0.1600\n",
      "Epoch [1/10], Step [190/630], Loss: 0.1600\n",
      "Epoch [1/10], Step [200/630], Loss: 0.1184\n",
      "Epoch [1/10], Step [200/630], Loss: 0.1184\n",
      "Epoch [1/10], Step [210/630], Loss: 0.2666\n",
      "Epoch [1/10], Step [210/630], Loss: 0.2666\n",
      "Epoch [1/10], Step [220/630], Loss: 0.1679\n",
      "Epoch [1/10], Step [220/630], Loss: 0.1679\n",
      "Epoch [1/10], Step [230/630], Loss: 0.2760\n",
      "Epoch [1/10], Step [230/630], Loss: 0.2760\n",
      "Epoch [1/10], Step [240/630], Loss: 0.1208\n",
      "Epoch [1/10], Step [240/630], Loss: 0.1208\n",
      "Epoch [1/10], Step [250/630], Loss: 0.3847\n",
      "Epoch [1/10], Step [250/630], Loss: 0.3847\n",
      "Epoch [1/10], Step [260/630], Loss: 0.2143\n",
      "Epoch [1/10], Step [260/630], Loss: 0.2143\n",
      "Epoch [1/10], Step [270/630], Loss: 0.2295\n",
      "Epoch [1/10], Step [270/630], Loss: 0.2295\n",
      "Epoch [1/10], Step [280/630], Loss: 0.0242\n",
      "Epoch [1/10], Step [280/630], Loss: 0.0242\n",
      "Epoch [1/10], Step [290/630], Loss: 0.0660\n",
      "Epoch [1/10], Step [290/630], Loss: 0.0660\n",
      "Epoch [1/10], Step [300/630], Loss: 0.2436\n",
      "Epoch [1/10], Step [300/630], Loss: 0.2436\n",
      "Epoch [1/10], Step [310/630], Loss: 0.1458\n",
      "Epoch [1/10], Step [310/630], Loss: 0.1458\n",
      "Epoch [1/10], Step [320/630], Loss: 0.0943\n",
      "Epoch [1/10], Step [320/630], Loss: 0.0943\n",
      "Epoch [1/10], Step [330/630], Loss: 0.0905\n",
      "Epoch [1/10], Step [330/630], Loss: 0.0905\n",
      "Epoch [1/10], Step [340/630], Loss: 0.2857\n",
      "Epoch [1/10], Step [340/630], Loss: 0.2857\n",
      "Epoch [1/10], Step [350/630], Loss: 0.0950\n",
      "Epoch [1/10], Step [350/630], Loss: 0.0950\n",
      "Epoch [1/10], Step [360/630], Loss: 0.1478\n",
      "Epoch [1/10], Step [360/630], Loss: 0.1478\n",
      "Epoch [1/10], Step [370/630], Loss: 0.2297\n",
      "Epoch [1/10], Step [370/630], Loss: 0.2297\n",
      "Epoch [1/10], Step [380/630], Loss: 0.2407\n",
      "Epoch [1/10], Step [380/630], Loss: 0.2407\n",
      "Epoch [1/10], Step [390/630], Loss: 0.2280\n",
      "Epoch [1/10], Step [390/630], Loss: 0.2280\n",
      "Epoch [1/10], Step [400/630], Loss: 0.1149\n",
      "Epoch [1/10], Step [400/630], Loss: 0.1149\n",
      "Epoch [1/10], Step [410/630], Loss: 0.2201\n",
      "Epoch [1/10], Step [410/630], Loss: 0.2201\n",
      "Epoch [1/10], Step [420/630], Loss: 0.1201\n",
      "Epoch [1/10], Step [420/630], Loss: 0.1201\n",
      "Epoch [1/10], Step [430/630], Loss: 0.0841\n",
      "Epoch [1/10], Step [430/630], Loss: 0.0841\n",
      "Epoch [1/10], Step [440/630], Loss: 0.1612\n",
      "Epoch [1/10], Step [440/630], Loss: 0.1612\n",
      "Epoch [1/10], Step [450/630], Loss: 0.0505\n",
      "Epoch [1/10], Step [450/630], Loss: 0.0505\n",
      "Epoch [1/10], Step [460/630], Loss: 0.1626\n",
      "Epoch [1/10], Step [460/630], Loss: 0.1626\n",
      "Epoch [1/10], Step [470/630], Loss: 0.1699\n",
      "Epoch [1/10], Step [470/630], Loss: 0.1699\n",
      "Epoch [1/10], Step [480/630], Loss: 0.1604\n",
      "Epoch [1/10], Step [480/630], Loss: 0.1604\n",
      "Epoch [1/10], Step [490/630], Loss: 0.4060\n",
      "Epoch [1/10], Step [490/630], Loss: 0.4060\n",
      "Epoch [1/10], Step [500/630], Loss: 0.0980\n",
      "Epoch [1/10], Step [500/630], Loss: 0.0980\n",
      "Epoch [1/10], Step [510/630], Loss: 0.1936\n",
      "Epoch [1/10], Step [510/630], Loss: 0.1936\n",
      "Epoch [1/10], Step [520/630], Loss: 0.2128\n",
      "Epoch [1/10], Step [520/630], Loss: 0.2128\n",
      "Epoch [1/10], Step [530/630], Loss: 0.2025\n",
      "Epoch [1/10], Step [530/630], Loss: 0.2025\n",
      "Epoch [1/10], Step [540/630], Loss: 0.1176\n",
      "Epoch [1/10], Step [540/630], Loss: 0.1176\n",
      "Epoch [1/10], Step [550/630], Loss: 0.0916\n",
      "Epoch [1/10], Step [550/630], Loss: 0.0916\n",
      "Epoch [1/10], Step [560/630], Loss: 0.2385\n",
      "Epoch [1/10], Step [560/630], Loss: 0.2385\n",
      "Epoch [1/10], Step [570/630], Loss: 0.0930\n",
      "Epoch [1/10], Step [570/630], Loss: 0.0930\n",
      "Epoch [1/10], Step [580/630], Loss: 0.2670\n",
      "Epoch [1/10], Step [580/630], Loss: 0.2670\n",
      "Epoch [1/10], Step [590/630], Loss: 0.1441\n",
      "Epoch [1/10], Step [590/630], Loss: 0.1441\n",
      "Epoch [1/10], Step [600/630], Loss: 0.0596\n",
      "Epoch [1/10], Step [600/630], Loss: 0.0596\n",
      "Epoch [1/10], Step [610/630], Loss: 0.1610\n",
      "Epoch [1/10], Step [610/630], Loss: 0.1610\n",
      "Epoch [1/10], Step [620/630], Loss: 0.1531\n",
      "Epoch [1/10], Step [620/630], Loss: 0.1531\n",
      "Epoch [1/10], Step [630/630], Loss: 0.1954\n",
      "--- Epoch 1 Summary ---\n",
      "Average Loss: 0.2178\n",
      "\n",
      "Epoch [1/10], Step [630/630], Loss: 0.1954\n",
      "--- Epoch 1 Summary ---\n",
      "Average Loss: 0.2178\n",
      "\n",
      "Epoch [2/10], Step [10/630], Loss: 0.2465\n",
      "Epoch [2/10], Step [10/630], Loss: 0.2465\n",
      "Epoch [2/10], Step [20/630], Loss: 0.1118\n",
      "Epoch [2/10], Step [20/630], Loss: 0.1118\n",
      "Epoch [2/10], Step [30/630], Loss: 0.1688\n",
      "Epoch [2/10], Step [30/630], Loss: 0.1688\n",
      "Epoch [2/10], Step [40/630], Loss: 0.0743\n",
      "Epoch [2/10], Step [40/630], Loss: 0.0743\n",
      "Epoch [2/10], Step [50/630], Loss: 0.0592\n",
      "Epoch [2/10], Step [50/630], Loss: 0.0592\n",
      "Epoch [2/10], Step [60/630], Loss: 0.1785\n",
      "Epoch [2/10], Step [60/630], Loss: 0.1785\n",
      "Epoch [2/10], Step [70/630], Loss: 0.0971\n",
      "Epoch [2/10], Step [70/630], Loss: 0.0971\n",
      "Epoch [2/10], Step [80/630], Loss: 0.1987\n",
      "Epoch [2/10], Step [80/630], Loss: 0.1987\n",
      "Epoch [2/10], Step [90/630], Loss: 0.1745\n",
      "Epoch [2/10], Step [90/630], Loss: 0.1745\n",
      "Epoch [2/10], Step [100/630], Loss: 0.0655\n",
      "Epoch [2/10], Step [100/630], Loss: 0.0655\n",
      "Epoch [2/10], Step [110/630], Loss: 0.0327\n",
      "Epoch [2/10], Step [110/630], Loss: 0.0327\n",
      "Epoch [2/10], Step [120/630], Loss: 0.0814\n",
      "Epoch [2/10], Step [120/630], Loss: 0.0814\n",
      "Epoch [2/10], Step [130/630], Loss: 0.1897\n",
      "Epoch [2/10], Step [130/630], Loss: 0.1897\n",
      "Epoch [2/10], Step [140/630], Loss: 0.0674\n",
      "Epoch [2/10], Step [140/630], Loss: 0.0674\n",
      "Epoch [2/10], Step [150/630], Loss: 0.1969\n",
      "Epoch [2/10], Step [150/630], Loss: 0.1969\n",
      "Epoch [2/10], Step [160/630], Loss: 0.0003\n",
      "Epoch [2/10], Step [160/630], Loss: 0.0003\n",
      "Epoch [2/10], Step [170/630], Loss: 0.1683\n",
      "Epoch [2/10], Step [170/630], Loss: 0.1683\n",
      "Epoch [2/10], Step [180/630], Loss: 0.1816\n",
      "Epoch [2/10], Step [180/630], Loss: 0.1816\n",
      "Epoch [2/10], Step [190/630], Loss: 0.0871\n",
      "Epoch [2/10], Step [190/630], Loss: 0.0871\n",
      "Epoch [2/10], Step [200/630], Loss: 0.1438\n",
      "Epoch [2/10], Step [200/630], Loss: 0.1438\n",
      "Epoch [2/10], Step [210/630], Loss: 0.0282\n",
      "Epoch [2/10], Step [210/630], Loss: 0.0282\n",
      "Epoch [2/10], Step [220/630], Loss: 0.2143\n",
      "Epoch [2/10], Step [220/630], Loss: 0.2143\n",
      "Epoch [2/10], Step [230/630], Loss: 0.0908\n",
      "Epoch [2/10], Step [230/630], Loss: 0.0908\n",
      "Epoch [2/10], Step [240/630], Loss: 0.1565\n",
      "Epoch [2/10], Step [240/630], Loss: 0.1565\n",
      "Epoch [2/10], Step [250/630], Loss: 0.1269\n",
      "Epoch [2/10], Step [250/630], Loss: 0.1269\n",
      "Epoch [2/10], Step [260/630], Loss: 0.1459\n",
      "Epoch [2/10], Step [260/630], Loss: 0.1459\n",
      "Epoch [2/10], Step [270/630], Loss: 0.0250\n",
      "Epoch [2/10], Step [270/630], Loss: 0.0250\n",
      "Epoch [2/10], Step [280/630], Loss: 0.0629\n",
      "Epoch [2/10], Step [280/630], Loss: 0.0629\n",
      "Epoch [2/10], Step [290/630], Loss: 0.1129\n",
      "Epoch [2/10], Step [290/630], Loss: 0.1129\n",
      "Epoch [2/10], Step [300/630], Loss: 0.0910\n",
      "Epoch [2/10], Step [300/630], Loss: 0.0910\n",
      "Epoch [2/10], Step [310/630], Loss: 0.1367\n",
      "Epoch [2/10], Step [310/630], Loss: 0.1367\n",
      "Epoch [2/10], Step [320/630], Loss: 0.0000\n",
      "Epoch [2/10], Step [320/630], Loss: 0.0000\n",
      "Epoch [2/10], Step [330/630], Loss: 0.0470\n",
      "Epoch [2/10], Step [330/630], Loss: 0.0470\n",
      "Epoch [2/10], Step [340/630], Loss: 0.1250\n",
      "Epoch [2/10], Step [340/630], Loss: 0.1250\n",
      "Epoch [2/10], Step [350/630], Loss: 0.0595\n",
      "Epoch [2/10], Step [350/630], Loss: 0.0595\n",
      "Epoch [2/10], Step [360/630], Loss: 0.0525\n",
      "Epoch [2/10], Step [360/630], Loss: 0.0525\n",
      "Epoch [2/10], Step [370/630], Loss: 0.0490\n",
      "Epoch [2/10], Step [370/630], Loss: 0.0490\n",
      "Epoch [2/10], Step [380/630], Loss: 0.0955\n",
      "Epoch [2/10], Step [380/630], Loss: 0.0955\n",
      "Epoch [2/10], Step [390/630], Loss: 0.0603\n",
      "Epoch [2/10], Step [390/630], Loss: 0.0603\n",
      "Epoch [2/10], Step [400/630], Loss: 0.2135\n",
      "Epoch [2/10], Step [400/630], Loss: 0.2135\n",
      "Epoch [2/10], Step [410/630], Loss: 0.1860\n",
      "Epoch [2/10], Step [410/630], Loss: 0.1860\n",
      "Epoch [2/10], Step [420/630], Loss: 0.2069\n",
      "Epoch [2/10], Step [420/630], Loss: 0.2069\n",
      "Epoch [2/10], Step [430/630], Loss: 0.0203\n",
      "Epoch [2/10], Step [430/630], Loss: 0.0203\n",
      "Epoch [2/10], Step [440/630], Loss: 0.0811\n",
      "Epoch [2/10], Step [440/630], Loss: 0.0811\n",
      "Epoch [2/10], Step [450/630], Loss: 0.2026\n",
      "Epoch [2/10], Step [450/630], Loss: 0.2026\n",
      "Epoch [2/10], Step [460/630], Loss: 0.0786\n",
      "Epoch [2/10], Step [460/630], Loss: 0.0786\n",
      "Epoch [2/10], Step [470/630], Loss: 0.1575\n",
      "Epoch [2/10], Step [470/630], Loss: 0.1575\n",
      "Epoch [2/10], Step [480/630], Loss: 0.0452\n",
      "Epoch [2/10], Step [480/630], Loss: 0.0452\n",
      "Epoch [2/10], Step [490/630], Loss: 0.2198\n",
      "Epoch [2/10], Step [490/630], Loss: 0.2198\n",
      "Epoch [2/10], Step [500/630], Loss: 0.0993\n",
      "Epoch [2/10], Step [500/630], Loss: 0.0993\n",
      "Epoch [2/10], Step [510/630], Loss: 0.1192\n",
      "Epoch [2/10], Step [510/630], Loss: 0.1192\n",
      "Epoch [2/10], Step [520/630], Loss: 0.1471\n",
      "Epoch [2/10], Step [520/630], Loss: 0.1471\n",
      "Epoch [2/10], Step [530/630], Loss: 0.0202\n",
      "Epoch [2/10], Step [530/630], Loss: 0.0202\n",
      "Epoch [2/10], Step [540/630], Loss: 0.0398\n",
      "Epoch [2/10], Step [540/630], Loss: 0.0398\n",
      "Epoch [2/10], Step [550/630], Loss: 0.1448\n",
      "Epoch [2/10], Step [550/630], Loss: 0.1448\n",
      "Epoch [2/10], Step [560/630], Loss: 0.0870\n",
      "Epoch [2/10], Step [560/630], Loss: 0.0870\n",
      "Epoch [2/10], Step [570/630], Loss: 0.0087\n",
      "Epoch [2/10], Step [570/630], Loss: 0.0087\n",
      "Epoch [2/10], Step [580/630], Loss: 0.0501\n",
      "Epoch [2/10], Step [580/630], Loss: 0.0501\n",
      "Epoch [2/10], Step [590/630], Loss: 0.1233\n",
      "Epoch [2/10], Step [590/630], Loss: 0.1233\n",
      "Epoch [2/10], Step [600/630], Loss: 0.1280\n",
      "Epoch [2/10], Step [600/630], Loss: 0.1280\n",
      "Epoch [2/10], Step [610/630], Loss: 0.0722\n",
      "Epoch [2/10], Step [610/630], Loss: 0.0722\n",
      "Epoch [2/10], Step [620/630], Loss: 0.0132\n",
      "Epoch [2/10], Step [620/630], Loss: 0.0132\n",
      "Epoch [2/10], Step [630/630], Loss: 0.1153\n",
      "--- Epoch 2 Summary ---\n",
      "Average Loss: 0.1090\n",
      "\n",
      "Epoch [2/10], Step [630/630], Loss: 0.1153\n",
      "--- Epoch 2 Summary ---\n",
      "Average Loss: 0.1090\n",
      "\n",
      "Epoch [3/10], Step [10/630], Loss: 0.1627\n",
      "Epoch [3/10], Step [10/630], Loss: 0.1627\n",
      "Epoch [3/10], Step [20/630], Loss: 0.0658\n",
      "Epoch [3/10], Step [20/630], Loss: 0.0658\n",
      "Epoch [3/10], Step [30/630], Loss: 0.1031\n",
      "Epoch [3/10], Step [30/630], Loss: 0.1031\n",
      "Epoch [3/10], Step [40/630], Loss: 0.0522\n",
      "Epoch [3/10], Step [40/630], Loss: 0.0522\n",
      "Epoch [3/10], Step [50/630], Loss: 0.0000\n",
      "Epoch [3/10], Step [50/630], Loss: 0.0000\n",
      "Epoch [3/10], Step [60/630], Loss: 0.3523\n",
      "Epoch [3/10], Step [60/630], Loss: 0.3523\n",
      "Epoch [3/10], Step [70/630], Loss: 0.0573\n",
      "Epoch [3/10], Step [70/630], Loss: 0.0573\n",
      "Epoch [3/10], Step [80/630], Loss: 0.0093\n",
      "Epoch [3/10], Step [80/630], Loss: 0.0093\n",
      "Epoch [3/10], Step [90/630], Loss: 0.0908\n",
      "Epoch [3/10], Step [90/630], Loss: 0.0908\n",
      "Epoch [3/10], Step [100/630], Loss: 0.1915\n",
      "Epoch [3/10], Step [100/630], Loss: 0.1915\n",
      "Epoch [3/10], Step [110/630], Loss: 0.0738\n",
      "Epoch [3/10], Step [110/630], Loss: 0.0738\n",
      "Epoch [3/10], Step [120/630], Loss: 0.1270\n",
      "Epoch [3/10], Step [120/630], Loss: 0.1270\n",
      "Epoch [3/10], Step [130/630], Loss: 0.1068\n",
      "Epoch [3/10], Step [130/630], Loss: 0.1068\n",
      "Epoch [3/10], Step [140/630], Loss: 0.0754\n",
      "Epoch [3/10], Step [140/630], Loss: 0.0754\n",
      "Epoch [3/10], Step [150/630], Loss: 0.0700\n",
      "Epoch [3/10], Step [150/630], Loss: 0.0700\n",
      "Epoch [3/10], Step [160/630], Loss: 0.0192\n",
      "Epoch [3/10], Step [160/630], Loss: 0.0192\n",
      "Epoch [3/10], Step [170/630], Loss: 0.0057\n",
      "Epoch [3/10], Step [170/630], Loss: 0.0057\n",
      "Epoch [3/10], Step [180/630], Loss: 0.1095\n",
      "Epoch [3/10], Step [180/630], Loss: 0.1095\n",
      "Epoch [3/10], Step [190/630], Loss: 0.1142\n",
      "Epoch [3/10], Step [190/630], Loss: 0.1142\n",
      "Epoch [3/10], Step [200/630], Loss: 0.1574\n",
      "Epoch [3/10], Step [200/630], Loss: 0.1574\n",
      "Epoch [3/10], Step [210/630], Loss: 0.0645\n",
      "Epoch [3/10], Step [210/630], Loss: 0.0645\n",
      "Epoch [3/10], Step [220/630], Loss: 0.1948\n",
      "Epoch [3/10], Step [220/630], Loss: 0.1948\n",
      "Epoch [3/10], Step [230/630], Loss: 0.0000\n",
      "Epoch [3/10], Step [230/630], Loss: 0.0000\n",
      "Epoch [3/10], Step [240/630], Loss: 0.1247\n",
      "Epoch [3/10], Step [240/630], Loss: 0.1247\n",
      "Epoch [3/10], Step [250/630], Loss: 0.0475\n",
      "Epoch [3/10], Step [250/630], Loss: 0.0475\n",
      "Epoch [3/10], Step [260/630], Loss: 0.0486\n",
      "Epoch [3/10], Step [260/630], Loss: 0.0486\n",
      "Epoch [3/10], Step [270/630], Loss: 0.1531\n",
      "Epoch [3/10], Step [270/630], Loss: 0.1531\n",
      "Epoch [3/10], Step [280/630], Loss: 0.0026\n",
      "Epoch [3/10], Step [280/630], Loss: 0.0026\n",
      "Epoch [3/10], Step [290/630], Loss: 0.0803\n",
      "Epoch [3/10], Step [290/630], Loss: 0.0803\n",
      "Epoch [3/10], Step [300/630], Loss: 0.0300\n",
      "Epoch [3/10], Step [300/630], Loss: 0.0300\n",
      "Epoch [3/10], Step [310/630], Loss: 0.1341\n",
      "Epoch [3/10], Step [310/630], Loss: 0.1341\n",
      "Epoch [3/10], Step [320/630], Loss: 0.0601\n",
      "Epoch [3/10], Step [320/630], Loss: 0.0601\n",
      "Epoch [3/10], Step [330/630], Loss: 0.1089\n",
      "Epoch [3/10], Step [330/630], Loss: 0.1089\n",
      "Epoch [3/10], Step [340/630], Loss: 0.4905\n",
      "Epoch [3/10], Step [340/630], Loss: 0.4905\n",
      "Epoch [3/10], Step [350/630], Loss: 0.0530\n",
      "Epoch [3/10], Step [350/630], Loss: 0.0530\n",
      "Epoch [3/10], Step [360/630], Loss: 0.0038\n",
      "Epoch [3/10], Step [360/630], Loss: 0.0038\n",
      "Epoch [3/10], Step [370/630], Loss: 0.1059\n",
      "Epoch [3/10], Step [370/630], Loss: 0.1059\n",
      "Epoch [3/10], Step [380/630], Loss: 0.0262\n",
      "Epoch [3/10], Step [380/630], Loss: 0.0262\n",
      "Epoch [3/10], Step [390/630], Loss: 0.0704\n",
      "Epoch [3/10], Step [390/630], Loss: 0.0704\n",
      "Epoch [3/10], Step [400/630], Loss: 0.1182\n",
      "Epoch [3/10], Step [400/630], Loss: 0.1182\n",
      "Epoch [3/10], Step [410/630], Loss: 0.0091\n",
      "Epoch [3/10], Step [410/630], Loss: 0.0091\n",
      "Epoch [3/10], Step [420/630], Loss: 0.0581\n",
      "Epoch [3/10], Step [420/630], Loss: 0.0581\n",
      "Epoch [3/10], Step [430/630], Loss: 0.0403\n",
      "Epoch [3/10], Step [430/630], Loss: 0.0403\n",
      "Epoch [3/10], Step [440/630], Loss: 0.2870\n",
      "Epoch [3/10], Step [440/630], Loss: 0.2870\n",
      "Epoch [3/10], Step [450/630], Loss: 0.0283\n",
      "Epoch [3/10], Step [450/630], Loss: 0.0283\n",
      "Epoch [3/10], Step [460/630], Loss: 0.0743\n",
      "Epoch [3/10], Step [460/630], Loss: 0.0743\n",
      "Epoch [3/10], Step [470/630], Loss: 0.0932\n",
      "Epoch [3/10], Step [470/630], Loss: 0.0932\n",
      "Epoch [3/10], Step [480/630], Loss: 0.0844\n",
      "Epoch [3/10], Step [480/630], Loss: 0.0844\n",
      "Epoch [3/10], Step [490/630], Loss: 0.0272\n",
      "Epoch [3/10], Step [490/630], Loss: 0.0272\n",
      "Epoch [3/10], Step [500/630], Loss: 0.1500\n",
      "Epoch [3/10], Step [500/630], Loss: 0.1500\n",
      "Epoch [3/10], Step [510/630], Loss: 0.0902\n",
      "Epoch [3/10], Step [510/630], Loss: 0.0902\n",
      "Epoch [3/10], Step [520/630], Loss: 0.0827\n",
      "Epoch [3/10], Step [520/630], Loss: 0.0827\n",
      "Epoch [3/10], Step [530/630], Loss: 0.0626\n",
      "Epoch [3/10], Step [530/630], Loss: 0.0626\n",
      "Epoch [3/10], Step [540/630], Loss: 0.1260\n",
      "Epoch [3/10], Step [540/630], Loss: 0.1260\n",
      "Epoch [3/10], Step [550/630], Loss: 0.1123\n",
      "Epoch [3/10], Step [550/630], Loss: 0.1123\n",
      "Epoch [3/10], Step [560/630], Loss: 0.1849\n",
      "Epoch [3/10], Step [560/630], Loss: 0.1849\n",
      "Epoch [3/10], Step [570/630], Loss: 0.0685\n",
      "Epoch [3/10], Step [570/630], Loss: 0.0685\n",
      "Epoch [3/10], Step [580/630], Loss: 0.0857\n",
      "Epoch [3/10], Step [580/630], Loss: 0.0857\n",
      "Epoch [3/10], Step [590/630], Loss: 0.0543\n",
      "Epoch [3/10], Step [590/630], Loss: 0.0543\n",
      "Epoch [3/10], Step [600/630], Loss: 0.0000\n",
      "Epoch [3/10], Step [600/630], Loss: 0.0000\n",
      "Epoch [3/10], Step [610/630], Loss: 0.0582\n",
      "Epoch [3/10], Step [610/630], Loss: 0.0582\n",
      "Epoch [3/10], Step [620/630], Loss: 0.0460\n",
      "Epoch [3/10], Step [620/630], Loss: 0.0460\n",
      "Epoch [3/10], Step [630/630], Loss: 0.1213\n",
      "--- Epoch 3 Summary ---\n",
      "Average Loss: 0.0863\n",
      "\n",
      "Epoch [3/10], Step [630/630], Loss: 0.1213\n",
      "--- Epoch 3 Summary ---\n",
      "Average Loss: 0.0863\n",
      "\n",
      "Epoch [4/10], Step [10/630], Loss: 0.0913\n",
      "Epoch [4/10], Step [10/630], Loss: 0.0913\n",
      "Epoch [4/10], Step [20/630], Loss: 0.0157\n",
      "Epoch [4/10], Step [20/630], Loss: 0.0157\n",
      "Epoch [4/10], Step [30/630], Loss: 0.0279\n",
      "Epoch [4/10], Step [30/630], Loss: 0.0279\n",
      "Epoch [4/10], Step [40/630], Loss: 0.2519\n",
      "Epoch [4/10], Step [40/630], Loss: 0.2519\n",
      "Epoch [4/10], Step [50/630], Loss: 0.0813\n",
      "Epoch [4/10], Step [50/630], Loss: 0.0813\n",
      "Epoch [4/10], Step [60/630], Loss: 0.1095\n",
      "Epoch [4/10], Step [60/630], Loss: 0.1095\n",
      "Epoch [4/10], Step [70/630], Loss: 0.0109\n",
      "Epoch [4/10], Step [70/630], Loss: 0.0109\n",
      "Epoch [4/10], Step [80/630], Loss: 0.1728\n",
      "Epoch [4/10], Step [80/630], Loss: 0.1728\n",
      "Epoch [4/10], Step [90/630], Loss: 0.0442\n",
      "Epoch [4/10], Step [90/630], Loss: 0.0442\n",
      "Epoch [4/10], Step [100/630], Loss: 0.0625\n",
      "Epoch [4/10], Step [100/630], Loss: 0.0625\n",
      "Epoch [4/10], Step [110/630], Loss: 0.0017\n",
      "Epoch [4/10], Step [110/630], Loss: 0.0017\n",
      "Epoch [4/10], Step [120/630], Loss: 0.0148\n",
      "Epoch [4/10], Step [120/630], Loss: 0.0148\n",
      "Epoch [4/10], Step [130/630], Loss: 0.2000\n",
      "Epoch [4/10], Step [130/630], Loss: 0.2000\n",
      "Epoch [4/10], Step [140/630], Loss: 0.1681\n",
      "Epoch [4/10], Step [140/630], Loss: 0.1681\n",
      "Epoch [4/10], Step [150/630], Loss: 0.0674\n",
      "Epoch [4/10], Step [150/630], Loss: 0.0674\n",
      "Epoch [4/10], Step [160/630], Loss: 0.0383\n",
      "Epoch [4/10], Step [160/630], Loss: 0.0383\n",
      "Epoch [4/10], Step [170/630], Loss: 0.0096\n",
      "Epoch [4/10], Step [170/630], Loss: 0.0096\n",
      "Epoch [4/10], Step [180/630], Loss: 0.0431\n",
      "Epoch [4/10], Step [180/630], Loss: 0.0431\n",
      "Epoch [4/10], Step [190/630], Loss: 0.1582\n",
      "Epoch [4/10], Step [190/630], Loss: 0.1582\n",
      "Epoch [4/10], Step [200/630], Loss: 0.0719\n",
      "Epoch [4/10], Step [200/630], Loss: 0.0719\n",
      "Epoch [4/10], Step [210/630], Loss: 0.0545\n",
      "Epoch [4/10], Step [210/630], Loss: 0.0545\n",
      "Epoch [4/10], Step [220/630], Loss: 0.0083\n",
      "Epoch [4/10], Step [220/630], Loss: 0.0083\n",
      "Epoch [4/10], Step [230/630], Loss: 0.0245\n",
      "Epoch [4/10], Step [230/630], Loss: 0.0245\n",
      "Epoch [4/10], Step [240/630], Loss: 0.0967\n",
      "Epoch [4/10], Step [240/630], Loss: 0.0967\n",
      "Epoch [4/10], Step [250/630], Loss: 0.1652\n",
      "Epoch [4/10], Step [250/630], Loss: 0.1652\n",
      "Epoch [4/10], Step [260/630], Loss: 0.0380\n",
      "Epoch [4/10], Step [260/630], Loss: 0.0380\n",
      "Epoch [4/10], Step [270/630], Loss: 0.0000\n",
      "Epoch [4/10], Step [270/630], Loss: 0.0000\n",
      "Epoch [4/10], Step [280/630], Loss: 0.0646\n",
      "Epoch [4/10], Step [280/630], Loss: 0.0646\n",
      "Epoch [4/10], Step [290/630], Loss: 0.0251\n",
      "Epoch [4/10], Step [290/630], Loss: 0.0251\n",
      "Epoch [4/10], Step [300/630], Loss: 0.3487\n",
      "Epoch [4/10], Step [300/630], Loss: 0.3487\n",
      "Epoch [4/10], Step [310/630], Loss: 0.0396\n",
      "Epoch [4/10], Step [310/630], Loss: 0.0396\n",
      "Epoch [4/10], Step [320/630], Loss: 0.0468\n",
      "Epoch [4/10], Step [320/630], Loss: 0.0468\n",
      "Epoch [4/10], Step [330/630], Loss: 0.0759\n",
      "Epoch [4/10], Step [330/630], Loss: 0.0759\n",
      "Epoch [4/10], Step [340/630], Loss: 0.0652\n",
      "Epoch [4/10], Step [340/630], Loss: 0.0652\n",
      "Epoch [4/10], Step [350/630], Loss: 0.0994\n",
      "Epoch [4/10], Step [350/630], Loss: 0.0994\n",
      "Epoch [4/10], Step [360/630], Loss: 0.1812\n",
      "Epoch [4/10], Step [360/630], Loss: 0.1812\n",
      "Epoch [4/10], Step [370/630], Loss: 0.0137\n",
      "Epoch [4/10], Step [370/630], Loss: 0.0137\n",
      "Epoch [4/10], Step [380/630], Loss: 0.0497\n",
      "Epoch [4/10], Step [380/630], Loss: 0.0497\n",
      "Epoch [4/10], Step [390/630], Loss: 0.0628\n",
      "Epoch [4/10], Step [390/630], Loss: 0.0628\n",
      "Epoch [4/10], Step [400/630], Loss: 0.0105\n",
      "Epoch [4/10], Step [400/630], Loss: 0.0105\n",
      "Epoch [4/10], Step [410/630], Loss: 0.1181\n",
      "Epoch [4/10], Step [410/630], Loss: 0.1181\n",
      "Epoch [4/10], Step [420/630], Loss: 0.0590\n",
      "Epoch [4/10], Step [420/630], Loss: 0.0590\n",
      "Epoch [4/10], Step [430/630], Loss: 0.0056\n",
      "Epoch [4/10], Step [430/630], Loss: 0.0056\n",
      "Epoch [4/10], Step [440/630], Loss: 0.1269\n",
      "Epoch [4/10], Step [440/630], Loss: 0.1269\n",
      "Epoch [4/10], Step [450/630], Loss: 0.1469\n",
      "Epoch [4/10], Step [450/630], Loss: 0.1469\n",
      "Epoch [4/10], Step [460/630], Loss: 0.0860\n",
      "Epoch [4/10], Step [460/630], Loss: 0.0860\n",
      "Epoch [4/10], Step [470/630], Loss: 0.0638\n",
      "Epoch [4/10], Step [470/630], Loss: 0.0638\n",
      "Epoch [4/10], Step [480/630], Loss: 0.1558\n",
      "Epoch [4/10], Step [480/630], Loss: 0.1558\n",
      "Epoch [4/10], Step [490/630], Loss: 0.0528\n",
      "Epoch [4/10], Step [490/630], Loss: 0.0528\n",
      "Epoch [4/10], Step [500/630], Loss: 0.2370\n",
      "Epoch [4/10], Step [500/630], Loss: 0.2370\n",
      "Epoch [4/10], Step [510/630], Loss: 0.0706\n",
      "Epoch [4/10], Step [510/630], Loss: 0.0706\n",
      "Epoch [4/10], Step [520/630], Loss: 0.0843\n",
      "Epoch [4/10], Step [520/630], Loss: 0.0843\n",
      "Epoch [4/10], Step [530/630], Loss: 0.1054\n",
      "Epoch [4/10], Step [530/630], Loss: 0.1054\n",
      "Epoch [4/10], Step [540/630], Loss: 0.0262\n",
      "Epoch [4/10], Step [540/630], Loss: 0.0262\n",
      "Epoch [4/10], Step [550/630], Loss: 0.1506\n",
      "Epoch [4/10], Step [550/630], Loss: 0.1506\n",
      "Epoch [4/10], Step [560/630], Loss: 0.0015\n",
      "Epoch [4/10], Step [560/630], Loss: 0.0015\n",
      "Epoch [4/10], Step [570/630], Loss: 0.0013\n",
      "Epoch [4/10], Step [570/630], Loss: 0.0013\n",
      "Epoch [4/10], Step [580/630], Loss: 0.0876\n",
      "Epoch [4/10], Step [580/630], Loss: 0.0876\n",
      "Epoch [4/10], Step [590/630], Loss: 0.0815\n",
      "Epoch [4/10], Step [590/630], Loss: 0.0815\n",
      "Epoch [4/10], Step [600/630], Loss: 0.0988\n",
      "Epoch [4/10], Step [600/630], Loss: 0.0988\n",
      "Epoch [4/10], Step [610/630], Loss: 0.0613\n",
      "Epoch [4/10], Step [610/630], Loss: 0.0613\n",
      "Epoch [4/10], Step [620/630], Loss: 0.0458\n",
      "Epoch [4/10], Step [620/630], Loss: 0.0458\n",
      "Epoch [4/10], Step [630/630], Loss: 0.0068\n",
      "--- Epoch 4 Summary ---\n",
      "Average Loss: 0.0718\n",
      "\n",
      "Epoch [4/10], Step [630/630], Loss: 0.0068\n",
      "--- Epoch 4 Summary ---\n",
      "Average Loss: 0.0718\n",
      "\n",
      "Epoch [5/10], Step [10/630], Loss: 0.0460\n",
      "Epoch [5/10], Step [10/630], Loss: 0.0460\n",
      "Epoch [5/10], Step [20/630], Loss: 0.1233\n",
      "Epoch [5/10], Step [20/630], Loss: 0.1233\n",
      "Epoch [5/10], Step [30/630], Loss: 0.0514\n",
      "Epoch [5/10], Step [30/630], Loss: 0.0514\n",
      "Epoch [5/10], Step [40/630], Loss: 0.1327\n",
      "Epoch [5/10], Step [40/630], Loss: 0.1327\n",
      "Epoch [5/10], Step [50/630], Loss: 0.1820\n",
      "Epoch [5/10], Step [50/630], Loss: 0.1820\n",
      "Epoch [5/10], Step [60/630], Loss: 0.0423\n",
      "Epoch [5/10], Step [60/630], Loss: 0.0423\n",
      "Epoch [5/10], Step [70/630], Loss: 0.1753\n",
      "Epoch [5/10], Step [70/630], Loss: 0.1753\n",
      "Epoch [5/10], Step [80/630], Loss: 0.0657\n",
      "Epoch [5/10], Step [80/630], Loss: 0.0657\n",
      "Epoch [5/10], Step [90/630], Loss: 0.0473\n",
      "Epoch [5/10], Step [90/630], Loss: 0.0473\n",
      "Epoch [5/10], Step [100/630], Loss: 0.1276\n",
      "Epoch [5/10], Step [100/630], Loss: 0.1276\n",
      "Epoch [5/10], Step [110/630], Loss: 0.0912\n",
      "Epoch [5/10], Step [110/630], Loss: 0.0912\n",
      "Epoch [5/10], Step [120/630], Loss: 0.0182\n",
      "Epoch [5/10], Step [120/630], Loss: 0.0182\n",
      "Epoch [5/10], Step [130/630], Loss: 0.0000\n",
      "Epoch [5/10], Step [130/630], Loss: 0.0000\n",
      "Epoch [5/10], Step [140/630], Loss: 0.0000\n",
      "Epoch [5/10], Step [140/630], Loss: 0.0000\n",
      "Epoch [5/10], Step [150/630], Loss: 0.0054\n",
      "Epoch [5/10], Step [150/630], Loss: 0.0054\n",
      "Epoch [5/10], Step [160/630], Loss: 0.0512\n",
      "Epoch [5/10], Step [160/630], Loss: 0.0512\n",
      "Epoch [5/10], Step [170/630], Loss: 0.1526\n",
      "Epoch [5/10], Step [170/630], Loss: 0.1526\n",
      "Epoch [5/10], Step [180/630], Loss: 0.0422\n",
      "Epoch [5/10], Step [180/630], Loss: 0.0422\n",
      "Epoch [5/10], Step [190/630], Loss: 0.0218\n",
      "Epoch [5/10], Step [190/630], Loss: 0.0218\n",
      "Epoch [5/10], Step [200/630], Loss: 0.0941\n",
      "Epoch [5/10], Step [200/630], Loss: 0.0941\n",
      "Epoch [5/10], Step [210/630], Loss: 0.1061\n",
      "Epoch [5/10], Step [210/630], Loss: 0.1061\n",
      "Epoch [5/10], Step [220/630], Loss: 0.0799\n",
      "Epoch [5/10], Step [220/630], Loss: 0.0799\n",
      "Epoch [5/10], Step [230/630], Loss: 0.1946\n",
      "Epoch [5/10], Step [230/630], Loss: 0.1946\n",
      "Epoch [5/10], Step [240/630], Loss: 0.0708\n",
      "Epoch [5/10], Step [240/630], Loss: 0.0708\n",
      "Epoch [5/10], Step [250/630], Loss: 0.1043\n",
      "Epoch [5/10], Step [250/630], Loss: 0.1043\n",
      "Epoch [5/10], Step [260/630], Loss: 0.0341\n",
      "Epoch [5/10], Step [260/630], Loss: 0.0341\n",
      "Epoch [5/10], Step [270/630], Loss: 0.1249\n",
      "Epoch [5/10], Step [270/630], Loss: 0.1249\n",
      "Epoch [5/10], Step [280/630], Loss: 0.0000\n",
      "Epoch [5/10], Step [280/630], Loss: 0.0000\n",
      "Epoch [5/10], Step [290/630], Loss: 0.0000\n",
      "Epoch [5/10], Step [290/630], Loss: 0.0000\n",
      "Epoch [5/10], Step [300/630], Loss: 0.0113\n",
      "Epoch [5/10], Step [300/630], Loss: 0.0113\n",
      "Epoch [5/10], Step [310/630], Loss: 0.0000\n",
      "Epoch [5/10], Step [310/630], Loss: 0.0000\n",
      "Epoch [5/10], Step [320/630], Loss: 0.0722\n",
      "Epoch [5/10], Step [320/630], Loss: 0.0722\n",
      "Epoch [5/10], Step [330/630], Loss: 0.0177\n",
      "Epoch [5/10], Step [330/630], Loss: 0.0177\n",
      "Epoch [5/10], Step [340/630], Loss: 0.0156\n",
      "Epoch [5/10], Step [340/630], Loss: 0.0156\n",
      "Epoch [5/10], Step [350/630], Loss: 0.0938\n",
      "Epoch [5/10], Step [350/630], Loss: 0.0938\n",
      "Epoch [5/10], Step [360/630], Loss: 0.0383\n",
      "Epoch [5/10], Step [360/630], Loss: 0.0383\n",
      "Epoch [5/10], Step [370/630], Loss: 0.0656\n",
      "Epoch [5/10], Step [370/630], Loss: 0.0656\n",
      "Epoch [5/10], Step [380/630], Loss: 0.0149\n",
      "Epoch [5/10], Step [380/630], Loss: 0.0149\n",
      "Epoch [5/10], Step [390/630], Loss: 0.0074\n",
      "Epoch [5/10], Step [390/630], Loss: 0.0074\n",
      "Epoch [5/10], Step [400/630], Loss: 0.1097\n",
      "Epoch [5/10], Step [400/630], Loss: 0.1097\n",
      "Epoch [5/10], Step [410/630], Loss: 0.0417\n",
      "Epoch [5/10], Step [410/630], Loss: 0.0417\n",
      "Epoch [5/10], Step [420/630], Loss: 0.0505\n",
      "Epoch [5/10], Step [420/630], Loss: 0.0505\n",
      "Epoch [5/10], Step [430/630], Loss: 0.0643\n",
      "Epoch [5/10], Step [430/630], Loss: 0.0643\n",
      "Epoch [5/10], Step [440/630], Loss: 0.0000\n",
      "Epoch [5/10], Step [440/630], Loss: 0.0000\n",
      "Epoch [5/10], Step [450/630], Loss: 0.0370\n",
      "Epoch [5/10], Step [450/630], Loss: 0.0370\n",
      "Epoch [5/10], Step [460/630], Loss: 0.0122\n",
      "Epoch [5/10], Step [460/630], Loss: 0.0122\n",
      "Epoch [5/10], Step [470/630], Loss: 0.1519\n",
      "Epoch [5/10], Step [470/630], Loss: 0.1519\n",
      "Epoch [5/10], Step [480/630], Loss: 0.0172\n",
      "Epoch [5/10], Step [480/630], Loss: 0.0172\n",
      "Epoch [5/10], Step [490/630], Loss: 0.0679\n",
      "Epoch [5/10], Step [490/630], Loss: 0.0679\n",
      "Epoch [5/10], Step [500/630], Loss: 0.1210\n",
      "Epoch [5/10], Step [500/630], Loss: 0.1210\n",
      "Epoch [5/10], Step [510/630], Loss: 0.1666\n",
      "Epoch [5/10], Step [510/630], Loss: 0.1666\n",
      "Epoch [5/10], Step [520/630], Loss: 0.0195\n",
      "Epoch [5/10], Step [520/630], Loss: 0.0195\n",
      "Epoch [5/10], Step [530/630], Loss: 0.0201\n",
      "Epoch [5/10], Step [530/630], Loss: 0.0201\n",
      "Epoch [5/10], Step [540/630], Loss: 0.0000\n",
      "Epoch [5/10], Step [540/630], Loss: 0.0000\n",
      "Epoch [5/10], Step [550/630], Loss: 0.0000\n",
      "Epoch [5/10], Step [550/630], Loss: 0.0000\n",
      "Epoch [5/10], Step [560/630], Loss: 0.0560\n",
      "Epoch [5/10], Step [560/630], Loss: 0.0560\n",
      "Epoch [5/10], Step [570/630], Loss: 0.0312\n",
      "Epoch [5/10], Step [570/630], Loss: 0.0312\n",
      "Epoch [5/10], Step [580/630], Loss: 0.1101\n",
      "Epoch [5/10], Step [580/630], Loss: 0.1101\n",
      "Epoch [5/10], Step [590/630], Loss: 0.0897\n",
      "Epoch [5/10], Step [590/630], Loss: 0.0897\n",
      "Epoch [5/10], Step [600/630], Loss: 0.0000\n",
      "Epoch [5/10], Step [600/630], Loss: 0.0000\n",
      "Epoch [5/10], Step [610/630], Loss: 0.0595\n",
      "Epoch [5/10], Step [610/630], Loss: 0.0595\n",
      "Epoch [5/10], Step [620/630], Loss: 0.0253\n",
      "Epoch [5/10], Step [620/630], Loss: 0.0253\n",
      "Epoch [5/10], Step [630/630], Loss: 0.1578\n",
      "--- Epoch 5 Summary ---\n",
      "Average Loss: 0.0611\n",
      "\n",
      "Epoch [5/10], Step [630/630], Loss: 0.1578\n",
      "--- Epoch 5 Summary ---\n",
      "Average Loss: 0.0611\n",
      "\n",
      "Epoch [6/10], Step [10/630], Loss: 0.0298\n",
      "Epoch [6/10], Step [10/630], Loss: 0.0298\n",
      "Epoch [6/10], Step [20/630], Loss: 0.1393\n",
      "Epoch [6/10], Step [20/630], Loss: 0.1393\n",
      "Epoch [6/10], Step [30/630], Loss: 0.1541\n",
      "Epoch [6/10], Step [30/630], Loss: 0.1541\n",
      "Epoch [6/10], Step [40/630], Loss: 0.0132\n",
      "Epoch [6/10], Step [40/630], Loss: 0.0132\n",
      "Epoch [6/10], Step [50/630], Loss: 0.1204\n",
      "Epoch [6/10], Step [50/630], Loss: 0.1204\n",
      "Epoch [6/10], Step [60/630], Loss: 0.0285\n",
      "Epoch [6/10], Step [60/630], Loss: 0.0285\n",
      "Epoch [6/10], Step [70/630], Loss: 0.2019\n",
      "Epoch [6/10], Step [70/630], Loss: 0.2019\n",
      "Epoch [6/10], Step [80/630], Loss: 0.0044\n",
      "Epoch [6/10], Step [80/630], Loss: 0.0044\n",
      "Epoch [6/10], Step [90/630], Loss: 0.0738\n",
      "Epoch [6/10], Step [90/630], Loss: 0.0738\n",
      "Epoch [6/10], Step [100/630], Loss: 0.0000\n",
      "Epoch [6/10], Step [100/630], Loss: 0.0000\n",
      "Epoch [6/10], Step [110/630], Loss: 0.0073\n",
      "Epoch [6/10], Step [110/630], Loss: 0.0073\n",
      "Epoch [6/10], Step [120/630], Loss: 0.0342\n",
      "Epoch [6/10], Step [120/630], Loss: 0.0342\n",
      "Epoch [6/10], Step [130/630], Loss: 0.0541\n",
      "Epoch [6/10], Step [130/630], Loss: 0.0541\n",
      "Epoch [6/10], Step [140/630], Loss: 0.0770\n",
      "Epoch [6/10], Step [140/630], Loss: 0.0770\n",
      "Epoch [6/10], Step [150/630], Loss: 0.0000\n",
      "Epoch [6/10], Step [150/630], Loss: 0.0000\n",
      "Epoch [6/10], Step [160/630], Loss: 0.0172\n",
      "Epoch [6/10], Step [160/630], Loss: 0.0172\n",
      "Epoch [6/10], Step [170/630], Loss: 0.0822\n",
      "Epoch [6/10], Step [170/630], Loss: 0.0822\n",
      "Epoch [6/10], Step [180/630], Loss: 0.0297\n",
      "Epoch [6/10], Step [180/630], Loss: 0.0297\n",
      "Epoch [6/10], Step [190/630], Loss: 0.0202\n",
      "Epoch [6/10], Step [190/630], Loss: 0.0202\n",
      "Epoch [6/10], Step [200/630], Loss: 0.0494\n",
      "Epoch [6/10], Step [200/630], Loss: 0.0494\n",
      "Epoch [6/10], Step [210/630], Loss: 0.0062\n",
      "Epoch [6/10], Step [210/630], Loss: 0.0062\n",
      "Epoch [6/10], Step [220/630], Loss: 0.0198\n",
      "Epoch [6/10], Step [220/630], Loss: 0.0198\n",
      "Epoch [6/10], Step [230/630], Loss: 0.0303\n",
      "Epoch [6/10], Step [230/630], Loss: 0.0303\n",
      "Epoch [6/10], Step [240/630], Loss: 0.0921\n",
      "Epoch [6/10], Step [240/630], Loss: 0.0921\n",
      "Epoch [6/10], Step [250/630], Loss: 0.0209\n",
      "Epoch [6/10], Step [250/630], Loss: 0.0209\n",
      "Epoch [6/10], Step [260/630], Loss: 0.0145\n",
      "Epoch [6/10], Step [260/630], Loss: 0.0145\n",
      "Epoch [6/10], Step [270/630], Loss: 0.0068\n",
      "Epoch [6/10], Step [270/630], Loss: 0.0068\n",
      "Epoch [6/10], Step [280/630], Loss: 0.0000\n",
      "Epoch [6/10], Step [280/630], Loss: 0.0000\n",
      "Epoch [6/10], Step [290/630], Loss: 0.0009\n",
      "Epoch [6/10], Step [290/630], Loss: 0.0009\n",
      "Epoch [6/10], Step [300/630], Loss: 0.0627\n",
      "Epoch [6/10], Step [300/630], Loss: 0.0627\n",
      "Epoch [6/10], Step [310/630], Loss: 0.0078\n",
      "Epoch [6/10], Step [310/630], Loss: 0.0078\n",
      "Epoch [6/10], Step [320/630], Loss: 0.0547\n",
      "Epoch [6/10], Step [320/630], Loss: 0.0547\n",
      "Epoch [6/10], Step [330/630], Loss: 0.0274\n",
      "Epoch [6/10], Step [330/630], Loss: 0.0274\n",
      "Epoch [6/10], Step [340/630], Loss: 0.0000\n",
      "Epoch [6/10], Step [340/630], Loss: 0.0000\n",
      "Epoch [6/10], Step [350/630], Loss: 0.0869\n",
      "Epoch [6/10], Step [350/630], Loss: 0.0869\n",
      "Epoch [6/10], Step [360/630], Loss: 0.2308\n",
      "Epoch [6/10], Step [360/630], Loss: 0.2308\n",
      "Epoch [6/10], Step [370/630], Loss: 0.0602\n",
      "Epoch [6/10], Step [370/630], Loss: 0.0602\n",
      "Epoch [6/10], Step [380/630], Loss: 0.0966\n",
      "Epoch [6/10], Step [380/630], Loss: 0.0966\n",
      "Epoch [6/10], Step [390/630], Loss: 0.1370\n",
      "Epoch [6/10], Step [390/630], Loss: 0.1370\n",
      "Epoch [6/10], Step [400/630], Loss: 0.0660\n",
      "Epoch [6/10], Step [400/630], Loss: 0.0660\n",
      "Epoch [6/10], Step [410/630], Loss: 0.0214\n",
      "Epoch [6/10], Step [410/630], Loss: 0.0214\n",
      "Epoch [6/10], Step [420/630], Loss: 0.1100\n",
      "Epoch [6/10], Step [420/630], Loss: 0.1100\n",
      "Epoch [6/10], Step [430/630], Loss: 0.0623\n",
      "Epoch [6/10], Step [430/630], Loss: 0.0623\n",
      "Epoch [6/10], Step [440/630], Loss: 0.0470\n",
      "Epoch [6/10], Step [440/630], Loss: 0.0470\n",
      "Epoch [6/10], Step [450/630], Loss: 0.1750\n",
      "Epoch [6/10], Step [450/630], Loss: 0.1750\n",
      "Epoch [6/10], Step [460/630], Loss: 0.1065\n",
      "Epoch [6/10], Step [460/630], Loss: 0.1065\n",
      "Epoch [6/10], Step [470/630], Loss: 0.1056\n",
      "Epoch [6/10], Step [470/630], Loss: 0.1056\n",
      "Epoch [6/10], Step [480/630], Loss: 0.0337\n",
      "Epoch [6/10], Step [480/630], Loss: 0.0337\n",
      "Epoch [6/10], Step [490/630], Loss: 0.0126\n",
      "Epoch [6/10], Step [490/630], Loss: 0.0126\n",
      "Epoch [6/10], Step [500/630], Loss: 0.0153\n",
      "Epoch [6/10], Step [500/630], Loss: 0.0153\n",
      "Epoch [6/10], Step [510/630], Loss: 0.0660\n",
      "Epoch [6/10], Step [510/630], Loss: 0.0660\n",
      "Epoch [6/10], Step [520/630], Loss: 0.0653\n",
      "Epoch [6/10], Step [520/630], Loss: 0.0653\n",
      "Epoch [6/10], Step [530/630], Loss: 0.0356\n",
      "Epoch [6/10], Step [530/630], Loss: 0.0356\n",
      "Epoch [6/10], Step [540/630], Loss: 0.0846\n",
      "Epoch [6/10], Step [540/630], Loss: 0.0846\n",
      "Epoch [6/10], Step [550/630], Loss: 0.0112\n",
      "Epoch [6/10], Step [550/630], Loss: 0.0112\n",
      "Epoch [6/10], Step [560/630], Loss: 0.0910\n",
      "Epoch [6/10], Step [560/630], Loss: 0.0910\n",
      "Epoch [6/10], Step [570/630], Loss: 0.0788\n",
      "Epoch [6/10], Step [570/630], Loss: 0.0788\n",
      "Epoch [6/10], Step [580/630], Loss: 0.1495\n",
      "Epoch [6/10], Step [580/630], Loss: 0.1495\n",
      "Epoch [6/10], Step [590/630], Loss: 0.0176\n",
      "Epoch [6/10], Step [590/630], Loss: 0.0176\n",
      "Epoch [6/10], Step [600/630], Loss: 0.1166\n",
      "Epoch [6/10], Step [600/630], Loss: 0.1166\n",
      "Epoch [6/10], Step [610/630], Loss: 0.1132\n",
      "Epoch [6/10], Step [610/630], Loss: 0.1132\n",
      "Epoch [6/10], Step [620/630], Loss: 0.0000\n",
      "Epoch [6/10], Step [620/630], Loss: 0.0000\n",
      "Epoch [6/10], Step [630/630], Loss: 0.0339\n",
      "--- Epoch 6 Summary ---\n",
      "Average Loss: 0.0543\n",
      "\n",
      "Epoch [6/10], Step [630/630], Loss: 0.0339\n",
      "--- Epoch 6 Summary ---\n",
      "Average Loss: 0.0543\n",
      "\n",
      "Epoch [7/10], Step [10/630], Loss: 0.0265\n",
      "Epoch [7/10], Step [10/630], Loss: 0.0265\n",
      "Epoch [7/10], Step [20/630], Loss: 0.0121\n",
      "Epoch [7/10], Step [20/630], Loss: 0.0121\n",
      "Epoch [7/10], Step [30/630], Loss: 0.0373\n",
      "Epoch [7/10], Step [30/630], Loss: 0.0373\n",
      "Epoch [7/10], Step [40/630], Loss: 0.0656\n",
      "Epoch [7/10], Step [40/630], Loss: 0.0656\n",
      "Epoch [7/10], Step [50/630], Loss: 0.0000\n",
      "Epoch [7/10], Step [50/630], Loss: 0.0000\n",
      "Epoch [7/10], Step [60/630], Loss: 0.0529\n",
      "Epoch [7/10], Step [60/630], Loss: 0.0529\n",
      "Epoch [7/10], Step [70/630], Loss: 0.0852\n",
      "Epoch [7/10], Step [70/630], Loss: 0.0852\n",
      "Epoch [7/10], Step [80/630], Loss: 0.0000\n",
      "Epoch [7/10], Step [80/630], Loss: 0.0000\n",
      "Epoch [7/10], Step [90/630], Loss: 0.0000\n",
      "Epoch [7/10], Step [90/630], Loss: 0.0000\n",
      "Epoch [7/10], Step [100/630], Loss: 0.0305\n",
      "Epoch [7/10], Step [100/630], Loss: 0.0305\n",
      "Epoch [7/10], Step [110/630], Loss: 0.0000\n",
      "Epoch [7/10], Step [110/630], Loss: 0.0000\n",
      "Epoch [7/10], Step [120/630], Loss: 0.0549\n",
      "Epoch [7/10], Step [120/630], Loss: 0.0549\n",
      "Epoch [7/10], Step [130/630], Loss: 0.0179\n",
      "Epoch [7/10], Step [130/630], Loss: 0.0179\n",
      "Epoch [7/10], Step [140/630], Loss: 0.0946\n",
      "Epoch [7/10], Step [140/630], Loss: 0.0946\n",
      "Epoch [7/10], Step [150/630], Loss: 0.0163\n",
      "Epoch [7/10], Step [150/630], Loss: 0.0163\n",
      "Epoch [7/10], Step [160/630], Loss: 0.0861\n",
      "Epoch [7/10], Step [160/630], Loss: 0.0861\n",
      "Epoch [7/10], Step [170/630], Loss: 0.0000\n",
      "Epoch [7/10], Step [170/630], Loss: 0.0000\n",
      "Epoch [7/10], Step [180/630], Loss: 0.0130\n",
      "Epoch [7/10], Step [180/630], Loss: 0.0130\n",
      "Epoch [7/10], Step [190/630], Loss: 0.0000\n",
      "Epoch [7/10], Step [190/630], Loss: 0.0000\n",
      "Epoch [7/10], Step [200/630], Loss: 0.0000\n",
      "Epoch [7/10], Step [200/630], Loss: 0.0000\n",
      "Epoch [7/10], Step [210/630], Loss: 0.0103\n",
      "Epoch [7/10], Step [210/630], Loss: 0.0103\n",
      "Epoch [7/10], Step [220/630], Loss: 0.0000\n",
      "Epoch [7/10], Step [220/630], Loss: 0.0000\n",
      "Epoch [7/10], Step [230/630], Loss: 0.1594\n",
      "Epoch [7/10], Step [230/630], Loss: 0.1594\n",
      "Epoch [7/10], Step [240/630], Loss: 0.0348\n",
      "Epoch [7/10], Step [240/630], Loss: 0.0348\n",
      "Epoch [7/10], Step [250/630], Loss: 0.0000\n",
      "Epoch [7/10], Step [250/630], Loss: 0.0000\n",
      "Epoch [7/10], Step [260/630], Loss: 0.0000\n",
      "Epoch [7/10], Step [260/630], Loss: 0.0000\n",
      "Epoch [7/10], Step [270/630], Loss: 0.0000\n",
      "Epoch [7/10], Step [270/630], Loss: 0.0000\n",
      "Epoch [7/10], Step [280/630], Loss: 0.1050\n",
      "Epoch [7/10], Step [280/630], Loss: 0.1050\n",
      "Epoch [7/10], Step [290/630], Loss: 0.0293\n",
      "Epoch [7/10], Step [290/630], Loss: 0.0293\n",
      "Epoch [7/10], Step [300/630], Loss: 0.0000\n",
      "Epoch [7/10], Step [300/630], Loss: 0.0000\n",
      "Epoch [7/10], Step [310/630], Loss: 0.0000\n",
      "Epoch [7/10], Step [310/630], Loss: 0.0000\n",
      "Epoch [7/10], Step [320/630], Loss: 0.0005\n",
      "Epoch [7/10], Step [320/630], Loss: 0.0005\n",
      "Epoch [7/10], Step [330/630], Loss: 0.0508\n",
      "Epoch [7/10], Step [330/630], Loss: 0.0508\n",
      "Epoch [7/10], Step [340/630], Loss: 0.1150\n",
      "Epoch [7/10], Step [340/630], Loss: 0.1150\n",
      "Epoch [7/10], Step [350/630], Loss: 0.0179\n",
      "Epoch [7/10], Step [350/630], Loss: 0.0179\n",
      "Epoch [7/10], Step [360/630], Loss: 0.0223\n",
      "Epoch [7/10], Step [360/630], Loss: 0.0223\n",
      "Epoch [7/10], Step [370/630], Loss: 0.0393\n",
      "Epoch [7/10], Step [370/630], Loss: 0.0393\n",
      "Epoch [7/10], Step [380/630], Loss: 0.0108\n",
      "Epoch [7/10], Step [380/630], Loss: 0.0108\n",
      "Epoch [7/10], Step [390/630], Loss: 0.0066\n",
      "Epoch [7/10], Step [390/630], Loss: 0.0066\n",
      "Epoch [7/10], Step [400/630], Loss: 0.0028\n",
      "Epoch [7/10], Step [400/630], Loss: 0.0028\n",
      "Epoch [7/10], Step [410/630], Loss: 0.0008\n",
      "Epoch [7/10], Step [410/630], Loss: 0.0008\n",
      "Epoch [7/10], Step [420/630], Loss: 0.0518\n",
      "Epoch [7/10], Step [420/630], Loss: 0.0518\n",
      "Epoch [7/10], Step [430/630], Loss: 0.0144\n",
      "Epoch [7/10], Step [430/630], Loss: 0.0144\n",
      "Epoch [7/10], Step [440/630], Loss: 0.0672\n",
      "Epoch [7/10], Step [440/630], Loss: 0.0672\n",
      "Epoch [7/10], Step [450/630], Loss: 0.0997\n",
      "Epoch [7/10], Step [450/630], Loss: 0.0997\n",
      "Epoch [7/10], Step [460/630], Loss: 0.1295\n",
      "Epoch [7/10], Step [460/630], Loss: 0.1295\n",
      "Epoch [7/10], Step [470/630], Loss: 0.0000\n",
      "Epoch [7/10], Step [470/630], Loss: 0.0000\n",
      "Epoch [7/10], Step [480/630], Loss: 0.0106\n",
      "Epoch [7/10], Step [480/630], Loss: 0.0106\n",
      "Epoch [7/10], Step [490/630], Loss: 0.2155\n",
      "Epoch [7/10], Step [490/630], Loss: 0.2155\n",
      "Epoch [7/10], Step [500/630], Loss: 0.0602\n",
      "Epoch [7/10], Step [500/630], Loss: 0.0602\n",
      "Epoch [7/10], Step [510/630], Loss: 0.0159\n",
      "Epoch [7/10], Step [510/630], Loss: 0.0159\n",
      "Epoch [7/10], Step [520/630], Loss: 0.0622\n",
      "Epoch [7/10], Step [520/630], Loss: 0.0622\n",
      "Epoch [7/10], Step [530/630], Loss: 0.0385\n",
      "Epoch [7/10], Step [530/630], Loss: 0.0385\n",
      "Epoch [7/10], Step [540/630], Loss: 0.0579\n",
      "Epoch [7/10], Step [540/630], Loss: 0.0579\n",
      "Epoch [7/10], Step [550/630], Loss: 0.0168\n",
      "Epoch [7/10], Step [550/630], Loss: 0.0168\n",
      "Epoch [7/10], Step [560/630], Loss: 0.0432\n",
      "Epoch [7/10], Step [560/630], Loss: 0.0432\n",
      "Epoch [7/10], Step [570/630], Loss: 0.0066\n",
      "Epoch [7/10], Step [570/630], Loss: 0.0066\n",
      "Epoch [7/10], Step [580/630], Loss: 0.0385\n",
      "Epoch [7/10], Step [580/630], Loss: 0.0385\n",
      "Epoch [7/10], Step [590/630], Loss: 0.1127\n",
      "Epoch [7/10], Step [590/630], Loss: 0.1127\n",
      "Epoch [7/10], Step [600/630], Loss: 0.0862\n",
      "Epoch [7/10], Step [600/630], Loss: 0.0862\n",
      "Epoch [7/10], Step [610/630], Loss: 0.0654\n",
      "Epoch [7/10], Step [610/630], Loss: 0.0654\n",
      "Epoch [7/10], Step [620/630], Loss: 0.0000\n",
      "Epoch [7/10], Step [620/630], Loss: 0.0000\n",
      "Epoch [7/10], Step [630/630], Loss: 0.1372\n",
      "--- Epoch 7 Summary ---\n",
      "Average Loss: 0.0454\n",
      "\n",
      "Epoch [7/10], Step [630/630], Loss: 0.1372\n",
      "--- Epoch 7 Summary ---\n",
      "Average Loss: 0.0454\n",
      "\n",
      "Epoch [8/10], Step [10/630], Loss: 0.0789\n",
      "Epoch [8/10], Step [10/630], Loss: 0.0789\n",
      "Epoch [8/10], Step [20/630], Loss: 0.0637\n",
      "Epoch [8/10], Step [20/630], Loss: 0.0637\n",
      "Epoch [8/10], Step [30/630], Loss: 0.0287\n",
      "Epoch [8/10], Step [30/630], Loss: 0.0287\n",
      "Epoch [8/10], Step [40/630], Loss: 0.1284\n",
      "Epoch [8/10], Step [40/630], Loss: 0.1284\n",
      "Epoch [8/10], Step [50/630], Loss: 0.0355\n",
      "Epoch [8/10], Step [50/630], Loss: 0.0355\n",
      "Epoch [8/10], Step [60/630], Loss: 0.0028\n",
      "Epoch [8/10], Step [60/630], Loss: 0.0028\n",
      "Epoch [8/10], Step [70/630], Loss: 0.1490\n",
      "Epoch [8/10], Step [70/630], Loss: 0.1490\n",
      "Epoch [8/10], Step [80/630], Loss: 0.0000\n",
      "Epoch [8/10], Step [80/630], Loss: 0.0000\n",
      "Epoch [8/10], Step [90/630], Loss: 0.0420\n",
      "Epoch [8/10], Step [90/630], Loss: 0.0420\n",
      "Epoch [8/10], Step [100/630], Loss: 0.0000\n",
      "Epoch [8/10], Step [100/630], Loss: 0.0000\n",
      "Epoch [8/10], Step [110/630], Loss: 0.0217\n",
      "Epoch [8/10], Step [110/630], Loss: 0.0217\n",
      "Epoch [8/10], Step [120/630], Loss: 0.0432\n",
      "Epoch [8/10], Step [120/630], Loss: 0.0432\n",
      "Epoch [8/10], Step [130/630], Loss: 0.0342\n",
      "Epoch [8/10], Step [130/630], Loss: 0.0342\n",
      "Epoch [8/10], Step [140/630], Loss: 0.0660\n",
      "Epoch [8/10], Step [140/630], Loss: 0.0660\n",
      "Epoch [8/10], Step [150/630], Loss: 0.0423\n",
      "Epoch [8/10], Step [150/630], Loss: 0.0423\n",
      "Epoch [8/10], Step [160/630], Loss: 0.0148\n",
      "Epoch [8/10], Step [160/630], Loss: 0.0148\n",
      "Epoch [8/10], Step [170/630], Loss: 0.0000\n",
      "Epoch [8/10], Step [170/630], Loss: 0.0000\n",
      "Epoch [8/10], Step [180/630], Loss: 0.0000\n",
      "Epoch [8/10], Step [180/630], Loss: 0.0000\n",
      "Epoch [8/10], Step [190/630], Loss: 0.0737\n",
      "Epoch [8/10], Step [190/630], Loss: 0.0737\n",
      "Epoch [8/10], Step [200/630], Loss: 0.0000\n",
      "Epoch [8/10], Step [200/630], Loss: 0.0000\n",
      "Epoch [8/10], Step [210/630], Loss: 0.0734\n",
      "Epoch [8/10], Step [210/630], Loss: 0.0734\n",
      "Epoch [8/10], Step [220/630], Loss: 0.0077\n",
      "Epoch [8/10], Step [220/630], Loss: 0.0077\n",
      "Epoch [8/10], Step [230/630], Loss: 0.0012\n",
      "Epoch [8/10], Step [230/630], Loss: 0.0012\n",
      "Epoch [8/10], Step [240/630], Loss: 0.0158\n",
      "Epoch [8/10], Step [240/630], Loss: 0.0158\n",
      "Epoch [8/10], Step [250/630], Loss: 0.0000\n",
      "Epoch [8/10], Step [250/630], Loss: 0.0000\n",
      "Epoch [8/10], Step [260/630], Loss: 0.2490\n",
      "Epoch [8/10], Step [260/630], Loss: 0.2490\n",
      "Epoch [8/10], Step [270/630], Loss: 0.0820\n",
      "Epoch [8/10], Step [270/630], Loss: 0.0820\n",
      "Epoch [8/10], Step [280/630], Loss: 0.0507\n",
      "Epoch [8/10], Step [280/630], Loss: 0.0507\n",
      "Epoch [8/10], Step [290/630], Loss: 0.1012\n",
      "Epoch [8/10], Step [290/630], Loss: 0.1012\n",
      "Epoch [8/10], Step [300/630], Loss: 0.0255\n",
      "Epoch [8/10], Step [300/630], Loss: 0.0255\n",
      "Epoch [8/10], Step [310/630], Loss: 0.0591\n",
      "Epoch [8/10], Step [310/630], Loss: 0.0591\n",
      "Epoch [8/10], Step [320/630], Loss: 0.0744\n",
      "Epoch [8/10], Step [320/630], Loss: 0.0744\n",
      "Epoch [8/10], Step [330/630], Loss: 0.0386\n",
      "Epoch [8/10], Step [330/630], Loss: 0.0386\n",
      "Epoch [8/10], Step [340/630], Loss: 0.0678\n",
      "Epoch [8/10], Step [340/630], Loss: 0.0678\n",
      "Epoch [8/10], Step [350/630], Loss: 0.0143\n",
      "Epoch [8/10], Step [350/630], Loss: 0.0143\n",
      "Epoch [8/10], Step [360/630], Loss: 0.1691\n",
      "Epoch [8/10], Step [360/630], Loss: 0.1691\n",
      "Epoch [8/10], Step [370/630], Loss: 0.1681\n",
      "Epoch [8/10], Step [370/630], Loss: 0.1681\n",
      "Epoch [8/10], Step [380/630], Loss: 0.0510\n",
      "Epoch [8/10], Step [380/630], Loss: 0.0510\n",
      "Epoch [8/10], Step [390/630], Loss: 0.0708\n",
      "Epoch [8/10], Step [390/630], Loss: 0.0708\n",
      "Epoch [8/10], Step [400/630], Loss: 0.0325\n",
      "Epoch [8/10], Step [400/630], Loss: 0.0325\n",
      "Epoch [8/10], Step [410/630], Loss: 0.0726\n",
      "Epoch [8/10], Step [410/630], Loss: 0.0726\n",
      "Epoch [8/10], Step [420/630], Loss: 0.0617\n",
      "Epoch [8/10], Step [420/630], Loss: 0.0617\n",
      "Epoch [8/10], Step [430/630], Loss: 0.0000\n",
      "Epoch [8/10], Step [430/630], Loss: 0.0000\n",
      "Epoch [8/10], Step [440/630], Loss: 0.0249\n",
      "Epoch [8/10], Step [440/630], Loss: 0.0249\n",
      "Epoch [8/10], Step [450/630], Loss: 0.0502\n",
      "Epoch [8/10], Step [450/630], Loss: 0.0502\n",
      "Epoch [8/10], Step [460/630], Loss: 0.0000\n",
      "Epoch [8/10], Step [460/630], Loss: 0.0000\n",
      "Epoch [8/10], Step [470/630], Loss: 0.0166\n",
      "Epoch [8/10], Step [470/630], Loss: 0.0166\n",
      "Epoch [8/10], Step [480/630], Loss: 0.1234\n",
      "Epoch [8/10], Step [480/630], Loss: 0.1234\n",
      "Epoch [8/10], Step [490/630], Loss: 0.0022\n",
      "Epoch [8/10], Step [490/630], Loss: 0.0022\n",
      "Epoch [8/10], Step [500/630], Loss: 0.0276\n",
      "Epoch [8/10], Step [500/630], Loss: 0.0276\n",
      "Epoch [8/10], Step [510/630], Loss: 0.2903\n",
      "Epoch [8/10], Step [510/630], Loss: 0.2903\n",
      "Epoch [8/10], Step [520/630], Loss: 0.1537\n",
      "Epoch [8/10], Step [520/630], Loss: 0.1537\n",
      "Epoch [8/10], Step [530/630], Loss: 0.0288\n",
      "Epoch [8/10], Step [530/630], Loss: 0.0288\n",
      "Epoch [8/10], Step [540/630], Loss: 0.0019\n",
      "Epoch [8/10], Step [540/630], Loss: 0.0019\n",
      "Epoch [8/10], Step [550/630], Loss: 0.0000\n",
      "Epoch [8/10], Step [550/630], Loss: 0.0000\n",
      "Epoch [8/10], Step [560/630], Loss: 0.0183\n",
      "Epoch [8/10], Step [560/630], Loss: 0.0183\n",
      "Epoch [8/10], Step [570/630], Loss: 0.0638\n",
      "Epoch [8/10], Step [570/630], Loss: 0.0638\n",
      "Epoch [8/10], Step [580/630], Loss: 0.0068\n",
      "Epoch [8/10], Step [580/630], Loss: 0.0068\n",
      "Epoch [8/10], Step [590/630], Loss: 0.0021\n",
      "Epoch [8/10], Step [590/630], Loss: 0.0021\n",
      "Epoch [8/10], Step [600/630], Loss: 0.0010\n",
      "Epoch [8/10], Step [600/630], Loss: 0.0010\n",
      "Epoch [8/10], Step [610/630], Loss: 0.0545\n",
      "Epoch [8/10], Step [610/630], Loss: 0.0545\n",
      "Epoch [8/10], Step [620/630], Loss: 0.0154\n",
      "Epoch [8/10], Step [620/630], Loss: 0.0154\n",
      "Epoch [8/10], Step [630/630], Loss: 0.0000\n",
      "--- Epoch 8 Summary ---\n",
      "Average Loss: 0.0432\n",
      "\n",
      "Epoch [8/10], Step [630/630], Loss: 0.0000\n",
      "--- Epoch 8 Summary ---\n",
      "Average Loss: 0.0432\n",
      "\n",
      "Epoch [9/10], Step [10/630], Loss: 0.0380\n",
      "Epoch [9/10], Step [10/630], Loss: 0.0380\n",
      "Epoch [9/10], Step [20/630], Loss: 0.0000\n",
      "Epoch [9/10], Step [20/630], Loss: 0.0000\n",
      "Epoch [9/10], Step [30/630], Loss: 0.0000\n",
      "Epoch [9/10], Step [30/630], Loss: 0.0000\n",
      "Epoch [9/10], Step [40/630], Loss: 0.0430\n",
      "Epoch [9/10], Step [40/630], Loss: 0.0430\n",
      "Epoch [9/10], Step [50/630], Loss: 0.0274\n",
      "Epoch [9/10], Step [50/630], Loss: 0.0274\n",
      "Epoch [9/10], Step [60/630], Loss: 0.0238\n",
      "Epoch [9/10], Step [60/630], Loss: 0.0238\n",
      "Epoch [9/10], Step [70/630], Loss: 0.0074\n",
      "Epoch [9/10], Step [70/630], Loss: 0.0074\n",
      "Epoch [9/10], Step [80/630], Loss: 0.0065\n",
      "Epoch [9/10], Step [80/630], Loss: 0.0065\n",
      "Epoch [9/10], Step [90/630], Loss: 0.0648\n",
      "Epoch [9/10], Step [90/630], Loss: 0.0648\n",
      "Epoch [9/10], Step [100/630], Loss: 0.0449\n",
      "Epoch [9/10], Step [100/630], Loss: 0.0449\n",
      "Epoch [9/10], Step [110/630], Loss: 0.0046\n",
      "Epoch [9/10], Step [110/630], Loss: 0.0046\n",
      "Epoch [9/10], Step [120/630], Loss: 0.0699\n",
      "Epoch [9/10], Step [120/630], Loss: 0.0699\n",
      "Epoch [9/10], Step [130/630], Loss: 0.0000\n",
      "Epoch [9/10], Step [130/630], Loss: 0.0000\n",
      "Epoch [9/10], Step [140/630], Loss: 0.0000\n",
      "Epoch [9/10], Step [140/630], Loss: 0.0000\n",
      "Epoch [9/10], Step [150/630], Loss: 0.0000\n",
      "Epoch [9/10], Step [150/630], Loss: 0.0000\n",
      "Epoch [9/10], Step [160/630], Loss: 0.0115\n",
      "Epoch [9/10], Step [160/630], Loss: 0.0115\n",
      "Epoch [9/10], Step [170/630], Loss: 0.1033\n",
      "Epoch [9/10], Step [170/630], Loss: 0.1033\n",
      "Epoch [9/10], Step [180/630], Loss: 0.0289\n",
      "Epoch [9/10], Step [180/630], Loss: 0.0289\n",
      "Epoch [9/10], Step [190/630], Loss: 0.0030\n",
      "Epoch [9/10], Step [190/630], Loss: 0.0030\n",
      "Epoch [9/10], Step [200/630], Loss: 0.0107\n",
      "Epoch [9/10], Step [200/630], Loss: 0.0107\n",
      "Epoch [9/10], Step [210/630], Loss: 0.0269\n",
      "Epoch [9/10], Step [210/630], Loss: 0.0269\n",
      "Epoch [9/10], Step [220/630], Loss: 0.0473\n",
      "Epoch [9/10], Step [220/630], Loss: 0.0473\n",
      "Epoch [9/10], Step [230/630], Loss: 0.0110\n",
      "Epoch [9/10], Step [230/630], Loss: 0.0110\n",
      "Epoch [9/10], Step [240/630], Loss: 0.0000\n",
      "Epoch [9/10], Step [240/630], Loss: 0.0000\n",
      "Epoch [9/10], Step [250/630], Loss: 0.0029\n",
      "Epoch [9/10], Step [250/630], Loss: 0.0029\n",
      "Epoch [9/10], Step [260/630], Loss: 0.0231\n",
      "Epoch [9/10], Step [260/630], Loss: 0.0231\n",
      "Epoch [9/10], Step [270/630], Loss: 0.0358\n",
      "Epoch [9/10], Step [270/630], Loss: 0.0358\n",
      "Epoch [9/10], Step [280/630], Loss: 0.0204\n",
      "Epoch [9/10], Step [280/630], Loss: 0.0204\n",
      "Epoch [9/10], Step [290/630], Loss: 0.0088\n",
      "Epoch [9/10], Step [290/630], Loss: 0.0088\n",
      "Epoch [9/10], Step [300/630], Loss: 0.0000\n",
      "Epoch [9/10], Step [300/630], Loss: 0.0000\n",
      "Epoch [9/10], Step [310/630], Loss: 0.0426\n",
      "Epoch [9/10], Step [310/630], Loss: 0.0426\n",
      "Epoch [9/10], Step [320/630], Loss: 0.0540\n",
      "Epoch [9/10], Step [320/630], Loss: 0.0540\n",
      "Epoch [9/10], Step [330/630], Loss: 0.0975\n",
      "Epoch [9/10], Step [330/630], Loss: 0.0975\n",
      "Epoch [9/10], Step [340/630], Loss: 0.0657\n",
      "Epoch [9/10], Step [340/630], Loss: 0.0657\n",
      "Epoch [9/10], Step [350/630], Loss: 0.0779\n",
      "Epoch [9/10], Step [350/630], Loss: 0.0779\n",
      "Epoch [9/10], Step [360/630], Loss: 0.0057\n",
      "Epoch [9/10], Step [360/630], Loss: 0.0057\n",
      "Epoch [9/10], Step [370/630], Loss: 0.0364\n",
      "Epoch [9/10], Step [370/630], Loss: 0.0364\n",
      "Epoch [9/10], Step [380/630], Loss: 0.0361\n",
      "Epoch [9/10], Step [380/630], Loss: 0.0361\n",
      "Epoch [9/10], Step [390/630], Loss: 0.0225\n",
      "Epoch [9/10], Step [390/630], Loss: 0.0225\n",
      "Epoch [9/10], Step [400/630], Loss: 0.0156\n",
      "Epoch [9/10], Step [400/630], Loss: 0.0156\n",
      "Epoch [9/10], Step [410/630], Loss: 0.0000\n",
      "Epoch [9/10], Step [410/630], Loss: 0.0000\n",
      "Epoch [9/10], Step [420/630], Loss: 0.1195\n",
      "Epoch [9/10], Step [420/630], Loss: 0.1195\n",
      "Epoch [9/10], Step [430/630], Loss: 0.0000\n",
      "Epoch [9/10], Step [430/630], Loss: 0.0000\n",
      "Epoch [9/10], Step [440/630], Loss: 0.1013\n",
      "Epoch [9/10], Step [440/630], Loss: 0.1013\n",
      "Epoch [9/10], Step [450/630], Loss: 0.0000\n",
      "Epoch [9/10], Step [450/630], Loss: 0.0000\n",
      "Epoch [9/10], Step [460/630], Loss: 0.1648\n",
      "Epoch [9/10], Step [460/630], Loss: 0.1648\n",
      "Epoch [9/10], Step [470/630], Loss: 0.0323\n",
      "Epoch [9/10], Step [470/630], Loss: 0.0323\n",
      "Epoch [9/10], Step [480/630], Loss: 0.0000\n",
      "Epoch [9/10], Step [480/630], Loss: 0.0000\n",
      "Epoch [9/10], Step [490/630], Loss: 0.0117\n",
      "Epoch [9/10], Step [490/630], Loss: 0.0117\n",
      "Epoch [9/10], Step [500/630], Loss: 0.0079\n",
      "Epoch [9/10], Step [500/630], Loss: 0.0079\n",
      "Epoch [9/10], Step [510/630], Loss: 0.0048\n",
      "Epoch [9/10], Step [510/630], Loss: 0.0048\n",
      "Epoch [9/10], Step [520/630], Loss: 0.0295\n",
      "Epoch [9/10], Step [520/630], Loss: 0.0295\n",
      "Epoch [9/10], Step [530/630], Loss: 0.0084\n",
      "Epoch [9/10], Step [530/630], Loss: 0.0084\n",
      "Epoch [9/10], Step [540/630], Loss: 0.0353\n",
      "Epoch [9/10], Step [540/630], Loss: 0.0353\n",
      "Epoch [9/10], Step [550/630], Loss: 0.0388\n",
      "Epoch [9/10], Step [550/630], Loss: 0.0388\n",
      "Epoch [9/10], Step [560/630], Loss: 0.0660\n",
      "Epoch [9/10], Step [560/630], Loss: 0.0660\n",
      "Epoch [9/10], Step [570/630], Loss: 0.0782\n",
      "Epoch [9/10], Step [570/630], Loss: 0.0782\n",
      "Epoch [9/10], Step [580/630], Loss: 0.0269\n",
      "Epoch [9/10], Step [580/630], Loss: 0.0269\n",
      "Epoch [9/10], Step [590/630], Loss: 0.0258\n",
      "Epoch [9/10], Step [590/630], Loss: 0.0258\n",
      "Epoch [9/10], Step [600/630], Loss: 0.0556\n",
      "Epoch [9/10], Step [600/630], Loss: 0.0556\n",
      "Epoch [9/10], Step [610/630], Loss: 0.1347\n",
      "Epoch [9/10], Step [610/630], Loss: 0.1347\n",
      "Epoch [9/10], Step [620/630], Loss: 0.0000\n",
      "Epoch [9/10], Step [620/630], Loss: 0.0000\n",
      "Epoch [9/10], Step [630/630], Loss: 0.0415\n",
      "--- Epoch 9 Summary ---\n",
      "Average Loss: 0.0369\n",
      "\n",
      "Epoch [9/10], Step [630/630], Loss: 0.0415\n",
      "--- Epoch 9 Summary ---\n",
      "Average Loss: 0.0369\n",
      "\n",
      "Epoch [10/10], Step [10/630], Loss: 0.0337\n",
      "Epoch [10/10], Step [10/630], Loss: 0.0337\n",
      "Epoch [10/10], Step [20/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [20/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [30/630], Loss: 0.0184\n",
      "Epoch [10/10], Step [30/630], Loss: 0.0184\n",
      "Epoch [10/10], Step [40/630], Loss: 0.0097\n",
      "Epoch [10/10], Step [40/630], Loss: 0.0097\n",
      "Epoch [10/10], Step [50/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [50/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [60/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [60/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [70/630], Loss: 0.0106\n",
      "Epoch [10/10], Step [70/630], Loss: 0.0106\n",
      "Epoch [10/10], Step [80/630], Loss: 0.0455\n",
      "Epoch [10/10], Step [80/630], Loss: 0.0455\n",
      "Epoch [10/10], Step [90/630], Loss: 0.0038\n",
      "Epoch [10/10], Step [90/630], Loss: 0.0038\n",
      "Epoch [10/10], Step [100/630], Loss: 0.0078\n",
      "Epoch [10/10], Step [100/630], Loss: 0.0078\n",
      "Epoch [10/10], Step [110/630], Loss: 0.0131\n",
      "Epoch [10/10], Step [110/630], Loss: 0.0131\n",
      "Epoch [10/10], Step [120/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [120/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [130/630], Loss: 0.0045\n",
      "Epoch [10/10], Step [130/630], Loss: 0.0045\n",
      "Epoch [10/10], Step [140/630], Loss: 0.2934\n",
      "Epoch [10/10], Step [140/630], Loss: 0.2934\n",
      "Epoch [10/10], Step [150/630], Loss: 0.0207\n",
      "Epoch [10/10], Step [150/630], Loss: 0.0207\n",
      "Epoch [10/10], Step [160/630], Loss: 0.0547\n",
      "Epoch [10/10], Step [160/630], Loss: 0.0547\n",
      "Epoch [10/10], Step [170/630], Loss: 0.0179\n",
      "Epoch [10/10], Step [170/630], Loss: 0.0179\n",
      "Epoch [10/10], Step [180/630], Loss: 0.0429\n",
      "Epoch [10/10], Step [180/630], Loss: 0.0429\n",
      "Epoch [10/10], Step [190/630], Loss: 0.0415\n",
      "Epoch [10/10], Step [190/630], Loss: 0.0415\n",
      "Epoch [10/10], Step [200/630], Loss: 0.0224\n",
      "Epoch [10/10], Step [200/630], Loss: 0.0224\n",
      "Epoch [10/10], Step [210/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [210/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [220/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [220/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [230/630], Loss: 0.0028\n",
      "Epoch [10/10], Step [230/630], Loss: 0.0028\n",
      "Epoch [10/10], Step [240/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [240/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [250/630], Loss: 0.0470\n",
      "Epoch [10/10], Step [250/630], Loss: 0.0470\n",
      "Epoch [10/10], Step [260/630], Loss: 0.0028\n",
      "Epoch [10/10], Step [260/630], Loss: 0.0028\n",
      "Epoch [10/10], Step [270/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [270/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [280/630], Loss: 0.0540\n",
      "Epoch [10/10], Step [280/630], Loss: 0.0540\n",
      "Epoch [10/10], Step [290/630], Loss: 0.0078\n",
      "Epoch [10/10], Step [290/630], Loss: 0.0078\n",
      "Epoch [10/10], Step [300/630], Loss: 0.0154\n",
      "Epoch [10/10], Step [300/630], Loss: 0.0154\n",
      "Epoch [10/10], Step [310/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [310/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [320/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [320/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [330/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [330/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [340/630], Loss: 0.0979\n",
      "Epoch [10/10], Step [340/630], Loss: 0.0979\n",
      "Epoch [10/10], Step [350/630], Loss: 0.1132\n",
      "Epoch [10/10], Step [350/630], Loss: 0.1132\n",
      "Epoch [10/10], Step [360/630], Loss: 0.0603\n",
      "Epoch [10/10], Step [360/630], Loss: 0.0603\n",
      "Epoch [10/10], Step [370/630], Loss: 0.0141\n",
      "Epoch [10/10], Step [370/630], Loss: 0.0141\n",
      "Epoch [10/10], Step [380/630], Loss: 0.0125\n",
      "Epoch [10/10], Step [380/630], Loss: 0.0125\n",
      "Epoch [10/10], Step [390/630], Loss: 0.0027\n",
      "Epoch [10/10], Step [390/630], Loss: 0.0027\n",
      "Epoch [10/10], Step [400/630], Loss: 0.0049\n",
      "Epoch [10/10], Step [400/630], Loss: 0.0049\n",
      "Epoch [10/10], Step [410/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [410/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [420/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [420/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [430/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [430/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [440/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [440/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [450/630], Loss: 0.0153\n",
      "Epoch [10/10], Step [450/630], Loss: 0.0153\n",
      "Epoch [10/10], Step [460/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [460/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [470/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [470/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [480/630], Loss: 0.0261\n",
      "Epoch [10/10], Step [480/630], Loss: 0.0261\n",
      "Epoch [10/10], Step [490/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [490/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [500/630], Loss: 0.0454\n",
      "Epoch [10/10], Step [500/630], Loss: 0.0454\n",
      "Epoch [10/10], Step [510/630], Loss: 0.0933\n",
      "Epoch [10/10], Step [510/630], Loss: 0.0933\n",
      "Epoch [10/10], Step [520/630], Loss: 0.0471\n",
      "Epoch [10/10], Step [520/630], Loss: 0.0471\n",
      "Epoch [10/10], Step [530/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [530/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [540/630], Loss: 0.0226\n",
      "Epoch [10/10], Step [540/630], Loss: 0.0226\n",
      "Epoch [10/10], Step [550/630], Loss: 0.0408\n",
      "Epoch [10/10], Step [550/630], Loss: 0.0408\n",
      "Epoch [10/10], Step [560/630], Loss: 0.0202\n",
      "Epoch [10/10], Step [560/630], Loss: 0.0202\n",
      "Epoch [10/10], Step [570/630], Loss: 0.0630\n",
      "Epoch [10/10], Step [570/630], Loss: 0.0630\n",
      "Epoch [10/10], Step [580/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [580/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [590/630], Loss: 0.1194\n",
      "Epoch [10/10], Step [590/630], Loss: 0.1194\n",
      "Epoch [10/10], Step [600/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [600/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [610/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [610/630], Loss: 0.0000\n",
      "Epoch [10/10], Step [620/630], Loss: 0.0418\n",
      "Epoch [10/10], Step [620/630], Loss: 0.0418\n",
      "Epoch [10/10], Step [630/630], Loss: 0.0000\n",
      "--- Epoch 10 Summary ---\n",
      "Average Loss: 0.0343\n",
      "\n",
      "Finished Training\n",
      "Model saved to lacrosse_reid_model.pth\n",
      "Epoch [10/10], Step [630/630], Loss: 0.0000\n",
      "--- Epoch 10 Summary ---\n",
      "Average Loss: 0.0343\n",
      "\n",
      "Finished Training\n",
      "Model saved to lacrosse_reid_model.pth\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, dataloader, optimizer, loss_fn, num_epochs=20):\n",
    "    \"\"\"The main training loop\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        batch_count = 0\n",
    "        \n",
    "        for i, (anchor, positive, negative, _) in enumerate(dataloader):\n",
    "            anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            emb_anchor, emb_positive, emb_negative = model.forward_triplet(anchor, positive, negative)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = loss_fn(emb_anchor, emb_positive, emb_negative)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            batch_count += 1\n",
    "            if (i + 1) % 10 == 0:  # Reduced frequency for notebook\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        if batch_count > 0:\n",
    "            epoch_loss = running_loss / batch_count\n",
    "        else:\n",
    "            epoch_loss = 0.0\n",
    "        print(f\"--- Epoch {epoch+1} Summary ---\")\n",
    "        print(f\"Average Loss: {epoch_loss:.4f}\\n\")\n",
    "        \n",
    "    print(\"Finished Training\")\n",
    "    return model\n",
    "\n",
    "# --- Main training block for notebook ---\n",
    "# Hyperparameters\n",
    "TRAIN_DIR = 'data/train'  # Fixed path\n",
    "EMBEDDING_DIM = 128\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 16  # Reduced batch size for stability\n",
    "NUM_EPOCHS = 10   # Reduced epochs for testing\n",
    "MARGIN = 0.5 # Margin for the triplet loss\n",
    "\n",
    "# Check if train directory exists and has data\n",
    "if not os.path.exists(TRAIN_DIR):\n",
    "    print(f\"Error: {TRAIN_DIR} directory does not exist!\")\n",
    "    print(\"Available directories:\", [d for d in os.listdir('.') if os.path.isdir(d)])\n",
    "else:\n",
    "    train_folders = [d for d in os.listdir(TRAIN_DIR) if os.path.isdir(os.path.join(TRAIN_DIR, d))]\n",
    "    print(f\"Found {len(train_folders)} player folders in {TRAIN_DIR}\")\n",
    "    \n",
    "    if len(train_folders) < 2:\n",
    "        print(\"Error: Need at least 2 player folders for triplet loss training!\")\n",
    "    else:\n",
    "        # 1. Setup Dataset and DataLoader\n",
    "        train_dataset = LacrossePlayerDataset(image_dir=TRAIN_DIR, transform=data_transforms)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)  # Fixed num_workers\n",
    "        \n",
    "        print(f\"Dataset size: {len(train_dataset)} images\")\n",
    "        print(f\"Number of batches: {len(train_dataloader)}\")\n",
    "        \n",
    "        # 2. Initialize Model, Loss, and Optimizer\n",
    "        siamese_model = SiameseNet(embedding_dim=EMBEDDING_DIM)\n",
    "        triplet_loss = nn.TripletMarginLoss(margin=MARGIN, p=2)\n",
    "        optimizer = torch.optim.Adam(siamese_model.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        # 3. Start Training\n",
    "        trained_model = train_model(siamese_model, train_dataloader, optimizer, triplet_loss, num_epochs=NUM_EPOCHS)\n",
    "        \n",
    "        # 4. Save the trained model\n",
    "        torch.save(trained_model.state_dict(), 'lacrosse_reid_model.pth')\n",
    "        print(\"Model saved to lacrosse_reid_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7a184e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All crops copied to 'data/all_unfiltered_crops' (flat, ungrouped).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "src_data_dir = \"data/orig\"\n",
    "all_crops_dir = os.path.join(\"data\", \"all_unfiltered_crops\")\n",
    "os.makedirs(all_crops_dir, exist_ok=True)\n",
    "\n",
    "for track_id in os.listdir(src_data_dir):\n",
    "    track_path = os.path.join(src_data_dir, track_id)\n",
    "    if not os.path.isdir(track_path) or track_id == \"all_unfiltered_crops\":\n",
    "        continue\n",
    "    for fname in os.listdir(track_path):\n",
    "        if fname.lower().endswith(('.jpg', '.png')):\n",
    "            src = os.path.join(track_path, fname)\n",
    "            dst = os.path.join(all_crops_dir, fname)\n",
    "            # If duplicate filenames exist, add track_id as prefix\n",
    "            if os.path.exists(dst):\n",
    "                dst = os.path.join(all_crops_dir, f\"{track_id}_{fname}\")\n",
    "            shutil.copy2(src, dst)\n",
    "\n",
    "print(f\"All crops copied to '{all_crops_dir}' (flat, ungrouped).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb458655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Starting Player Re-identification Clustering Pipeline\n",
      "============================================================\n",
      "🖥️  Using device: mps\n",
      "📊 Found 12618 images to process\n",
      "\n",
      "🔄 Loading trained model...\n",
      "✅ Model loaded successfully!\n",
      "\n",
      "⚙️  Setting up inference pipeline...\n",
      "🚀 Generating embeddings for 12618 images...\n",
      "✅ Model loaded successfully!\n",
      "\n",
      "⚙️  Setting up inference pipeline...\n",
      "🚀 Generating embeddings for 12618 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 99/99 [00:09<00:00, 10.40it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embedding generation complete! (9.52s, 12618 images)\n",
      "\n",
      "🧠 Starting DBSCAN clustering on 12618 embeddings...\n",
      "   📊 Embedding dimensions: 128\n",
      "   ⚙️  Parameters: eps=0.05, min_samples=20\n",
      "   🔄 Running DBSCAN algorithm...\n",
      "✨ Clustering complete! (0.12s)\n",
      "   🎯 Found 20 unique players (clusters)\n",
      "   🗑️  5677 images classified as noise (45.0%)\n",
      "   📈 Cluster sizes: min=20, max=1063, avg=347.1\n",
      "\n",
      "📁 Reorganizing data into 'data/clustered_train/'...\n",
      "   🧹 Cleaning up existing directory...\n",
      "   📂 Creating 20 cluster directories...\n",
      "   📋 Copying files to cluster directories...\n",
      "   📂 Creating 20 cluster directories...\n",
      "   📋 Copying files to cluster directories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 100%|██████████| 12618/12618 [00:01<00:00, 9781.58it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully copied 6941 images into 20 cluster folders\n",
      "   📊 Average images per cluster: 347.1\n",
      "\n",
      "🎉 All steps complete!\n",
      "📁 A new, clean dataset is ready for retraining in 'data/clustered_train/'\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "import os\n",
    "import shutil\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Paths\n",
    "INITIAL_MODEL_PATH = 'lacrosse_reid_model.pth'\n",
    "ALL_CROPS_DIR = 'data/all_unfiltered_crops/'\n",
    "CLUSTERED_DATA_DIR = 'data/clustered_train/'\n",
    "\n",
    "# Model & Data Loader Params\n",
    "EMBEDDING_DIM = 128\n",
    "BATCH_SIZE = 128 # Use a larger batch size for fast inference\n",
    "\n",
    "# DBSCAN Hyperparameters (IMPORTANT: You MUST tune these)\n",
    "# eps: The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n",
    "# This is the most critical parameter to tune. Start around 0.5-0.7 and adjust.\n",
    "DBSCAN_EPS = 0.05 \n",
    "# min_samples: The number of samples in a neighborhood for a point to be considered as a core point.\n",
    "DBSCAN_MIN_SAMPLES = 20\n",
    "\n",
    "# --- STEP 1: HELPER DATASET FOR INFERENCE ---\n",
    "# A simple dataset to load individual images for generating embeddings\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, img_path\n",
    "\n",
    "# --- STEP 2: GENERATE EMBEDDINGS ---\n",
    "def generate_all_embeddings(model, dataloader, device):\n",
    "    \"\"\"Uses the model to generate embeddings for all images in the dataloader.\"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    all_embeddings = []\n",
    "    all_paths = []\n",
    "\n",
    "    print(f\"🚀 Generating embeddings for {len(dataloader.dataset)} images...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (images, paths) in enumerate(tqdm(dataloader, desc=\"Generating embeddings\")):\n",
    "            images = images.to(device)\n",
    "            embeddings = model(images)\n",
    "            all_embeddings.append(embeddings.cpu().numpy())\n",
    "            all_paths.extend(paths)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"✅ Embedding generation complete! ({elapsed_time:.2f}s, {len(all_paths)} images)\")\n",
    "    return np.vstack(all_embeddings), all_paths\n",
    "\n",
    "# --- STEP 3: CLUSTER EMBEDDINGS ---\n",
    "def cluster_embeddings(embeddings, eps, min_samples):\n",
    "    \"\"\"Performs DBSCAN clustering on the embedding vectors.\"\"\"\n",
    "    print(f\"\\n🧠 Starting DBSCAN clustering on {embeddings.shape[0]} embeddings...\")\n",
    "    print(f\"   📊 Embedding dimensions: {embeddings.shape[1]}\")\n",
    "    print(f\"   ⚙️  Parameters: eps={eps}, min_samples={min_samples}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    clustering = DBSCAN(eps=eps, min_samples=min_samples, metric='euclidean', n_jobs=-1)\n",
    "    \n",
    "    print(\"   🔄 Running DBSCAN algorithm...\")\n",
    "    cluster_labels = clustering.fit_predict(embeddings)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Print clustering summary\n",
    "    unique_labels = set(cluster_labels)\n",
    "    num_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n",
    "    num_noise = np.sum(cluster_labels == -1)\n",
    "    \n",
    "    print(f\"✨ Clustering complete! ({elapsed_time:.2f}s)\")\n",
    "    print(f\"   🎯 Found {num_clusters} unique players (clusters)\")\n",
    "    print(f\"   🗑️  {num_noise} images classified as noise ({num_noise/len(cluster_labels)*100:.1f}%)\")\n",
    "    \n",
    "    # Show cluster size distribution\n",
    "    if num_clusters > 0:\n",
    "        cluster_counts = Counter(cluster_labels)\n",
    "        if -1 in cluster_counts:\n",
    "            del cluster_counts[-1]  # Remove noise count\n",
    "        \n",
    "        print(f\"   📈 Cluster sizes: min={min(cluster_counts.values())}, max={max(cluster_counts.values())}, avg={np.mean(list(cluster_counts.values())):.1f}\")\n",
    "    \n",
    "    return cluster_labels\n",
    "\n",
    "# --- STEP 4: REORGANIZE DATASET BASED ON CLUSTERS ---\n",
    "def reorganize_data_into_clusters(image_paths, cluster_labels, output_dir):\n",
    "    \"\"\"Copies image files into new folders named after their cluster ID.\"\"\"\n",
    "    print(f\"\\n📁 Reorganizing data into '{output_dir}'...\")\n",
    "    \n",
    "    if os.path.exists(output_dir):\n",
    "        print(\"   🧹 Cleaning up existing directory...\")\n",
    "        shutil.rmtree(output_dir) # Clean up old directory\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "    # Create a mapping from image path to its cluster label\n",
    "    path_to_label = dict(zip(image_paths, cluster_labels))\n",
    "    \n",
    "    # Count clusters and create directories\n",
    "    valid_labels = [label for label in cluster_labels if label != -1]\n",
    "    unique_clusters = set(valid_labels)\n",
    "    \n",
    "    print(f\"   📂 Creating {len(unique_clusters)} cluster directories...\")\n",
    "    for label in unique_clusters:\n",
    "        cluster_dir = os.path.join(output_dir, f\"player_{label:04d}\")\n",
    "        os.makedirs(cluster_dir, exist_ok=True)\n",
    "    \n",
    "    copied_count = 0\n",
    "    print(\"   📋 Copying files to cluster directories...\")\n",
    "    \n",
    "    for path, label in tqdm(path_to_label.items(), desc=\"Copying files\"):\n",
    "        # Ignore noise points (label -1)\n",
    "        if label == -1:\n",
    "            continue\n",
    "            \n",
    "        # Create a directory for the new cluster ID if it doesn't exist\n",
    "        cluster_dir = os.path.join(output_dir, f\"player_{label:04d}\")\n",
    "        \n",
    "        # Copy the file\n",
    "        filename = os.path.basename(path)\n",
    "        dst_path = os.path.join(cluster_dir, filename)\n",
    "        shutil.copy2(path, dst_path)\n",
    "        copied_count += 1\n",
    "        \n",
    "    print(f\"✅ Successfully copied {copied_count} images into {len(unique_clusters)} cluster folders\")\n",
    "    \n",
    "    # Show final statistics\n",
    "    if len(unique_clusters) > 0:\n",
    "        print(f\"   📊 Average images per cluster: {copied_count/len(unique_clusters):.1f}\")\n",
    "\n",
    "\n",
    "# --- MAIN EXECUTION SCRIPT ---\n",
    "\n",
    "print(\"🎯 Starting Player Re-identification Clustering Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"🖥️  Using device: {device}\")\n",
    "\n",
    "# Check if required files exist\n",
    "if not os.path.exists(INITIAL_MODEL_PATH):\n",
    "    print(f\"❌ Error: Model file '{INITIAL_MODEL_PATH}' not found!\")\n",
    "    print(\"   Please ensure the model has been trained and saved first.\")\n",
    "else:\n",
    "    # Create all_unfiltered_crops directory if it doesn't exist\n",
    "    if not os.path.exists(ALL_CROPS_DIR):\n",
    "        print(f\"📂 Creating '{ALL_CROPS_DIR}' directory...\")\n",
    "        os.makedirs(ALL_CROPS_DIR, exist_ok=True)\n",
    "        \n",
    "        # Copy all crops from individual tracker folders to the all_crops directory\n",
    "        data_dir = \"data\"\n",
    "        if os.path.exists(data_dir):\n",
    "            print(\"   🔄 Copying crops from individual tracker folders...\")\n",
    "            for item in os.listdir(data_dir):\n",
    "                item_path = os.path.join(data_dir, item)\n",
    "                if os.path.isdir(item_path) and item.isdigit():  # Only process numbered tracker folders\n",
    "                    for crop_file in os.listdir(item_path):\n",
    "                        if crop_file.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "                            src = os.path.join(item_path, crop_file)\n",
    "                            dst = os.path.join(ALL_CROPS_DIR, f\"{item}_{crop_file}\")  # Prefix with tracker_id\n",
    "                            shutil.copy2(src, dst)\n",
    "    \n",
    "    # Check if we have images to process\n",
    "    if not os.path.exists(ALL_CROPS_DIR) or len(os.listdir(ALL_CROPS_DIR)) == 0:\n",
    "        print(f\"❌ Error: No images found in '{ALL_CROPS_DIR}'!\")\n",
    "        print(\"   Please ensure crop extraction has been completed first.\")\n",
    "    else:\n",
    "        num_images = len([f for f in os.listdir(ALL_CROPS_DIR) if f.lower().endswith(('.jpg', '.png', '.jpeg'))])\n",
    "        print(f\"📊 Found {num_images} images to process\")\n",
    "        \n",
    "        # 1. Load initial model\n",
    "        print(\"\\n🔄 Loading trained model...\")\n",
    "        model = SiameseNet(embedding_dim=EMBEDDING_DIM)\n",
    "        model.load_state_dict(torch.load(INITIAL_MODEL_PATH, map_location=device))\n",
    "        model.to(device)\n",
    "        print(\"✅ Model loaded successfully!\")\n",
    "\n",
    "        # 2. Setup inference pipeline\n",
    "        print(\"\\n⚙️  Setting up inference pipeline...\")\n",
    "        # Use the same transforms as your validation/test set\n",
    "        inference_transforms = transforms.Compose([\n",
    "            transforms.Resize((80, 40)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        inference_dataset = InferenceDataset(image_dir=ALL_CROPS_DIR, transform=inference_transforms)\n",
    "        inference_dataloader = DataLoader(inference_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "        \n",
    "        embeddings_array, image_paths = generate_all_embeddings(model, inference_dataloader, device)\n",
    "\n",
    "        # 3. Cluster the embeddings\n",
    "        cluster_labels = cluster_embeddings(embeddings_array, eps=DBSCAN_EPS, min_samples=DBSCAN_MIN_SAMPLES)\n",
    "        \n",
    "        # 4. Reorganize files into a new, clean training directory\n",
    "        reorganize_data_into_clusters(image_paths, cluster_labels, CLUSTERED_DATA_DIR)\n",
    "        \n",
    "        print(\"\\n🎉 All steps complete!\")\n",
    "        print(f\"📁 A new, clean dataset is ready for retraining in '{CLUSTERED_DATA_DIR}'\")\n",
    "        print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "820b01ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Crops split into 'data/clustered_train/train' and 'data/clustered_train/val' with per-track structure.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import random\n",
    "import os\n",
    "\n",
    "random.seed(42)  # For reproducibility\n",
    "\n",
    "src_data_dir = \"data/clustered_train\"\n",
    "train_dir = \"data/clustered_train/train\"\n",
    "val_dir = \"data/clustered_train/val\"\n",
    "\n",
    "# Create train and val directories\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# For each tracker_id directory in data/\n",
    "for track_id in os.listdir(src_data_dir):\n",
    "    track_path = os.path.join(src_data_dir, track_id)\n",
    "    if not os.path.isdir(track_path):\n",
    "        continue\n",
    "\n",
    "    # List all crop files for this track\n",
    "    crop_files = [f for f in os.listdir(track_path) if f.endswith('.jpg')]\n",
    "    random.shuffle(crop_files)\n",
    "\n",
    "    split_idx = int(0.8 * len(crop_files))\n",
    "    train_files = crop_files[:split_idx]\n",
    "    val_files = crop_files[split_idx:]\n",
    "\n",
    "    # Create per-track folders in train/ and val/\n",
    "    train_track_dir = os.path.join(train_dir, track_id)\n",
    "    val_track_dir = os.path.join(val_dir, track_id)\n",
    "    os.makedirs(train_track_dir, exist_ok=True)\n",
    "    os.makedirs(val_track_dir, exist_ok=True)\n",
    "\n",
    "    # Copy files\n",
    "    for fname in train_files:\n",
    "        src = os.path.join(track_path, fname)\n",
    "        dst = os.path.join(train_track_dir, fname)\n",
    "        shutil.copy2(src, dst)\n",
    "\n",
    "    for fname in val_files:\n",
    "        src = os.path.join(track_path, fname)\n",
    "        dst = os.path.join(val_track_dir, fname)\n",
    "        shutil.copy2(src, dst)\n",
    "\n",
    "print(f\"Done! Crops split into '{train_dir}' and '{val_dir}' with per-track structure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af6857ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 player folders in data/clustered_train/train\n",
      "Dataset initialized with 20 players and 5546 total images\n",
      "  Player player_0008: 100 images\n",
      "  Player player_0006: 529 images\n",
      "  Player player_0001: 350 images\n",
      "  Player player_0000: 788 images\n",
      "  Player player_0007: 215 images\n",
      "  Player player_0009: 324 images\n",
      "  Player player_0014: 86 images\n",
      "  Player player_0013: 128 images\n",
      "  Player player_0012: 90 images\n",
      "  Player player_0015: 45 images\n",
      "  Player player_0002: 486 images\n",
      "  Player player_0005: 323 images\n",
      "  Player player_0004: 850 images\n",
      "  Player player_0003: 828 images\n",
      "  Player player_0010: 105 images\n",
      "  Player player_0017: 42 images\n",
      "  Player player_0019: 16 images\n",
      "  Player player_0018: 29 images\n",
      "  Player player_0016: 16 images\n",
      "  Player player_0011: 196 images\n",
      "Dataset size: 5546 images\n",
      "Number of batches: 347\n",
      "Epoch [1/10], Step [10/347], Loss: 0.2367\n",
      "Epoch [1/10], Step [10/347], Loss: 0.2367\n",
      "Epoch [1/10], Step [20/347], Loss: 0.3118\n",
      "Epoch [1/10], Step [20/347], Loss: 0.3118\n",
      "Epoch [1/10], Step [30/347], Loss: 0.2691\n",
      "Epoch [1/10], Step [30/347], Loss: 0.2691\n",
      "Epoch [1/10], Step [40/347], Loss: 0.1409\n",
      "Epoch [1/10], Step [40/347], Loss: 0.1409\n",
      "Epoch [1/10], Step [50/347], Loss: 0.1309\n",
      "Epoch [1/10], Step [50/347], Loss: 0.1309\n",
      "Epoch [1/10], Step [60/347], Loss: 0.1259\n",
      "Epoch [1/10], Step [60/347], Loss: 0.1259\n",
      "Epoch [1/10], Step [70/347], Loss: 0.1676\n",
      "Epoch [1/10], Step [70/347], Loss: 0.1676\n",
      "Epoch [1/10], Step [80/347], Loss: 0.4044\n",
      "Epoch [1/10], Step [80/347], Loss: 0.4044\n",
      "Epoch [1/10], Step [90/347], Loss: 0.2842\n",
      "Epoch [1/10], Step [90/347], Loss: 0.2842\n",
      "Epoch [1/10], Step [100/347], Loss: 0.2124\n",
      "Epoch [1/10], Step [100/347], Loss: 0.2124\n",
      "Epoch [1/10], Step [110/347], Loss: 0.2007\n",
      "Epoch [1/10], Step [110/347], Loss: 0.2007\n",
      "Epoch [1/10], Step [120/347], Loss: 0.1164\n",
      "Epoch [1/10], Step [120/347], Loss: 0.1164\n",
      "Epoch [1/10], Step [130/347], Loss: 0.0883\n",
      "Epoch [1/10], Step [130/347], Loss: 0.0883\n",
      "Epoch [1/10], Step [140/347], Loss: 0.0796\n",
      "Epoch [1/10], Step [140/347], Loss: 0.0796\n",
      "Epoch [1/10], Step [150/347], Loss: 0.0469\n",
      "Epoch [1/10], Step [150/347], Loss: 0.0469\n",
      "Epoch [1/10], Step [160/347], Loss: 0.1502\n",
      "Epoch [1/10], Step [160/347], Loss: 0.1502\n",
      "Epoch [1/10], Step [170/347], Loss: 0.2025\n",
      "Epoch [1/10], Step [170/347], Loss: 0.2025\n",
      "Epoch [1/10], Step [180/347], Loss: 0.1436\n",
      "Epoch [1/10], Step [180/347], Loss: 0.1436\n",
      "Epoch [1/10], Step [190/347], Loss: 0.0664\n",
      "Epoch [1/10], Step [190/347], Loss: 0.0664\n",
      "Epoch [1/10], Step [200/347], Loss: 0.0403\n",
      "Epoch [1/10], Step [200/347], Loss: 0.0403\n",
      "Epoch [1/10], Step [210/347], Loss: 0.0827\n",
      "Epoch [1/10], Step [210/347], Loss: 0.0827\n",
      "Epoch [1/10], Step [220/347], Loss: 0.0386\n",
      "Epoch [1/10], Step [220/347], Loss: 0.0386\n",
      "Epoch [1/10], Step [230/347], Loss: 0.1543\n",
      "Epoch [1/10], Step [230/347], Loss: 0.1543\n",
      "Epoch [1/10], Step [240/347], Loss: 0.0741\n",
      "Epoch [1/10], Step [240/347], Loss: 0.0741\n",
      "Epoch [1/10], Step [250/347], Loss: 0.2697\n",
      "Epoch [1/10], Step [250/347], Loss: 0.2697\n",
      "Epoch [1/10], Step [260/347], Loss: 0.0498\n",
      "Epoch [1/10], Step [260/347], Loss: 0.0498\n",
      "Epoch [1/10], Step [270/347], Loss: 0.2890\n",
      "Epoch [1/10], Step [270/347], Loss: 0.2890\n",
      "Epoch [1/10], Step [280/347], Loss: 0.1800\n",
      "Epoch [1/10], Step [280/347], Loss: 0.1800\n",
      "Epoch [1/10], Step [290/347], Loss: 0.4450\n",
      "Epoch [1/10], Step [290/347], Loss: 0.4450\n",
      "Epoch [1/10], Step [300/347], Loss: 0.3685\n",
      "Epoch [1/10], Step [300/347], Loss: 0.3685\n",
      "Epoch [1/10], Step [310/347], Loss: 0.2648\n",
      "Epoch [1/10], Step [310/347], Loss: 0.2648\n",
      "Epoch [1/10], Step [320/347], Loss: 0.1491\n",
      "Epoch [1/10], Step [320/347], Loss: 0.1491\n",
      "Epoch [1/10], Step [330/347], Loss: 0.3387\n",
      "Epoch [1/10], Step [330/347], Loss: 0.3387\n",
      "Epoch [1/10], Step [340/347], Loss: 0.0556\n",
      "Epoch [1/10], Step [340/347], Loss: 0.0556\n",
      "--- Epoch 1 Summary ---\n",
      "Average Loss: 0.1707\n",
      "\n",
      "--- Epoch 1 Summary ---\n",
      "Average Loss: 0.1707\n",
      "\n",
      "Epoch [2/10], Step [10/347], Loss: 0.1021\n",
      "Epoch [2/10], Step [10/347], Loss: 0.1021\n",
      "Epoch [2/10], Step [20/347], Loss: 0.1598\n",
      "Epoch [2/10], Step [20/347], Loss: 0.1598\n",
      "Epoch [2/10], Step [30/347], Loss: 0.1677\n",
      "Epoch [2/10], Step [30/347], Loss: 0.1677\n",
      "Epoch [2/10], Step [40/347], Loss: 0.0280\n",
      "Epoch [2/10], Step [40/347], Loss: 0.0280\n",
      "Epoch [2/10], Step [50/347], Loss: 0.0528\n",
      "Epoch [2/10], Step [50/347], Loss: 0.0528\n",
      "Epoch [2/10], Step [60/347], Loss: 0.0753\n",
      "Epoch [2/10], Step [60/347], Loss: 0.0753\n",
      "Epoch [2/10], Step [70/347], Loss: 0.0854\n",
      "Epoch [2/10], Step [70/347], Loss: 0.0854\n",
      "Epoch [2/10], Step [80/347], Loss: 0.1447\n",
      "Epoch [2/10], Step [80/347], Loss: 0.1447\n",
      "Epoch [2/10], Step [90/347], Loss: 0.0956\n",
      "Epoch [2/10], Step [90/347], Loss: 0.0956\n",
      "Epoch [2/10], Step [100/347], Loss: 0.0550\n",
      "Epoch [2/10], Step [100/347], Loss: 0.0550\n",
      "Epoch [2/10], Step [110/347], Loss: 0.0334\n",
      "Epoch [2/10], Step [110/347], Loss: 0.0334\n",
      "Epoch [2/10], Step [120/347], Loss: 0.0894\n",
      "Epoch [2/10], Step [120/347], Loss: 0.0894\n",
      "Epoch [2/10], Step [130/347], Loss: 0.1566\n",
      "Epoch [2/10], Step [130/347], Loss: 0.1566\n",
      "Epoch [2/10], Step [140/347], Loss: 0.0617\n",
      "Epoch [2/10], Step [140/347], Loss: 0.0617\n",
      "Epoch [2/10], Step [150/347], Loss: 0.0834\n",
      "Epoch [2/10], Step [150/347], Loss: 0.0834\n",
      "Epoch [2/10], Step [160/347], Loss: 0.0000\n",
      "Epoch [2/10], Step [160/347], Loss: 0.0000\n",
      "Epoch [2/10], Step [170/347], Loss: 0.0467\n",
      "Epoch [2/10], Step [170/347], Loss: 0.0467\n",
      "Epoch [2/10], Step [180/347], Loss: 0.0995\n",
      "Epoch [2/10], Step [180/347], Loss: 0.0995\n",
      "Epoch [2/10], Step [190/347], Loss: 0.0334\n",
      "Epoch [2/10], Step [190/347], Loss: 0.0334\n",
      "Epoch [2/10], Step [200/347], Loss: 0.1611\n",
      "Epoch [2/10], Step [200/347], Loss: 0.1611\n",
      "Epoch [2/10], Step [210/347], Loss: 0.0050\n",
      "Epoch [2/10], Step [210/347], Loss: 0.0050\n",
      "Epoch [2/10], Step [220/347], Loss: 0.0100\n",
      "Epoch [2/10], Step [220/347], Loss: 0.0100\n",
      "Epoch [2/10], Step [230/347], Loss: 0.0460\n",
      "Epoch [2/10], Step [230/347], Loss: 0.0460\n",
      "Epoch [2/10], Step [240/347], Loss: 0.0305\n",
      "Epoch [2/10], Step [240/347], Loss: 0.0305\n",
      "Epoch [2/10], Step [250/347], Loss: 0.0131\n",
      "Epoch [2/10], Step [250/347], Loss: 0.0131\n",
      "Epoch [2/10], Step [260/347], Loss: 0.1298\n",
      "Epoch [2/10], Step [260/347], Loss: 0.1298\n",
      "Epoch [2/10], Step [270/347], Loss: 0.1441\n",
      "Epoch [2/10], Step [270/347], Loss: 0.1441\n",
      "Epoch [2/10], Step [280/347], Loss: 0.0058\n",
      "Epoch [2/10], Step [280/347], Loss: 0.0058\n",
      "Epoch [2/10], Step [290/347], Loss: 0.0235\n",
      "Epoch [2/10], Step [290/347], Loss: 0.0235\n",
      "Epoch [2/10], Step [300/347], Loss: 0.0963\n",
      "Epoch [2/10], Step [300/347], Loss: 0.0963\n",
      "Epoch [2/10], Step [310/347], Loss: 0.0047\n",
      "Epoch [2/10], Step [310/347], Loss: 0.0047\n",
      "Epoch [2/10], Step [320/347], Loss: 0.0868\n",
      "Epoch [2/10], Step [320/347], Loss: 0.0868\n",
      "Epoch [2/10], Step [330/347], Loss: 0.1240\n",
      "Epoch [2/10], Step [330/347], Loss: 0.1240\n",
      "Epoch [2/10], Step [340/347], Loss: 0.0949\n",
      "Epoch [2/10], Step [340/347], Loss: 0.0949\n",
      "--- Epoch 2 Summary ---\n",
      "Average Loss: 0.0704\n",
      "\n",
      "--- Epoch 2 Summary ---\n",
      "Average Loss: 0.0704\n",
      "\n",
      "Epoch [3/10], Step [10/347], Loss: 0.0623\n",
      "Epoch [3/10], Step [10/347], Loss: 0.0623\n",
      "Epoch [3/10], Step [20/347], Loss: 0.1267\n",
      "Epoch [3/10], Step [20/347], Loss: 0.1267\n",
      "Epoch [3/10], Step [30/347], Loss: 0.0390\n",
      "Epoch [3/10], Step [30/347], Loss: 0.0390\n",
      "Epoch [3/10], Step [40/347], Loss: 0.0321\n",
      "Epoch [3/10], Step [40/347], Loss: 0.0321\n",
      "Epoch [3/10], Step [50/347], Loss: 0.0794\n",
      "Epoch [3/10], Step [50/347], Loss: 0.0794\n",
      "Epoch [3/10], Step [60/347], Loss: 0.0358\n",
      "Epoch [3/10], Step [60/347], Loss: 0.0358\n",
      "Epoch [3/10], Step [70/347], Loss: 0.0933\n",
      "Epoch [3/10], Step [70/347], Loss: 0.0933\n",
      "Epoch [3/10], Step [80/347], Loss: 0.0157\n",
      "Epoch [3/10], Step [80/347], Loss: 0.0157\n",
      "Epoch [3/10], Step [90/347], Loss: 0.0627\n",
      "Epoch [3/10], Step [90/347], Loss: 0.0627\n",
      "Epoch [3/10], Step [100/347], Loss: 0.0523\n",
      "Epoch [3/10], Step [100/347], Loss: 0.0523\n",
      "Epoch [3/10], Step [110/347], Loss: 0.0000\n",
      "Epoch [3/10], Step [110/347], Loss: 0.0000\n",
      "Epoch [3/10], Step [120/347], Loss: 0.1547\n",
      "Epoch [3/10], Step [120/347], Loss: 0.1547\n",
      "Epoch [3/10], Step [130/347], Loss: 0.0194\n",
      "Epoch [3/10], Step [130/347], Loss: 0.0194\n",
      "Epoch [3/10], Step [140/347], Loss: 0.1071\n",
      "Epoch [3/10], Step [140/347], Loss: 0.1071\n",
      "Epoch [3/10], Step [150/347], Loss: 0.0560\n",
      "Epoch [3/10], Step [150/347], Loss: 0.0560\n",
      "Epoch [3/10], Step [160/347], Loss: 0.0332\n",
      "Epoch [3/10], Step [160/347], Loss: 0.0332\n",
      "Epoch [3/10], Step [170/347], Loss: 0.0320\n",
      "Epoch [3/10], Step [170/347], Loss: 0.0320\n",
      "Epoch [3/10], Step [180/347], Loss: 0.0329\n",
      "Epoch [3/10], Step [180/347], Loss: 0.0329\n",
      "Epoch [3/10], Step [190/347], Loss: 0.0577\n",
      "Epoch [3/10], Step [190/347], Loss: 0.0577\n",
      "Epoch [3/10], Step [200/347], Loss: 0.0195\n",
      "Epoch [3/10], Step [200/347], Loss: 0.0195\n",
      "Epoch [3/10], Step [210/347], Loss: 0.1021\n",
      "Epoch [3/10], Step [210/347], Loss: 0.1021\n",
      "Epoch [3/10], Step [220/347], Loss: 0.0510\n",
      "Epoch [3/10], Step [220/347], Loss: 0.0510\n",
      "Epoch [3/10], Step [230/347], Loss: 0.0364\n",
      "Epoch [3/10], Step [230/347], Loss: 0.0364\n",
      "Epoch [3/10], Step [240/347], Loss: 0.0000\n",
      "Epoch [3/10], Step [240/347], Loss: 0.0000\n",
      "Epoch [3/10], Step [250/347], Loss: 0.0244\n",
      "Epoch [3/10], Step [250/347], Loss: 0.0244\n",
      "Epoch [3/10], Step [260/347], Loss: 0.0220\n",
      "Epoch [3/10], Step [260/347], Loss: 0.0220\n",
      "Epoch [3/10], Step [270/347], Loss: 0.0082\n",
      "Epoch [3/10], Step [270/347], Loss: 0.0082\n",
      "Epoch [3/10], Step [280/347], Loss: 0.0683\n",
      "Epoch [3/10], Step [280/347], Loss: 0.0683\n",
      "Epoch [3/10], Step [290/347], Loss: 0.0216\n",
      "Epoch [3/10], Step [290/347], Loss: 0.0216\n",
      "Epoch [3/10], Step [300/347], Loss: 0.1326\n",
      "Epoch [3/10], Step [300/347], Loss: 0.1326\n",
      "Epoch [3/10], Step [310/347], Loss: 0.0301\n",
      "Epoch [3/10], Step [310/347], Loss: 0.0301\n",
      "Epoch [3/10], Step [320/347], Loss: 0.0000\n",
      "Epoch [3/10], Step [320/347], Loss: 0.0000\n",
      "Epoch [3/10], Step [330/347], Loss: 0.0321\n",
      "Epoch [3/10], Step [330/347], Loss: 0.0321\n",
      "Epoch [3/10], Step [340/347], Loss: 0.1330\n",
      "Epoch [3/10], Step [340/347], Loss: 0.1330\n",
      "--- Epoch 3 Summary ---\n",
      "Average Loss: 0.0410\n",
      "\n",
      "--- Epoch 3 Summary ---\n",
      "Average Loss: 0.0410\n",
      "\n",
      "Epoch [4/10], Step [10/347], Loss: 0.0053\n",
      "Epoch [4/10], Step [10/347], Loss: 0.0053\n",
      "Epoch [4/10], Step [20/347], Loss: 0.0152\n",
      "Epoch [4/10], Step [20/347], Loss: 0.0152\n",
      "Epoch [4/10], Step [30/347], Loss: 0.0000\n",
      "Epoch [4/10], Step [30/347], Loss: 0.0000\n",
      "Epoch [4/10], Step [40/347], Loss: 0.0431\n",
      "Epoch [4/10], Step [40/347], Loss: 0.0431\n",
      "Epoch [4/10], Step [50/347], Loss: 0.0195\n",
      "Epoch [4/10], Step [50/347], Loss: 0.0195\n",
      "Epoch [4/10], Step [60/347], Loss: 0.0006\n",
      "Epoch [4/10], Step [60/347], Loss: 0.0006\n",
      "Epoch [4/10], Step [70/347], Loss: 0.0294\n",
      "Epoch [4/10], Step [70/347], Loss: 0.0294\n",
      "Epoch [4/10], Step [80/347], Loss: 0.0842\n",
      "Epoch [4/10], Step [80/347], Loss: 0.0842\n",
      "Epoch [4/10], Step [90/347], Loss: 0.0111\n",
      "Epoch [4/10], Step [90/347], Loss: 0.0111\n",
      "Epoch [4/10], Step [100/347], Loss: 0.0123\n",
      "Epoch [4/10], Step [100/347], Loss: 0.0123\n",
      "Epoch [4/10], Step [110/347], Loss: 0.0467\n",
      "Epoch [4/10], Step [110/347], Loss: 0.0467\n",
      "Epoch [4/10], Step [120/347], Loss: 0.0136\n",
      "Epoch [4/10], Step [120/347], Loss: 0.0136\n",
      "Epoch [4/10], Step [130/347], Loss: 0.0000\n",
      "Epoch [4/10], Step [130/347], Loss: 0.0000\n",
      "Epoch [4/10], Step [140/347], Loss: 0.0408\n",
      "Epoch [4/10], Step [140/347], Loss: 0.0408\n",
      "Epoch [4/10], Step [150/347], Loss: 0.0865\n",
      "Epoch [4/10], Step [150/347], Loss: 0.0865\n",
      "Epoch [4/10], Step [160/347], Loss: 0.0292\n",
      "Epoch [4/10], Step [160/347], Loss: 0.0292\n",
      "Epoch [4/10], Step [170/347], Loss: 0.0281\n",
      "Epoch [4/10], Step [170/347], Loss: 0.0281\n",
      "Epoch [4/10], Step [180/347], Loss: 0.0278\n",
      "Epoch [4/10], Step [180/347], Loss: 0.0278\n",
      "Epoch [4/10], Step [190/347], Loss: 0.0037\n",
      "Epoch [4/10], Step [190/347], Loss: 0.0037\n",
      "Epoch [4/10], Step [200/347], Loss: 0.0455\n",
      "Epoch [4/10], Step [200/347], Loss: 0.0455\n",
      "Epoch [4/10], Step [210/347], Loss: 0.1071\n",
      "Epoch [4/10], Step [210/347], Loss: 0.1071\n",
      "Epoch [4/10], Step [220/347], Loss: 0.0000\n",
      "Epoch [4/10], Step [220/347], Loss: 0.0000\n",
      "Epoch [4/10], Step [230/347], Loss: 0.0050\n",
      "Epoch [4/10], Step [230/347], Loss: 0.0050\n",
      "Epoch [4/10], Step [240/347], Loss: 0.0273\n",
      "Epoch [4/10], Step [240/347], Loss: 0.0273\n",
      "Epoch [4/10], Step [250/347], Loss: 0.0237\n",
      "Epoch [4/10], Step [250/347], Loss: 0.0237\n",
      "Epoch [4/10], Step [260/347], Loss: 0.0078\n",
      "Epoch [4/10], Step [260/347], Loss: 0.0078\n",
      "Epoch [4/10], Step [270/347], Loss: 0.0028\n",
      "Epoch [4/10], Step [270/347], Loss: 0.0028\n",
      "Epoch [4/10], Step [280/347], Loss: 0.0000\n",
      "Epoch [4/10], Step [280/347], Loss: 0.0000\n",
      "Epoch [4/10], Step [290/347], Loss: 0.0232\n",
      "Epoch [4/10], Step [290/347], Loss: 0.0232\n",
      "Epoch [4/10], Step [300/347], Loss: 0.0301\n",
      "Epoch [4/10], Step [300/347], Loss: 0.0301\n",
      "Epoch [4/10], Step [310/347], Loss: 0.0926\n",
      "Epoch [4/10], Step [310/347], Loss: 0.0926\n",
      "Epoch [4/10], Step [320/347], Loss: 0.0064\n",
      "Epoch [4/10], Step [320/347], Loss: 0.0064\n",
      "Epoch [4/10], Step [330/347], Loss: 0.0538\n",
      "Epoch [4/10], Step [330/347], Loss: 0.0538\n",
      "Epoch [4/10], Step [340/347], Loss: 0.0245\n",
      "Epoch [4/10], Step [340/347], Loss: 0.0245\n",
      "--- Epoch 4 Summary ---\n",
      "Average Loss: 0.0253\n",
      "\n",
      "--- Epoch 4 Summary ---\n",
      "Average Loss: 0.0253\n",
      "\n",
      "Epoch [5/10], Step [10/347], Loss: 0.0404\n",
      "Epoch [5/10], Step [10/347], Loss: 0.0404\n",
      "Epoch [5/10], Step [20/347], Loss: 0.0246\n",
      "Epoch [5/10], Step [20/347], Loss: 0.0246\n",
      "Epoch [5/10], Step [30/347], Loss: 0.0332\n",
      "Epoch [5/10], Step [30/347], Loss: 0.0332\n",
      "Epoch [5/10], Step [40/347], Loss: 0.0577\n",
      "Epoch [5/10], Step [40/347], Loss: 0.0577\n",
      "Epoch [5/10], Step [50/347], Loss: 0.0289\n",
      "Epoch [5/10], Step [50/347], Loss: 0.0289\n",
      "Epoch [5/10], Step [60/347], Loss: 0.0146\n",
      "Epoch [5/10], Step [60/347], Loss: 0.0146\n",
      "Epoch [5/10], Step [70/347], Loss: 0.0118\n",
      "Epoch [5/10], Step [70/347], Loss: 0.0118\n",
      "Epoch [5/10], Step [80/347], Loss: 0.0000\n",
      "Epoch [5/10], Step [80/347], Loss: 0.0000\n",
      "Epoch [5/10], Step [90/347], Loss: 0.0243\n",
      "Epoch [5/10], Step [90/347], Loss: 0.0243\n",
      "Epoch [5/10], Step [100/347], Loss: 0.0000\n",
      "Epoch [5/10], Step [100/347], Loss: 0.0000\n",
      "Epoch [5/10], Step [110/347], Loss: 0.0244\n",
      "Epoch [5/10], Step [110/347], Loss: 0.0244\n",
      "Epoch [5/10], Step [120/347], Loss: 0.0010\n",
      "Epoch [5/10], Step [120/347], Loss: 0.0010\n",
      "Epoch [5/10], Step [130/347], Loss: 0.0249\n",
      "Epoch [5/10], Step [130/347], Loss: 0.0249\n",
      "Epoch [5/10], Step [140/347], Loss: 0.0184\n",
      "Epoch [5/10], Step [140/347], Loss: 0.0184\n",
      "Epoch [5/10], Step [150/347], Loss: 0.0336\n",
      "Epoch [5/10], Step [150/347], Loss: 0.0336\n",
      "Epoch [5/10], Step [160/347], Loss: 0.0000\n",
      "Epoch [5/10], Step [160/347], Loss: 0.0000\n",
      "Epoch [5/10], Step [170/347], Loss: 0.0193\n",
      "Epoch [5/10], Step [170/347], Loss: 0.0193\n",
      "Epoch [5/10], Step [180/347], Loss: 0.0000\n",
      "Epoch [5/10], Step [180/347], Loss: 0.0000\n",
      "Epoch [5/10], Step [190/347], Loss: 0.0149\n",
      "Epoch [5/10], Step [190/347], Loss: 0.0149\n",
      "Epoch [5/10], Step [200/347], Loss: 0.0166\n",
      "Epoch [5/10], Step [200/347], Loss: 0.0166\n",
      "Epoch [5/10], Step [210/347], Loss: 0.0000\n",
      "Epoch [5/10], Step [210/347], Loss: 0.0000\n",
      "Epoch [5/10], Step [220/347], Loss: 0.0671\n",
      "Epoch [5/10], Step [220/347], Loss: 0.0671\n",
      "Epoch [5/10], Step [230/347], Loss: 0.0048\n",
      "Epoch [5/10], Step [230/347], Loss: 0.0048\n",
      "Epoch [5/10], Step [240/347], Loss: 0.0217\n",
      "Epoch [5/10], Step [240/347], Loss: 0.0217\n",
      "Epoch [5/10], Step [250/347], Loss: 0.0296\n",
      "Epoch [5/10], Step [250/347], Loss: 0.0296\n",
      "Epoch [5/10], Step [260/347], Loss: 0.0404\n",
      "Epoch [5/10], Step [260/347], Loss: 0.0404\n",
      "Epoch [5/10], Step [270/347], Loss: 0.0156\n",
      "Epoch [5/10], Step [270/347], Loss: 0.0156\n",
      "Epoch [5/10], Step [280/347], Loss: 0.0104\n",
      "Epoch [5/10], Step [280/347], Loss: 0.0104\n",
      "Epoch [5/10], Step [290/347], Loss: 0.0290\n",
      "Epoch [5/10], Step [290/347], Loss: 0.0290\n",
      "Epoch [5/10], Step [300/347], Loss: 0.0293\n",
      "Epoch [5/10], Step [300/347], Loss: 0.0293\n",
      "Epoch [5/10], Step [310/347], Loss: 0.0027\n",
      "Epoch [5/10], Step [310/347], Loss: 0.0027\n",
      "Epoch [5/10], Step [320/347], Loss: 0.0345\n",
      "Epoch [5/10], Step [320/347], Loss: 0.0345\n",
      "Epoch [5/10], Step [330/347], Loss: 0.0000\n",
      "Epoch [5/10], Step [330/347], Loss: 0.0000\n",
      "Epoch [5/10], Step [340/347], Loss: 0.0098\n",
      "Epoch [5/10], Step [340/347], Loss: 0.0098\n",
      "--- Epoch 5 Summary ---\n",
      "Average Loss: 0.0214\n",
      "\n",
      "--- Epoch 5 Summary ---\n",
      "Average Loss: 0.0214\n",
      "\n",
      "Epoch [6/10], Step [10/347], Loss: 0.0000\n",
      "Epoch [6/10], Step [10/347], Loss: 0.0000\n",
      "Epoch [6/10], Step [20/347], Loss: 0.0345\n",
      "Epoch [6/10], Step [20/347], Loss: 0.0345\n",
      "Epoch [6/10], Step [30/347], Loss: 0.0235\n",
      "Epoch [6/10], Step [30/347], Loss: 0.0235\n",
      "Epoch [6/10], Step [40/347], Loss: 0.0000\n",
      "Epoch [6/10], Step [40/347], Loss: 0.0000\n",
      "Epoch [6/10], Step [50/347], Loss: 0.0181\n",
      "Epoch [6/10], Step [50/347], Loss: 0.0181\n",
      "Epoch [6/10], Step [60/347], Loss: 0.0742\n",
      "Epoch [6/10], Step [60/347], Loss: 0.0742\n",
      "Epoch [6/10], Step [70/347], Loss: 0.1040\n",
      "Epoch [6/10], Step [70/347], Loss: 0.1040\n",
      "Epoch [6/10], Step [80/347], Loss: 0.0000\n",
      "Epoch [6/10], Step [80/347], Loss: 0.0000\n",
      "Epoch [6/10], Step [90/347], Loss: 0.0693\n",
      "Epoch [6/10], Step [90/347], Loss: 0.0693\n",
      "Epoch [6/10], Step [100/347], Loss: 0.0271\n",
      "Epoch [6/10], Step [100/347], Loss: 0.0271\n",
      "Epoch [6/10], Step [110/347], Loss: 0.0000\n",
      "Epoch [6/10], Step [110/347], Loss: 0.0000\n",
      "Epoch [6/10], Step [120/347], Loss: 0.0000\n",
      "Epoch [6/10], Step [120/347], Loss: 0.0000\n",
      "Epoch [6/10], Step [130/347], Loss: 0.0227\n",
      "Epoch [6/10], Step [130/347], Loss: 0.0227\n",
      "Epoch [6/10], Step [140/347], Loss: 0.0227\n",
      "Epoch [6/10], Step [140/347], Loss: 0.0227\n",
      "Epoch [6/10], Step [150/347], Loss: 0.0031\n",
      "Epoch [6/10], Step [150/347], Loss: 0.0031\n",
      "Epoch [6/10], Step [160/347], Loss: 0.0000\n",
      "Epoch [6/10], Step [160/347], Loss: 0.0000\n",
      "Epoch [6/10], Step [170/347], Loss: 0.0008\n",
      "Epoch [6/10], Step [170/347], Loss: 0.0008\n",
      "Epoch [6/10], Step [180/347], Loss: 0.0061\n",
      "Epoch [6/10], Step [180/347], Loss: 0.0061\n",
      "Epoch [6/10], Step [190/347], Loss: 0.0000\n",
      "Epoch [6/10], Step [190/347], Loss: 0.0000\n",
      "Epoch [6/10], Step [200/347], Loss: 0.0052\n",
      "Epoch [6/10], Step [200/347], Loss: 0.0052\n",
      "Epoch [6/10], Step [210/347], Loss: 0.0205\n",
      "Epoch [6/10], Step [210/347], Loss: 0.0205\n",
      "Epoch [6/10], Step [220/347], Loss: 0.0000\n",
      "Epoch [6/10], Step [220/347], Loss: 0.0000\n",
      "Epoch [6/10], Step [230/347], Loss: 0.0199\n",
      "Epoch [6/10], Step [230/347], Loss: 0.0199\n",
      "Epoch [6/10], Step [240/347], Loss: 0.0459\n",
      "Epoch [6/10], Step [240/347], Loss: 0.0459\n",
      "Epoch [6/10], Step [250/347], Loss: 0.0000\n",
      "Epoch [6/10], Step [250/347], Loss: 0.0000\n",
      "Epoch [6/10], Step [260/347], Loss: 0.0226\n",
      "Epoch [6/10], Step [260/347], Loss: 0.0226\n",
      "Epoch [6/10], Step [270/347], Loss: 0.0807\n",
      "Epoch [6/10], Step [270/347], Loss: 0.0807\n",
      "Epoch [6/10], Step [280/347], Loss: 0.0000\n",
      "Epoch [6/10], Step [280/347], Loss: 0.0000\n",
      "Epoch [6/10], Step [290/347], Loss: 0.0082\n",
      "Epoch [6/10], Step [290/347], Loss: 0.0082\n",
      "Epoch [6/10], Step [300/347], Loss: 0.0000\n",
      "Epoch [6/10], Step [300/347], Loss: 0.0000\n",
      "Epoch [6/10], Step [310/347], Loss: 0.0000\n",
      "Epoch [6/10], Step [310/347], Loss: 0.0000\n",
      "Epoch [6/10], Step [320/347], Loss: 0.0000\n",
      "Epoch [6/10], Step [320/347], Loss: 0.0000\n",
      "Epoch [6/10], Step [330/347], Loss: 0.0136\n",
      "Epoch [6/10], Step [330/347], Loss: 0.0136\n",
      "Epoch [6/10], Step [340/347], Loss: 0.0000\n",
      "Epoch [6/10], Step [340/347], Loss: 0.0000\n",
      "--- Epoch 6 Summary ---\n",
      "Average Loss: 0.0157\n",
      "\n",
      "--- Epoch 6 Summary ---\n",
      "Average Loss: 0.0157\n",
      "\n",
      "Epoch [7/10], Step [10/347], Loss: 0.0000\n",
      "Epoch [7/10], Step [10/347], Loss: 0.0000\n",
      "Epoch [7/10], Step [20/347], Loss: 0.0010\n",
      "Epoch [7/10], Step [20/347], Loss: 0.0010\n",
      "Epoch [7/10], Step [30/347], Loss: 0.0064\n",
      "Epoch [7/10], Step [30/347], Loss: 0.0064\n",
      "Epoch [7/10], Step [40/347], Loss: 0.0000\n",
      "Epoch [7/10], Step [40/347], Loss: 0.0000\n",
      "Epoch [7/10], Step [50/347], Loss: 0.1077\n",
      "Epoch [7/10], Step [50/347], Loss: 0.1077\n",
      "Epoch [7/10], Step [60/347], Loss: 0.0226\n",
      "Epoch [7/10], Step [60/347], Loss: 0.0226\n",
      "Epoch [7/10], Step [70/347], Loss: 0.0000\n",
      "Epoch [7/10], Step [70/347], Loss: 0.0000\n",
      "Epoch [7/10], Step [80/347], Loss: 0.0634\n",
      "Epoch [7/10], Step [80/347], Loss: 0.0634\n",
      "Epoch [7/10], Step [90/347], Loss: 0.0405\n",
      "Epoch [7/10], Step [90/347], Loss: 0.0405\n",
      "Epoch [7/10], Step [100/347], Loss: 0.0000\n",
      "Epoch [7/10], Step [100/347], Loss: 0.0000\n",
      "Epoch [7/10], Step [110/347], Loss: 0.0130\n",
      "Epoch [7/10], Step [110/347], Loss: 0.0130\n",
      "Epoch [7/10], Step [120/347], Loss: 0.0000\n",
      "Epoch [7/10], Step [120/347], Loss: 0.0000\n",
      "Epoch [7/10], Step [130/347], Loss: 0.0000\n",
      "Epoch [7/10], Step [130/347], Loss: 0.0000\n",
      "Epoch [7/10], Step [140/347], Loss: 0.0168\n",
      "Epoch [7/10], Step [140/347], Loss: 0.0168\n",
      "Epoch [7/10], Step [150/347], Loss: 0.0006\n",
      "Epoch [7/10], Step [150/347], Loss: 0.0006\n",
      "Epoch [7/10], Step [160/347], Loss: 0.0000\n",
      "Epoch [7/10], Step [160/347], Loss: 0.0000\n",
      "Epoch [7/10], Step [170/347], Loss: 0.0000\n",
      "Epoch [7/10], Step [170/347], Loss: 0.0000\n",
      "Epoch [7/10], Step [180/347], Loss: 0.0136\n",
      "Epoch [7/10], Step [180/347], Loss: 0.0136\n",
      "Epoch [7/10], Step [190/347], Loss: 0.0000\n",
      "Epoch [7/10], Step [190/347], Loss: 0.0000\n",
      "Epoch [7/10], Step [200/347], Loss: 0.0000\n",
      "Epoch [7/10], Step [200/347], Loss: 0.0000\n",
      "Epoch [7/10], Step [210/347], Loss: 0.0171\n",
      "Epoch [7/10], Step [210/347], Loss: 0.0171\n",
      "Epoch [7/10], Step [220/347], Loss: 0.0000\n",
      "Epoch [7/10], Step [220/347], Loss: 0.0000\n",
      "Epoch [7/10], Step [230/347], Loss: 0.0248\n",
      "Epoch [7/10], Step [230/347], Loss: 0.0248\n",
      "Epoch [7/10], Step [240/347], Loss: 0.0281\n",
      "Epoch [7/10], Step [240/347], Loss: 0.0281\n",
      "Epoch [7/10], Step [250/347], Loss: 0.0000\n",
      "Epoch [7/10], Step [250/347], Loss: 0.0000\n",
      "Epoch [7/10], Step [260/347], Loss: 0.0000\n",
      "Epoch [7/10], Step [260/347], Loss: 0.0000\n",
      "Epoch [7/10], Step [270/347], Loss: 0.0417\n",
      "Epoch [7/10], Step [270/347], Loss: 0.0417\n",
      "Epoch [7/10], Step [280/347], Loss: 0.0314\n",
      "Epoch [7/10], Step [280/347], Loss: 0.0314\n",
      "Epoch [7/10], Step [290/347], Loss: 0.0025\n",
      "Epoch [7/10], Step [290/347], Loss: 0.0025\n",
      "Epoch [7/10], Step [300/347], Loss: 0.0000\n",
      "Epoch [7/10], Step [300/347], Loss: 0.0000\n",
      "Epoch [7/10], Step [310/347], Loss: 0.0000\n",
      "Epoch [7/10], Step [310/347], Loss: 0.0000\n",
      "Epoch [7/10], Step [320/347], Loss: 0.0490\n",
      "Epoch [7/10], Step [320/347], Loss: 0.0490\n",
      "Epoch [7/10], Step [330/347], Loss: 0.0000\n",
      "Epoch [7/10], Step [330/347], Loss: 0.0000\n",
      "Epoch [7/10], Step [340/347], Loss: 0.0000\n",
      "Epoch [7/10], Step [340/347], Loss: 0.0000\n",
      "--- Epoch 7 Summary ---\n",
      "Average Loss: 0.0129\n",
      "\n",
      "--- Epoch 7 Summary ---\n",
      "Average Loss: 0.0129\n",
      "\n",
      "Epoch [8/10], Step [10/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [10/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [20/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [20/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [30/347], Loss: 0.0069\n",
      "Epoch [8/10], Step [30/347], Loss: 0.0069\n",
      "Epoch [8/10], Step [40/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [40/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [50/347], Loss: 0.0105\n",
      "Epoch [8/10], Step [50/347], Loss: 0.0105\n",
      "Epoch [8/10], Step [60/347], Loss: 0.0040\n",
      "Epoch [8/10], Step [60/347], Loss: 0.0040\n",
      "Epoch [8/10], Step [70/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [70/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [80/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [80/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [90/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [90/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [100/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [100/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [110/347], Loss: 0.0229\n",
      "Epoch [8/10], Step [110/347], Loss: 0.0229\n",
      "Epoch [8/10], Step [120/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [120/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [130/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [130/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [140/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [140/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [150/347], Loss: 0.0023\n",
      "Epoch [8/10], Step [150/347], Loss: 0.0023\n",
      "Epoch [8/10], Step [160/347], Loss: 0.0056\n",
      "Epoch [8/10], Step [160/347], Loss: 0.0056\n",
      "Epoch [8/10], Step [170/347], Loss: 0.0085\n",
      "Epoch [8/10], Step [170/347], Loss: 0.0085\n",
      "Epoch [8/10], Step [180/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [180/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [190/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [190/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [200/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [200/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [210/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [210/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [220/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [220/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [230/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [230/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [240/347], Loss: 0.0076\n",
      "Epoch [8/10], Step [240/347], Loss: 0.0076\n",
      "Epoch [8/10], Step [250/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [250/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [260/347], Loss: 0.0187\n",
      "Epoch [8/10], Step [260/347], Loss: 0.0187\n",
      "Epoch [8/10], Step [270/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [270/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [280/347], Loss: 0.0182\n",
      "Epoch [8/10], Step [280/347], Loss: 0.0182\n",
      "Epoch [8/10], Step [290/347], Loss: 0.0177\n",
      "Epoch [8/10], Step [290/347], Loss: 0.0177\n",
      "Epoch [8/10], Step [300/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [300/347], Loss: 0.0000\n",
      "Epoch [8/10], Step [310/347], Loss: 0.0099\n",
      "Epoch [8/10], Step [310/347], Loss: 0.0099\n",
      "Epoch [8/10], Step [320/347], Loss: 0.0150\n",
      "Epoch [8/10], Step [320/347], Loss: 0.0150\n",
      "Epoch [8/10], Step [330/347], Loss: 0.0075\n",
      "Epoch [8/10], Step [330/347], Loss: 0.0075\n",
      "Epoch [8/10], Step [340/347], Loss: 0.0307\n",
      "Epoch [8/10], Step [340/347], Loss: 0.0307\n",
      "--- Epoch 8 Summary ---\n",
      "Average Loss: 0.0100\n",
      "\n",
      "--- Epoch 8 Summary ---\n",
      "Average Loss: 0.0100\n",
      "\n",
      "Epoch [9/10], Step [10/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [10/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [20/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [20/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [30/347], Loss: 0.0462\n",
      "Epoch [9/10], Step [30/347], Loss: 0.0462\n",
      "Epoch [9/10], Step [40/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [40/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [50/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [50/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [60/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [60/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [70/347], Loss: 0.0203\n",
      "Epoch [9/10], Step [70/347], Loss: 0.0203\n",
      "Epoch [9/10], Step [80/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [80/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [90/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [90/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [100/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [100/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [110/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [110/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [120/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [120/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [130/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [130/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [140/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [140/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [150/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [150/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [160/347], Loss: 0.0044\n",
      "Epoch [9/10], Step [160/347], Loss: 0.0044\n",
      "Epoch [9/10], Step [170/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [170/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [180/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [180/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [190/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [190/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [200/347], Loss: 0.0071\n",
      "Epoch [9/10], Step [200/347], Loss: 0.0071\n",
      "Epoch [9/10], Step [210/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [210/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [220/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [220/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [230/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [230/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [240/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [240/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [250/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [250/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [260/347], Loss: 0.0090\n",
      "Epoch [9/10], Step [260/347], Loss: 0.0090\n",
      "Epoch [9/10], Step [270/347], Loss: 0.0038\n",
      "Epoch [9/10], Step [270/347], Loss: 0.0038\n",
      "Epoch [9/10], Step [280/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [280/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [290/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [290/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [300/347], Loss: 0.0046\n",
      "Epoch [9/10], Step [300/347], Loss: 0.0046\n",
      "Epoch [9/10], Step [310/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [310/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [320/347], Loss: 0.0369\n",
      "Epoch [9/10], Step [320/347], Loss: 0.0369\n",
      "Epoch [9/10], Step [330/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [330/347], Loss: 0.0000\n",
      "Epoch [9/10], Step [340/347], Loss: 0.0770\n",
      "Epoch [9/10], Step [340/347], Loss: 0.0770\n",
      "--- Epoch 9 Summary ---\n",
      "Average Loss: 0.0095\n",
      "\n",
      "--- Epoch 9 Summary ---\n",
      "Average Loss: 0.0095\n",
      "\n",
      "Epoch [10/10], Step [10/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [10/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [20/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [20/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [30/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [30/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [40/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [40/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [50/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [50/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [60/347], Loss: 0.0197\n",
      "Epoch [10/10], Step [60/347], Loss: 0.0197\n",
      "Epoch [10/10], Step [70/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [70/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [80/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [80/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [90/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [90/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [100/347], Loss: 0.0001\n",
      "Epoch [10/10], Step [100/347], Loss: 0.0001\n",
      "Epoch [10/10], Step [110/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [110/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [120/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [120/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [130/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [130/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [140/347], Loss: 0.0404\n",
      "Epoch [10/10], Step [140/347], Loss: 0.0404\n",
      "Epoch [10/10], Step [150/347], Loss: 0.0931\n",
      "Epoch [10/10], Step [150/347], Loss: 0.0931\n",
      "Epoch [10/10], Step [160/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [160/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [170/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [170/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [180/347], Loss: 0.0320\n",
      "Epoch [10/10], Step [180/347], Loss: 0.0320\n",
      "Epoch [10/10], Step [190/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [190/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [200/347], Loss: 0.0771\n",
      "Epoch [10/10], Step [200/347], Loss: 0.0771\n",
      "Epoch [10/10], Step [210/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [210/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [220/347], Loss: 0.0007\n",
      "Epoch [10/10], Step [220/347], Loss: 0.0007\n",
      "Epoch [10/10], Step [230/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [230/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [240/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [240/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [250/347], Loss: 0.0157\n",
      "Epoch [10/10], Step [250/347], Loss: 0.0157\n",
      "Epoch [10/10], Step [260/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [260/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [270/347], Loss: 0.0674\n",
      "Epoch [10/10], Step [270/347], Loss: 0.0674\n",
      "Epoch [10/10], Step [280/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [280/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [290/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [290/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [300/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [300/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [310/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [310/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [320/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [320/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [330/347], Loss: 0.0011\n",
      "Epoch [10/10], Step [330/347], Loss: 0.0011\n",
      "Epoch [10/10], Step [340/347], Loss: 0.0000\n",
      "Epoch [10/10], Step [340/347], Loss: 0.0000\n",
      "--- Epoch 10 Summary ---\n",
      "Average Loss: 0.0073\n",
      "\n",
      "Finished Training\n",
      "Model saved to lacrosse_reid_player_model.pth\n",
      "--- Epoch 10 Summary ---\n",
      "Average Loss: 0.0073\n",
      "\n",
      "Finished Training\n",
      "Model saved to lacrosse_reid_player_model.pth\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, dataloader, optimizer, loss_fn, num_epochs=20):\n",
    "    \"\"\"The main training loop\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        batch_count = 0\n",
    "        \n",
    "        for i, (anchor, positive, negative, _) in enumerate(dataloader):\n",
    "            anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            emb_anchor, emb_positive, emb_negative = model.forward_triplet(anchor, positive, negative)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = loss_fn(emb_anchor, emb_positive, emb_negative)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            batch_count += 1\n",
    "            if (i + 1) % 10 == 0:  # Reduced frequency for notebook\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        if batch_count > 0:\n",
    "            epoch_loss = running_loss / batch_count\n",
    "        else:\n",
    "            epoch_loss = 0.0\n",
    "        print(f\"--- Epoch {epoch+1} Summary ---\")\n",
    "        print(f\"Average Loss: {epoch_loss:.4f}\\n\")\n",
    "        \n",
    "    print(\"Finished Training\")\n",
    "    return model\n",
    "\n",
    "# --- Main training block for notebook ---\n",
    "# Hyperparameters\n",
    "TRAIN_DIR = 'data/clustered_train/train'  # Fixed path\n",
    "EMBEDDING_DIM = 128\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 16  # Reduced batch size for stability\n",
    "NUM_EPOCHS = 10   # Reduced epochs for testing\n",
    "MARGIN = 0.5 # Margin for the triplet loss\n",
    "\n",
    "# Check if train directory exists and has data\n",
    "if not os.path.exists(TRAIN_DIR):\n",
    "    print(f\"Error: {TRAIN_DIR} directory does not exist!\")\n",
    "    print(\"Available directories:\", [d for d in os.listdir('.') if os.path.isdir(d)])\n",
    "else:\n",
    "    train_folders = [d for d in os.listdir(TRAIN_DIR) if os.path.isdir(os.path.join(TRAIN_DIR, d))]\n",
    "    print(f\"Found {len(train_folders)} player folders in {TRAIN_DIR}\")\n",
    "    \n",
    "    if len(train_folders) < 2:\n",
    "        print(\"Error: Need at least 2 player folders for triplet loss training!\")\n",
    "    else:\n",
    "        # 1. Setup Dataset and DataLoader\n",
    "        train_dataset = LacrossePlayerDataset(image_dir=TRAIN_DIR, transform=data_transforms)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)  # Fixed num_workers\n",
    "        \n",
    "        print(f\"Dataset size: {len(train_dataset)} images\")\n",
    "        print(f\"Number of batches: {len(train_dataloader)}\")\n",
    "        \n",
    "        # 2. Initialize Model, Loss, and Optimizer\n",
    "        siamese_model = SiameseNet(embedding_dim=EMBEDDING_DIM)\n",
    "        triplet_loss = nn.TripletMarginLoss(margin=MARGIN, p=2)\n",
    "        optimizer = torch.optim.Adam(siamese_model.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        # 3. Start Training\n",
    "        trained_model = train_model(siamese_model, train_dataloader, optimizer, triplet_loss, num_epochs=NUM_EPOCHS)\n",
    "        \n",
    "        # 4. Save the trained model\n",
    "        torch.save(trained_model.state_dict(), 'lacrosse_reid_player_model.pth')\n",
    "        print(\"Model saved to lacrosse_reid_player_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5cfde8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59cf575",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
