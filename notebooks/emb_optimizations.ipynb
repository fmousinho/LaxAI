{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebeb07f3",
   "metadata": {},
   "source": [
    "# Incremental Training Setup\n",
    "\n",
    "This notebook now supports **incremental training** - the model will automatically load existing weights from `augmented_embeddings_model.pth` if it exists, and continue training from there.\n",
    "\n",
    "## Training Behavior:\n",
    "- **First run**: Starts with random weights (fresh model)\n",
    "- **Subsequent runs**: Loads existing weights and continues training\n",
    "- **Benefits**: Accumulative learning, faster convergence, builds on previous training\n",
    "\n",
    "## Control Options:\n",
    "- To **start fresh**: Delete or rename the existing model file before training\n",
    "- To **continue**: Just run the training cell normally\n",
    "- To **backup**: Copy the model file before training to preserve previous versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32d50919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current model status:\n",
      "âœ— No model found: /Users/fernandomousinho/Documents/Learning_to_Code/LaxAI/notebooks/augmented_embeddings_model.pth\n",
      "\n",
      "ðŸ“š Training Options:\n",
      "1. Normal training: Loads saved weights if available, otherwise uses pre-trained ResNet18\n",
      "2. Fresh pre-trained: force_pretrained_training() - Ignores saved weights, starts with pre-trained ResNet18\n",
      "3. Complete reset: reset_model() then train - Deletes saved model, starts with pre-trained ResNet18\n"
     ]
    }
   ],
   "source": [
    "# Model Management Utilities\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "def check_model_status(model_path):\n",
    "    \"\"\"Check if model file exists and display its info.\"\"\"\n",
    "    if os.path.exists(model_path):\n",
    "        file_size = os.path.getsize(model_path) / (1024 * 1024)  # MB\n",
    "        mod_time = datetime.fromtimestamp(os.path.getmtime(model_path))\n",
    "        print(f\"âœ“ Model exists: {model_path}\")\n",
    "        print(f\"  Size: {file_size:.2f} MB\")\n",
    "        print(f\"  Last modified: {mod_time}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"âœ— No model found: {model_path}\")\n",
    "        return False\n",
    "\n",
    "def backup_model(model_path, backup_suffix=None):\n",
    "    \"\"\"Create a backup of the current model.\"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        print(\"No model to backup\")\n",
    "        return None\n",
    "    \n",
    "    if backup_suffix is None:\n",
    "        backup_suffix = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    backup_path = model_path.replace('.pth', f'_backup_{backup_suffix}.pth')\n",
    "    shutil.copy2(model_path, backup_path)\n",
    "    print(f\"âœ“ Model backed up to: {backup_path}\")\n",
    "    return backup_path\n",
    "\n",
    "def reset_model(model_path):\n",
    "    \"\"\"Delete the existing model to start fresh training.\"\"\"\n",
    "    if os.path.exists(model_path):\n",
    "        os.remove(model_path)\n",
    "        print(f\"âœ“ Model deleted: {model_path}\")\n",
    "        print(\"Next training will start with pre-trained ResNet18 weights\")\n",
    "    else:\n",
    "        print(\"No model to delete\")\n",
    "\n",
    "def force_pretrained_training():\n",
    "    \"\"\"\n",
    "    Helper function to train with fresh pre-trained ResNet18 weights.\n",
    "    Use this when you want to ignore saved weights and start from pre-trained backbone.\n",
    "    \"\"\"\n",
    "    print(\"ðŸ”„ Setting up training with fresh pre-trained ResNet18 weights...\")\n",
    "    \n",
    "    # Create EmbeddingsProcessor with force_pretrained option\n",
    "    fresh_processor = EmbeddingsProcessor(\n",
    "        train_dir=temp_data_dir,\n",
    "        model_save_path=model_save_path,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Setup model with force_pretrained=True\n",
    "    fresh_processor.setup_model(SiameseNet, force_pretrained=True)\n",
    "    \n",
    "    return fresh_processor\n",
    "\n",
    "# Check current model status\n",
    "notebooks_dir = \"/Users/fernandomousinho/Documents/Learning_to_Code/LaxAI/notebooks\"\n",
    "model_save_path = os.path.join(notebooks_dir, \"augmented_embeddings_model.pth\")\n",
    "\n",
    "print(\"Current model status:\")\n",
    "check_model_status(model_save_path)\n",
    "\n",
    "print(\"\\nðŸ“š Training Options:\")\n",
    "print(\"1. Normal training: Loads saved weights if available, otherwise uses pre-trained ResNet18\")\n",
    "print(\"2. Fresh pre-trained: force_pretrained_training() - Ignores saved weights, starts with pre-trained ResNet18\")\n",
    "print(\"3. Complete reset: reset_model() then train - Deletes saved model, starts with pre-trained ResNet18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15694d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0714 16:00:40.860000 49499 torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrain weights\n",
      "Loaded 15000 frames of detections from tracks.json\n",
      "Loaded 15000 frames of detections from tracks.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the parent directory to the path to import modules\n",
    "sys.path.append('/Users/fernandomousinho/Documents/Learning_to_Code/LaxAI')\n",
    "\n",
    "from modules.det_processor import DetectionProcessor\n",
    "from modules.detection import DetectionModel\n",
    "from modules.tracker import AffineAwareByteTrack\n",
    "from tools.store_driver import Store\n",
    "import torch\n",
    "\n",
    "\n",
    "# Initialize required components\n",
    "store = Store()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "detection_model = DetectionModel(store=store, device=device)\n",
    "tracker = AffineAwareByteTrack()\n",
    "\n",
    "# Create detection processor\n",
    "detection_processor = DetectionProcessor(\n",
    "    model=detection_model,\n",
    "    tracker=tracker,\n",
    "    detection_file_path=\"\"  # Not needed for loading\n",
    ")\n",
    "\n",
    "# Load multi_frame_detections from tracks.json\n",
    "tracks_json_path = \"/Users/fernandomousinho/Documents/Learning_to_Code/LaxAI/notebooks/tracks.json\"\n",
    "video_source = \"/Users/fernandomousinho/Library/CloudStorage/GoogleDrive-fmousinho76@gmail.com/My Drive/Colab_Notebooks/FCA_Upstate_NY_003.mp4\"\n",
    "\n",
    "multi_frame_detections = detection_processor.json_to_detections(\n",
    "    json_file_path=tracks_json_path,\n",
    "    update_tracker_state=True,\n",
    "    video_source=video_source\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ad0a700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "tracks_data = tracker.get_tracks_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d471050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 1 image -> Augmented: 17 images\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import List\n",
    "\n",
    "def augment_images(images: List[np.ndarray]) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Perform various augmentations on a list of BGR images using OpenCV.\n",
    "    \n",
    "    Args:\n",
    "        images: List of BGR numpy arrays (images)\n",
    "        \n",
    "    Returns:\n",
    "        List of augmented BGR numpy arrays\n",
    "    \"\"\"\n",
    "    augmented_images = []\n",
    "    \n",
    "    for img in images:\n",
    "        # Skip invalid or very small images\n",
    "        if img is None or img.size == 0 or len(img.shape) < 3:\n",
    "            continue\n",
    "            \n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        # Skip images that are too small for meaningful augmentation\n",
    "        if h < 20 or w < 20:\n",
    "            augmented_images.append(img.copy())\n",
    "            continue\n",
    "        \n",
    "        # Original image\n",
    "        augmented_images.append(img.copy())\n",
    "        \n",
    "        # 1. Left-to-right flip\n",
    "        flipped = cv2.flip(img, 1)\n",
    "        augmented_images.append(flipped)\n",
    "        \n",
    "        # 2. Rotations (+20 and -20 degrees)\n",
    "        center = (w // 2, h // 2)\n",
    "        \n",
    "        # +20 degree rotation\n",
    "        rotation_matrix_pos = cv2.getRotationMatrix2D(center, 20, 1.0)\n",
    "        rotated_pos = cv2.warpAffine(img, rotation_matrix_pos, (w, h), borderMode=cv2.BORDER_REFLECT)\n",
    "        augmented_images.append(rotated_pos)\n",
    "        \n",
    "        # -20 degree rotation\n",
    "        rotation_matrix_neg = cv2.getRotationMatrix2D(center, -20, 1.0)\n",
    "        rotated_neg = cv2.warpAffine(img, rotation_matrix_neg, (w, h), borderMode=cv2.BORDER_REFLECT)\n",
    "        augmented_images.append(rotated_neg)\n",
    "        \n",
    "        # 3. 10% stretching (horizontal and vertical)\n",
    "        stretch_factor = 1.1\n",
    "        \n",
    "        # Horizontal stretch\n",
    "        stretched_h = cv2.resize(img, (int(w * stretch_factor), h))\n",
    "        # Crop back to original size from center\n",
    "        start_x = (stretched_h.shape[1] - w) // 2\n",
    "        stretched_h_cropped = stretched_h[:, start_x:start_x + w]\n",
    "        augmented_images.append(stretched_h_cropped)\n",
    "        \n",
    "        # Vertical stretch\n",
    "        stretched_v = cv2.resize(img, (w, int(h * stretch_factor)))\n",
    "        # Crop back to original size from center\n",
    "        start_y = (stretched_v.shape[0] - h) // 2\n",
    "        stretched_v_cropped = stretched_v[start_y:start_y + h, :]\n",
    "        augmented_images.append(stretched_v_cropped)\n",
    "        \n",
    "        # 4. 20% crops (different sizes and positions) - only if image is large enough\n",
    "        if h > 50 and w > 50:\n",
    "            crop_sizes = [0.8, 0.85, 0.9]  # 20%, 15%, 10% crops\n",
    "            \n",
    "            for crop_ratio in crop_sizes:\n",
    "                crop_h = max(int(h * crop_ratio), 20)  # Ensure minimum size\n",
    "                crop_w = max(int(w * crop_ratio), 20)\n",
    "                \n",
    "                # Ensure we don't exceed image bounds\n",
    "                crop_h = min(crop_h, h)\n",
    "                crop_w = min(crop_w, w)\n",
    "                \n",
    "                # Random position for crop\n",
    "                max_start_y = max(0, h - crop_h)\n",
    "                max_start_x = max(0, w - crop_w)\n",
    "                \n",
    "                start_y = random.randint(0, max_start_y) if max_start_y > 0 else 0\n",
    "                start_x = random.randint(0, max_start_x) if max_start_x > 0 else 0\n",
    "                \n",
    "                cropped = img[start_y:start_y + crop_h, start_x:start_x + crop_w]\n",
    "                # Resize back to original size\n",
    "                resized_crop = cv2.resize(cropped, (w, h))\n",
    "                augmented_images.append(resized_crop)\n",
    "        \n",
    "        # 5. Random occlusion - adaptive size based on image dimensions\n",
    "        occluded = img.copy()\n",
    "        occ_samples = 5  # Number of occluded images generated\n",
    "\n",
    "        # Calculate occlusion size as percentage of image (minimum 5x5, maximum 15x15)\n",
    "        occ_size = min(15, max(5, min(h, w) // 6))\n",
    "        \n",
    "        # Only add occlusion if image is large enough\n",
    "        if h > occ_size and w > occ_size:\n",
    "            num_occlusions = random.randint(1, 3)  # 1-3 occlusions per image\n",
    "            for _ in range(num_occlusions):\n",
    "                for _ in range(occ_samples):\n",
    "                    occ_y = random.randint(0, h - occ_size)\n",
    "                    occ_x = random.randint(0, w - occ_size)\n",
    "                    # Fill with random color\n",
    "                    random_color = [random.randint(0, 255) for _ in range(3)]\n",
    "                    occluded[occ_y:occ_y + occ_size, occ_x:occ_x + occ_size] = random_color\n",
    "                    augmented_images.append(occluded)\n",
    "\n",
    "    \n",
    "        \n",
    "        # 6. Noise (Gaussian noise)\n",
    "        noise = np.random.normal(0, 15, img.shape).astype(np.int16)\n",
    "        noisy = np.clip(img.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n",
    "        augmented_images.append(noisy)\n",
    "        \n",
    "        # 7. Saturation/Darkness (simulating sun exposure)\n",
    "        # Convert to HSV for saturation adjustment\n",
    "        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "        hsv_float = hsv.astype(np.float32)\n",
    "        \n",
    "        # Reduce saturation and value (brightness)\n",
    "        hsv_float[:, :, 1] *= 0.7  # Reduce saturation by 30%\n",
    "        hsv_float[:, :, 2] *= 0.6  # Reduce brightness by 40%\n",
    "        \n",
    "        # Clip values and convert back\n",
    "        hsv_adjusted = np.clip(hsv_float, 0, 255).astype(np.uint8)\n",
    "        darkened = cv2.cvtColor(hsv_adjusted, cv2.COLOR_HSV2BGR)\n",
    "        augmented_images.append(darkened)\n",
    "        \n",
    "        # 8. Additional saturation variation (oversaturated)\n",
    "        hsv_over = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "        hsv_over_float = hsv_over.astype(np.float32)\n",
    "        \n",
    "        # Increase saturation\n",
    "        hsv_over_float[:, :, 1] *= 1.3  # Increase saturation by 30%\n",
    "        hsv_over_float[:, :, 2] *= 0.9  # Slightly reduce brightness\n",
    "        \n",
    "        hsv_over_adjusted = np.clip(hsv_over_float, 0, 255).astype(np.uint8)\n",
    "        oversaturated = cv2.cvtColor(hsv_over_adjusted, cv2.COLOR_HSV2BGR)\n",
    "        augmented_images.append(oversaturated)\n",
    "    \n",
    "    return augmented_images\n",
    "\n",
    "# Test function with a simple example\n",
    "def test_augmentation():\n",
    "    \"\"\"Test the augmentation function with sample images\"\"\"\n",
    "    # Create a simple test image (red square)\n",
    "    test_img = np.zeros((100, 100, 3), dtype=np.uint8)\n",
    "    test_img[25:75, 25:75] = [0, 0, 255]  # Red square in BGR\n",
    "    \n",
    "    # Test with single image\n",
    "    augmented = augment_images([test_img])\n",
    "    print(f\"Original: 1 image -> Augmented: {len(augmented)} images\")\n",
    "    \n",
    "    return augmented\n",
    "\n",
    "# Run test\n",
    "test_results = test_augmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddb474f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 100: Track IDs [ 2  5  7 10  9]\n",
      "Frame 300: Track IDs [15  5  9 14]\n",
      "Frame 500: Track IDs [15 14 18 21 20 22  9]\n",
      "Frame 700: Track IDs [15 14 25 21 23 18 24 20]\n",
      "Frame 900: Track IDs [34 27 32 39 20 25 18 35 23 15 29 33]\n",
      "Frame 1100: Track IDs [32 34 51 52 47 45 50 40 15 43 20 33 48]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "frame_id_sample = [100, 300, 500, 700, 900, 1100]\n",
    "track_ids_for_frame = {}\n",
    "\n",
    "# Initialize the dictionary with empty lists for each frame\n",
    "for frame_id in frame_id_sample:\n",
    "    track_ids_for_frame[frame_id] = []\n",
    "\n",
    "\n",
    "for detections in multi_frame_detections:\n",
    "    frame_id = detections.metadata['frame_id']\n",
    "    if frame_id in frame_id_sample:\n",
    "        # Handle class_id using numpy operations\n",
    "        mask = detections.class_id == 3\n",
    "        track_ids_for_frame[frame_id] = detections.tracker_id[mask]\n",
    "       \n",
    "for frame_id in frame_id_sample:\n",
    "    print(f\"Frame {frame_id}: Track IDs {track_ids_for_frame[frame_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6a8d7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_of_annotated_crops_dict = []\n",
    "for frame_id in frame_id_sample:\n",
    "    track_ids_to_process = track_ids_for_frame[frame_id]\n",
    "    track_data_to_process = [data for data in tracks_data.values() if data.track_id in track_ids_to_process]\n",
    "    augmented_crops_dict = {}\n",
    "    for data in track_data_to_process:\n",
    "        original_crops = data.crops[:5]\n",
    "        augmented_crops = augment_images(original_crops)\n",
    "        augmented_crops_dict[data.track_id] = augmented_crops\n",
    "    list_of_annotated_crops_dict.append(augmented_crops_dict.copy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d612b4ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'modules'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodules\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LacrossePlayerDataset\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconfig\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_transforms\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtempfile\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'modules'"
     ]
    }
   ],
   "source": [
    "from modules.dataset import LacrossePlayerDataset\n",
    "from config.transforms import get_transforms\n",
    "import tempfile\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "def create_augmented_dataset(\n",
    "    augmented_crops_dict,\n",
    "    transforms_config='opencv_safe_training',\n",
    "    temp_dir_prefix=\"augmented_data\",\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a LacrossePlayerDataset from augmented crops dictionary.\n",
    "    \n",
    "    Args:\n",
    "        augmented_crops_dict: Dictionary of {track_id: [augmented_crops]}\n",
    "        transforms_config: Transform configuration to use (default: 'opencv_safe_training')\n",
    "        temp_dir_prefix: Prefix for the temporary directory name\n",
    "        verbose: If True, print detailed information\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (dataset, temp_data_dir, total_images_saved)\n",
    "            - dataset: The created LacrossePlayerDataset\n",
    "            - temp_data_dir: Path to the temporary data directory\n",
    "            - total_images_saved: Number of images successfully saved\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a temporary directory structure for the dataset\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    data_dir = os.path.join(temp_dir, temp_dir_prefix)\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Creating dataset in temporary directory: {data_dir}\")\n",
    "\n",
    "    # Create subdirectories for each track (treating each track as a class)\n",
    "    track_dirs = {}\n",
    "    for track_id in augmented_crops_dict.keys():\n",
    "        track_dir = os.path.join(data_dir, f\"track_{track_id}\")\n",
    "        os.makedirs(track_dir, exist_ok=True)\n",
    "        track_dirs[track_id] = track_dir\n",
    "\n",
    "    # Save augmented crops as image files\n",
    "    total_images_saved = 0\n",
    "    failed_saves = 0\n",
    "\n",
    "    for track_id, augmented_crops in augmented_crops_dict.items():\n",
    "        track_dir = track_dirs[track_id]\n",
    "        \n",
    "        for idx, crop in enumerate(augmented_crops):\n",
    "            if crop is not None and crop.size > 0:\n",
    "                # Save as PNG file\n",
    "                image_path = os.path.join(track_dir, f\"crop_{idx:04d}.png\")\n",
    "                success = cv2.imwrite(image_path, crop)\n",
    "                if success:\n",
    "                    total_images_saved += 1\n",
    "                else:\n",
    "                    failed_saves += 1\n",
    "                    if verbose:\n",
    "                        print(f\"Failed to save image: {image_path}\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Saved {total_images_saved} augmented images across {len(track_dirs)} track directories\")\n",
    "        if failed_saves > 0:\n",
    "            print(f\"Warning: {failed_saves} images failed to save\")\n",
    "\n",
    "    # Create the LacrossePlayerDataset\n",
    "    transforms = get_transforms(transforms_config)\n",
    "\n",
    "    # Create dataset\n",
    "    augmented_dataset = LacrossePlayerDataset(\n",
    "        data_dir,\n",
    "        transform=transforms\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Created LacrossePlayerDataset with {len(augmented_dataset)} samples\")\n",
    "        print(f\"Temporary data directory: {data_dir}\")\n",
    "        print(\"Note: This directory can be cleaned up later with: import shutil; shutil.rmtree(temp_data_dir)\")\n",
    "\n",
    "    return augmented_dataset, data_dir, total_images_saved\n",
    "\n",
    "def cleanup_temp_directory(temp_data_dir, verbose=True):\n",
    "    \"\"\"\n",
    "    Clean up the temporary directory created for the dataset.\n",
    "    \n",
    "    Args:\n",
    "        temp_data_dir: Path to the temporary directory to clean up\n",
    "        verbose: If True, print cleanup information\n",
    "    \"\"\"\n",
    "    import shutil\n",
    "    \n",
    "    if os.path.exists(temp_data_dir):\n",
    "        try:\n",
    "            shutil.rmtree(temp_data_dir)\n",
    "            if verbose:\n",
    "                print(f\"âœ“ Cleaned up temporary directory: {temp_data_dir}\")\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"âœ— Failed to clean up directory {temp_data_dir}: {e}\")\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(f\"Directory does not exist: {temp_data_dir}\")\n",
    "\n",
    "# Example usage with the current augmented crops\n",
    "print(\"=== Creating Augmented Dataset ===\")\n",
    "\n",
    "# Create the dataset using the new function\n",
    "augmented_dataset, temp_data_dir, images_saved = create_augmented_dataset(\n",
    "    augmented_crops_dict=augmented_crops_dict,\n",
    "    transforms_config='opencv_safe_training',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Dataset creation completed!\")\n",
    "print(f\"  - Dataset samples: {len(augmented_dataset)}\")\n",
    "print(f\"  - Images saved: {images_saved}\")\n",
    "print(f\"  - Temporary directory: {temp_data_dir}\")\n",
    "\n",
    "# The temp_data_dir variable is now available for use in subsequent cells\n",
    "# You can clean it up later using: cleanup_temp_directory(temp_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41664404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.emb_processor import EmbeddingsProcessor\n",
    "from modules.siamesenet import SiameseNet\n",
    "from modules.dataset import LacrossePlayerDataset\n",
    "from config.transforms import get_transforms\n",
    "import os\n",
    "\n",
    "def train_embeddings_model(\n",
    "    dataset,\n",
    "    temp_data_dir,\n",
    "    model_save_path=None,\n",
    "    device=None,\n",
    "    force_pretrained=False,\n",
    "    notebooks_dir=\"/Users/fernandomousinho/Documents/Learning_to_Code/LaxAI/notebooks\",\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a SiameseNet embeddings model with the given dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The dataset to train on (e.g., augmented_dataset)\n",
    "        temp_data_dir: Temporary directory containing the training data\n",
    "        model_save_path: Path to save the trained model (optional)\n",
    "        device: Training device (cuda/cpu) - will auto-detect if None\n",
    "        force_pretrained: If True, ignore saved weights and start with pre-trained ResNet18\n",
    "        notebooks_dir: Directory where notebooks are stored\n",
    "        verbose: If True, print detailed training information\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (embeddings_processor, test_processor, training_success)\n",
    "            - embeddings_processor: The trained processor\n",
    "            - test_processor: A processor instance for inference testing\n",
    "            - training_success: Boolean indicating if training succeeded\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set default values\n",
    "    if device is None:\n",
    "        import torch\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    if model_save_path is None:\n",
    "        model_save_path = os.path.join(notebooks_dir, \"augmented_embeddings_model.pth\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Training model with augmented dataset...\")\n",
    "        print(f\"Dataset samples: {len(dataset)}\")\n",
    "        print(f\"Model will be saved to: {model_save_path}\")\n",
    "        print(f\"Device: {device}\")\n",
    "        print(f\"Force pretrained: {force_pretrained}\")\n",
    "\n",
    "    # Create EmbeddingsProcessor for training\n",
    "    embeddings_processor = EmbeddingsProcessor(\n",
    "        train_dir=temp_data_dir,\n",
    "        model_save_path=model_save_path,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Setup model (will auto-load saved weights if available, unless force_pretrained=True)\n",
    "    if force_pretrained:\n",
    "        if verbose:\n",
    "            print(\"ðŸ”„ Forcing fresh start with pre-trained ResNet18 weights...\")\n",
    "        embeddings_processor.setup_model(SiameseNet, force_pretrained=True)\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"ðŸ“ˆ Using incremental training (loads saved weights if available)...\")\n",
    "        embeddings_processor.setup_model(SiameseNet)\n",
    "\n",
    "    # Train the model\n",
    "    training_success = False\n",
    "    test_processor = None\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Starting training...\")\n",
    "    \n",
    "    try:\n",
    "        # Setup data and train\n",
    "        embeddings_processor.setup_data(LacrossePlayerDataset, get_transforms('opencv_safe_training'))\n",
    "        trained_model = embeddings_processor.train()\n",
    "        embeddings_processor.save_model()\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"âœ“ Training completed successfully!\")\n",
    "            print(f\"âœ“ Model saved to: {model_save_path}\")\n",
    "        \n",
    "        # Verify the model file was created\n",
    "        if os.path.exists(model_save_path):\n",
    "            file_size = os.path.getsize(model_save_path) / (1024 * 1024)  # Size in MB\n",
    "            if verbose:\n",
    "                print(f\"âœ“ Model file size: {file_size:.2f} MB\")\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"âš  Warning: Model file was not found after training\")\n",
    "        \n",
    "        training_success = True\n",
    "            \n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"âœ— Training failed with error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        return embeddings_processor, None, False\n",
    "\n",
    "    # Test the trained model by loading it\n",
    "    if verbose:\n",
    "        print(\"\\nTesting model loading...\")\n",
    "    \n",
    "    try:\n",
    "        # Create a new processor instance to test loading\n",
    "        test_processor = EmbeddingsProcessor(\n",
    "            train_dir=temp_data_dir,\n",
    "            model_save_path=model_save_path,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Setup the model (this will load the saved weights)\n",
    "        test_processor.setup_model(SiameseNet, inference_only=True)\n",
    "        if verbose:\n",
    "            print(\"âœ“ Model loaded successfully for inference!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"âœ— Model loading test failed: {e}\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nTraining summary:\")\n",
    "        print(f\"- Dataset: {len(dataset)} augmented samples\")\n",
    "        print(f\"- Model: SiameseNet with ResNet18 backbone\")\n",
    "        print(f\"- Starting point: {'Pre-trained ResNet18' if force_pretrained or not os.path.exists(model_save_path) else 'Saved fine-tuned weights'}\")\n",
    "        print(f\"- Device: {device}\")\n",
    "        print(f\"- Saved to: {model_save_path}\")\n",
    "    \n",
    "    return embeddings_processor, test_processor, training_success\n",
    "\n",
    "# Example usage with the current dataset and configuration\n",
    "print(\"=== Training Embeddings Model ===\")\n",
    "\n",
    "# Define paths\n",
    "notebooks_dir = \"/Users/fernandomousinho/Documents/Learning_to_Code/LaxAI/notebooks\"\n",
    "model_save_path = os.path.join(notebooks_dir, \"augmented_embeddings_model.pth\")\n",
    "\n",
    "# Training Options:\n",
    "USE_FORCE_PRETRAINED = False  # Set to True to ignore saved weights and start fresh with pre-trained ResNet18\n",
    "\n",
    "# Train the model using the new function\n",
    "embeddings_processor, test_processor, success = train_embeddings_model(\n",
    "    dataset=augmented_dataset,\n",
    "    temp_data_dir=temp_data_dir,\n",
    "    model_save_path=model_save_path,\n",
    "    device=device,\n",
    "    force_pretrained=USE_FORCE_PRETRAINED,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "if success:\n",
    "    print(\"\\nðŸŽ‰ Training pipeline completed successfully!\")\n",
    "    print(\"The trained model is ready for embedding generation and analysis.\")\n",
    "else:\n",
    "    print(\"\\nâŒ Training pipeline failed. Check the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ae1889",
   "metadata": {},
   "outputs": [],
   "source": [
    "for annotated_crops_dict in list_of_annotated_crops_dict:\n",
    "   create_augmented_dataset(annotated_crops_dict)\n",
    "   processor, test_proc, success = train_embeddings_model(augmented_dataset, temp_data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792d08bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "from config.transforms import get_transforms\n",
    "\n",
    "# Function to generate embeddings from crops\n",
    "def generate_embeddings_from_crops(crops_dict, embeddings_processor, max_crops_per_track=10):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a subset of crops from each track.\n",
    "    \n",
    "    Args:\n",
    "        crops_dict: Dictionary of {track_id: [crops]}\n",
    "        embeddings_processor: Trained EmbeddingsProcessor instance\n",
    "        max_crops_per_track: Maximum number of crops to process per track\n",
    "    \n",
    "    Returns:\n",
    "        embeddings, labels, track_ids\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    all_track_ids = []\n",
    "    \n",
    "    transforms = get_transforms('opencv_safe')\n",
    "    \n",
    "    for track_id, crops in crops_dict.items():\n",
    "        # Limit number of crops per track for visualization\n",
    "        selected_crops = crops[:max_crops_per_track]\n",
    "        \n",
    "        if len(selected_crops) > 0:\n",
    "            # Generate embeddings using the processor\n",
    "            try:\n",
    "                track_embeddings = embeddings_processor.create_embeddings_from_crops(\n",
    "                    crops=selected_crops,\n",
    "                    transform=transforms\n",
    "                )\n",
    "                \n",
    "                # Convert to numpy if needed\n",
    "                if hasattr(track_embeddings, 'cpu'):\n",
    "                    track_embeddings = track_embeddings.cpu().numpy()\n",
    "                \n",
    "                all_embeddings.extend(track_embeddings)\n",
    "                all_labels.extend([f\"Track_{track_id}\"] * len(track_embeddings))\n",
    "                all_track_ids.extend([track_id] * len(track_embeddings))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Failed to generate embeddings for track {track_id}: {e}\")\n",
    "    \n",
    "    return np.array(all_embeddings), all_labels, all_track_ids\n",
    "\n",
    "# Function to visualize embeddings using t-SNE\n",
    "def visualize_embeddings_tsne(embeddings, labels, title=\"t-SNE Visualization of Embeddings\"):\n",
    "    \"\"\"Visualize embeddings using t-SNE dimensionality reduction.\"\"\"\n",
    "    print(f\"Running t-SNE on {len(embeddings)} embeddings...\")\n",
    "    \n",
    "    # Run t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(embeddings)-1))\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Get unique labels and assign colors\n",
    "    unique_labels = list(set(labels))\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(unique_labels)))\n",
    "    \n",
    "    for i, label in enumerate(unique_labels):\n",
    "        mask = [l == label for l in labels]\n",
    "        plt.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1], \n",
    "                   c=[colors[i]], label=label, alpha=0.7, s=50)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"t-SNE Component 1\")\n",
    "    plt.ylabel(\"t-SNE Component 2\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return embeddings_2d\n",
    "\n",
    "# Function to visualize embeddings using PCA\n",
    "def visualize_embeddings_pca(embeddings, labels, title=\"PCA Visualization of Embeddings\"):\n",
    "    \"\"\"Visualize embeddings using PCA dimensionality reduction.\"\"\"\n",
    "    print(f\"Running PCA on {len(embeddings)} embeddings...\")\n",
    "    \n",
    "    # Run PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    embeddings_2d = pca.fit_transform(embeddings)\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Get unique labels and assign colors\n",
    "    unique_labels = list(set(labels))\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(unique_labels)))\n",
    "    \n",
    "    for i, label in enumerate(unique_labels):\n",
    "        mask = [l == label for l in labels]\n",
    "        plt.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1], \n",
    "                   c=[colors[i]], label=label, alpha=0.7, s=50)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)\")\n",
    "    plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Total variance explained: {sum(pca.explained_variance_ratio_):.2%}\")\n",
    "    return embeddings_2d, pca\n",
    "\n",
    "# Function to show sample crops alongside embeddings\n",
    "def visualize_crops_with_embeddings(crops_dict, embeddings_2d, labels, track_ids, max_display=20):\n",
    "    \"\"\"Display sample crops with their 2D embedding coordinates.\"\"\"\n",
    "    \n",
    "    # Select random samples to display\n",
    "    indices = np.random.choice(len(embeddings_2d), min(max_display, len(embeddings_2d)), replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(4, 5, figsize=(15, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "            \n",
    "        track_id = track_ids[idx]\n",
    "        # Find the corresponding crop (approximate - this is a simplified approach)\n",
    "        crops = crops_dict.get(track_id, [])\n",
    "        if crops:\n",
    "            crop_idx = idx % len(crops)  # Simple mapping\n",
    "            crop = crops[crop_idx]\n",
    "            \n",
    "            # Convert BGR to RGB for display\n",
    "            crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            axes[i].imshow(crop_rgb)\n",
    "            axes[i].set_title(f\"{labels[idx]}\\n({embeddings_2d[idx, 0]:.2f}, {embeddings_2d[idx, 1]:.2f})\", \n",
    "                            fontsize=8)\n",
    "            axes[i].axis('off')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(indices), len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(\"Sample Crops with 2D Embedding Coordinates\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate embeddings for visualization\n",
    "print(\"Generating embeddings for visualization...\")\n",
    "\n",
    "# First, set up the trained model\n",
    "test_processor = EmbeddingsProcessor(\n",
    "    train_dir=temp_data_dir,\n",
    "    model_save_path=model_save_path,\n",
    "    device=device\n",
    ")\n",
    "test_processor.setup_model(SiameseNet, inference_only=True)\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings, labels, track_ids = generate_embeddings_from_crops(\n",
    "    augmented_crops_dict, \n",
    "    test_processor, \n",
    "    max_crops_per_track=5  # Limit for faster visualization\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(embeddings)} embeddings from {len(set(track_ids))} tracks\")\n",
    "\n",
    "# Visualize using t-SNE\n",
    "if len(embeddings) > 1:\n",
    "    tsne_coords = visualize_embeddings_tsne(embeddings, labels, \"t-SNE: Track Embeddings\")\n",
    "    \n",
    "    # Visualize using PCA\n",
    "    pca_coords, pca_model = visualize_embeddings_pca(embeddings, labels, \"PCA: Track Embeddings\")\n",
    "    \n",
    "    # Show sample crops with coordinates\n",
    "    visualize_crops_with_embeddings(augmented_crops_dict, tsne_coords, labels, track_ids)\n",
    "    \n",
    "else:\n",
    "    print(\"Not enough embeddings generated for visualization\")\n",
    "\n",
    "print(\"Embedding visualization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc93ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from typing import List, Tuple\n",
    "from config.transforms import get_transforms\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer_name='backbone.layer4'):\n",
    "        self.model = model\n",
    "        self.target_layer_name = target_layer_name\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        \n",
    "        # Register hooks\n",
    "        self.register_hooks()\n",
    "        \n",
    "    def register_hooks(self):\n",
    "        \"\"\"Register forward and backward hooks on the target layer.\"\"\"\n",
    "        # Get the target layer\n",
    "        target_layer = self.get_target_layer()\n",
    "        \n",
    "        # Register forward hook to capture activations\n",
    "        target_layer.register_forward_hook(self.forward_hook)\n",
    "        \n",
    "        # Register backward hook to capture gradients\n",
    "        target_layer.register_backward_hook(self.backward_hook)\n",
    "    \n",
    "    def get_target_layer(self):\n",
    "        \"\"\"Get the target layer from the model.\"\"\"\n",
    "        layer_names = self.target_layer_name.split('.')\n",
    "        layer = self.model\n",
    "        for name in layer_names:\n",
    "            layer = getattr(layer, name)\n",
    "        return layer\n",
    "    \n",
    "    def forward_hook(self, module, input, output):\n",
    "        \"\"\"Hook to capture forward activations.\"\"\"\n",
    "        self.activations = output\n",
    "    \n",
    "    def backward_hook(self, module, grad_input, grad_output):\n",
    "        \"\"\"Hook to capture gradients.\"\"\"\n",
    "        self.gradients = grad_output[0]\n",
    "    \n",
    "    def generate_cam(self, input_tensor, class_idx=None):\n",
    "        \"\"\"Generate CAM heatmap for given input.\"\"\"\n",
    "        # Forward pass\n",
    "        self.model.eval()\n",
    "        output = self.model(input_tensor)\n",
    "        \n",
    "        # If class_idx is None, use the predicted class\n",
    "        if class_idx is None:\n",
    "            class_idx = output.argmax(dim=1)\n",
    "        \n",
    "        # Zero gradients\n",
    "        self.model.zero_grad()\n",
    "        \n",
    "        # Backward pass\n",
    "        output[0, class_idx].backward()\n",
    "        \n",
    "        # Get gradients and activations\n",
    "        gradients = self.gradients[0]  # Shape: [C, H, W]\n",
    "        activations = self.activations[0]  # Shape: [C, H, W]\n",
    "        \n",
    "        # Global average pooling of gradients\n",
    "        weights = torch.mean(gradients, dim=(1, 2))  # Shape: [C]\n",
    "        \n",
    "        # Weighted combination of activation maps\n",
    "        cam = torch.zeros(activations.shape[1:], dtype=torch.float32)  # Shape: [H, W]\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * activations[i]\n",
    "        \n",
    "        # Apply ReLU and normalize\n",
    "        cam = F.relu(cam)\n",
    "        cam = cam / torch.max(cam) if torch.max(cam) > 0 else cam\n",
    "        \n",
    "        return cam.detach().cpu().numpy()\n",
    "\n",
    "def apply_gradcam_to_crops(model, crops, max_crops=5, target_layer='backbone.layer4'):\n",
    "    \"\"\"Apply GradCAM to a list of crops and return heatmaps.\"\"\"\n",
    "    if len(crops) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Initialize GradCAM\n",
    "    gradcam = GradCAM(model, target_layer)\n",
    "    \n",
    "    # Get transforms for inference\n",
    "    transforms = get_transforms('opencv_safe')\n",
    "    \n",
    "    heatmaps = []\n",
    "    processed_crops = []\n",
    "    \n",
    "    # Process up to max_crops\n",
    "    for i, crop in enumerate(crops[:max_crops]):\n",
    "        if crop is None or crop.size == 0:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Convert BGR to RGB\n",
    "            crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Apply transforms\n",
    "            input_tensor = transforms(crop_rgb).unsqueeze(0)  # Add batch dimension\n",
    "            \n",
    "            # Generate CAM\n",
    "            cam = gradcam.generate_cam(input_tensor)\n",
    "            \n",
    "            heatmaps.append(cam)\n",
    "            processed_crops.append(crop_rgb)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing crop {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return heatmaps, processed_crops\n",
    "\n",
    "def visualize_gradcam_results(crops, heatmaps, track_id, max_display=5):\n",
    "    \"\"\"Visualize original crops with GradCAM heatmaps.\"\"\"\n",
    "    if len(crops) == 0 or len(heatmaps) == 0:\n",
    "        print(f\"No valid crops/heatmaps for track {track_id}\")\n",
    "        return\n",
    "    \n",
    "    num_crops = min(len(crops), len(heatmaps), max_display)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, num_crops, figsize=(3*num_crops, 9))\n",
    "    if num_crops == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    for i in range(num_crops):\n",
    "        crop = crops[i]\n",
    "        heatmap = heatmaps[i]\n",
    "        \n",
    "        # Original crop\n",
    "        axes[0, i].imshow(crop)\n",
    "        axes[0, i].set_title(f\"Original {i+1}\")\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Heatmap\n",
    "        axes[1, i].imshow(heatmap, cmap='jet')\n",
    "        axes[1, i].set_title(f\"GradCAM {i+1}\")\n",
    "        axes[1, i].axis('off')\n",
    "        \n",
    "        # Overlay\n",
    "        # Resize heatmap to match crop size\n",
    "        heatmap_resized = cv2.resize(heatmap, (crop.shape[1], crop.shape[0]))\n",
    "        \n",
    "        # Normalize heatmap for overlay\n",
    "        heatmap_normalized = (heatmap_resized * 255).astype(np.uint8)\n",
    "        heatmap_colored = cv2.applyColorMap(heatmap_normalized, cv2.COLORMAP_JET)\n",
    "        heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Create overlay\n",
    "        overlay = cv2.addWeighted(crop, 0.6, heatmap_colored, 0.4, 0)\n",
    "        \n",
    "        axes[2, i].imshow(overlay)\n",
    "        axes[2, i].set_title(f\"Overlay {i+1}\")\n",
    "        axes[2, i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"GradCAM Analysis - Track {track_id}\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def generate_gradcam_for_all_tracks(augmented_crops_dict, model, max_crops_per_track=3):\n",
    "    \"\"\"Generate GradCAM visualizations for all tracks.\"\"\"\n",
    "    print(\"Generating GradCAM visualizations for all tracks...\")\n",
    "    \n",
    "    for track_id, crops in augmented_crops_dict.items():\n",
    "        print(f\"\\nProcessing Track {track_id}...\")\n",
    "        \n",
    "        # Apply GradCAM\n",
    "        heatmaps, processed_crops = apply_gradcam_to_crops(\n",
    "            model, crops, max_crops=max_crops_per_track\n",
    "        )\n",
    "        \n",
    "        if len(heatmaps) > 0:\n",
    "            # Visualize results\n",
    "            visualize_gradcam_results(processed_crops, heatmaps, track_id, max_display=max_crops_per_track)\n",
    "        else:\n",
    "            print(f\"No valid heatmaps generated for Track {track_id}\")\n",
    "\n",
    "# Apply GradCAM to our trained model and augmented crops\n",
    "print(\"Setting up GradCAM analysis...\")\n",
    "\n",
    "# Make sure the model is in evaluation mode\n",
    "test_processor.model.eval()\n",
    "\n",
    "# Generate GradCAM for all tracks\n",
    "generate_gradcam_for_all_tracks(augmented_crops_dict, test_processor.model, max_crops_per_track=3)\n",
    "\n",
    "print(\"GradCAM analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452688e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
