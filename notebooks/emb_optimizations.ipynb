{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebeb07f3",
   "metadata": {},
   "source": [
    "# Incremental Training Setup\n",
    "\n",
    "This notebook now supports **incremental training** - the model will automatically load existing weights from `augmented_embeddings_model.pth` if it exists, and continue training from there.\n",
    "\n",
    "## Training Behavior:\n",
    "- **First run**: Starts with random weights (fresh model)\n",
    "- **Subsequent runs**: Loads existing weights and continues training\n",
    "- **Benefits**: Accumulative learning, faster convergence, builds on previous training\n",
    "\n",
    "## Control Options:\n",
    "- To **start fresh**: Delete or rename the existing model file before training\n",
    "- To **continue**: Just run the training cell normally\n",
    "- To **backup**: Copy the model file before training to preserve previous versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d50919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Management Utilities\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "def check_model_status(model_path):\n",
    "    \"\"\"Check if model file exists and display its info.\"\"\"\n",
    "    if os.path.exists(model_path):\n",
    "        file_size = os.path.getsize(model_path) / (1024 * 1024)  # MB\n",
    "        mod_time = datetime.fromtimestamp(os.path.getmtime(model_path))\n",
    "        print(f\"✓ Model exists: {model_path}\")\n",
    "        print(f\"  Size: {file_size:.2f} MB\")\n",
    "        print(f\"  Last modified: {mod_time}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"✗ No model found: {model_path}\")\n",
    "        return False\n",
    "\n",
    "def backup_model(model_path, backup_suffix=None):\n",
    "    \"\"\"Create a backup of the current model.\"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        print(\"No model to backup\")\n",
    "        return None\n",
    "    \n",
    "    if backup_suffix is None:\n",
    "        backup_suffix = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    backup_path = model_path.replace('.pth', f'_backup_{backup_suffix}.pth')\n",
    "    shutil.copy2(model_path, backup_path)\n",
    "    print(f\"✓ Model backed up to: {backup_path}\")\n",
    "    return backup_path\n",
    "\n",
    "def reset_model(model_path):\n",
    "    \"\"\"Delete the existing model to start fresh training.\"\"\"\n",
    "    if os.path.exists(model_path):\n",
    "        os.remove(model_path)\n",
    "        print(f\"✓ Model deleted: {model_path}\")\n",
    "        print(\"Next training will start with pre-trained ResNet18 weights\")\n",
    "    else:\n",
    "        print(\"No model to delete\")\n",
    "\n",
    "def force_pretrained_training():\n",
    "    \"\"\"\n",
    "    Helper function to train with fresh pre-trained ResNet18 weights.\n",
    "    Use this when you want to ignore saved weights and start from pre-trained backbone.\n",
    "    \"\"\"\n",
    "    print(\"🔄 Setting up training with fresh pre-trained ResNet18 weights...\")\n",
    "    \n",
    "    # Create EmbeddingsProcessor with force_pretrained option\n",
    "    fresh_processor = EmbeddingsProcessor(\n",
    "        train_dir=temp_data_dir,\n",
    "        model_save_path=model_save_path,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Setup model with force_pretrained=True\n",
    "    fresh_processor.setup_model(SiameseNet, force_pretrained=True)\n",
    "    \n",
    "    return fresh_processor\n",
    "\n",
    "# Check current model status\n",
    "notebooks_dir = \"/Users/fernandomousinho/Documents/Learning_to_Code/LaxAI/notebooks\"\n",
    "model_save_path = os.path.join(notebooks_dir, \"augmented_embeddings_model.pth\")\n",
    "\n",
    "print(\"Current model status:\")\n",
    "check_model_status(model_save_path)\n",
    "\n",
    "print(\"\\n📚 Training Options:\")\n",
    "print(\"1. Normal training: Loads saved weights if available, otherwise uses pre-trained ResNet18\")\n",
    "print(\"2. Fresh pre-trained: force_pretrained_training() - Ignores saved weights, starts with pre-trained ResNet18\")\n",
    "print(\"3. Complete reset: reset_model() then train - Deletes saved model, starts with pre-trained ResNet18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15694d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the parent directory to the path to import modules\n",
    "sys.path.append('/Users/fernandomousinho/Documents/Learning_to_Code/LaxAI')\n",
    "\n",
    "from modules.det_processor import DetectionProcessor\n",
    "from modules.detection import DetectionModel\n",
    "from modules.tracker import AffineAwareByteTrack\n",
    "from tools.store_driver import Store\n",
    "import torch\n",
    "\n",
    "\n",
    "# Initialize required components\n",
    "store = Store()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "detection_model = DetectionModel(store=store, device=device)\n",
    "tracker = AffineAwareByteTrack()\n",
    "\n",
    "# Create detection processor\n",
    "detection_processor = DetectionProcessor(\n",
    "    model=detection_model,\n",
    "    tracker=tracker,\n",
    "    detection_file_path=\"\"  # Not needed for loading\n",
    ")\n",
    "\n",
    "# Load multi_frame_detections from tracks.json\n",
    "tracks_json_path = \"/Users/fernandomousinho/Documents/Learning_to_Code/LaxAI/notebooks/tracks.json\"\n",
    "video_source = \"/Users/fernandomousinho/Library/CloudStorage/GoogleDrive-fmousinho76@gmail.com/My Drive/Colab_Notebooks/FCA_Upstate_NY_003.mp4\"\n",
    "\n",
    "multi_frame_detections = detection_processor.json_to_detections(\n",
    "    json_file_path=tracks_json_path,\n",
    "    update_tracker_state=True,\n",
    "    video_source=video_source\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad0a700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "tracks_data = tracker.get_tracks_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d471050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.augmentation import augment_images, test_augmentation\n",
    "\n",
    "# Test the augmentation function\n",
    "print(\"Testing augmentation function from modules.augmentation...\")\n",
    "test_results = test_augmentation()\n",
    "print(f\"✓ Augmentation module imported and tested successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb474f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "frame_id_sample = [50, 150, 250, 350, 450, 550, 650, 750, 850, 950, 1050]\n",
    "track_ids_for_frame = {}\n",
    "\n",
    "# Initialize the dictionary with empty lists for each frame\n",
    "for frame_id in frame_id_sample:\n",
    "    track_ids_for_frame[frame_id] = []\n",
    "\n",
    "\n",
    "for detections in multi_frame_detections:\n",
    "    frame_id = detections.metadata['frame_id']\n",
    "    if frame_id in frame_id_sample:\n",
    "        # Handle class_id using numpy operations\n",
    "        mask = detections.class_id == 3\n",
    "        track_ids_for_frame[frame_id] = detections.tracker_id[mask]\n",
    "       \n",
    "for frame_id in frame_id_sample:\n",
    "    print(f\"Frame {frame_id}: Track IDs {track_ids_for_frame[frame_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d926ac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "from modules.dataset import LacrossePlayerDataset\n",
    "from config.transforms import get_transforms, initialize_background_removal, refresh_transform_instances\n",
    "from config.all_config import transform_config\n",
    "video_info = sv.VideoInfo.from_video_path(video_source)\n",
    "transform_config.enable_background_removal = True\n",
    "stride = video_info.total_frames // transform_config.background_detector_sample_frames\n",
    "video_info = sv.VideoInfo.from_video_path(video_path=video_source)\n",
    "generator_params = {\n",
    "    \"source_path\": video_source,\n",
    "    \"end\": 1200\n",
    "}\n",
    "grass_crop_generator = sv.get_video_frames_generator(**generator_params, stride=stride )        \n",
    "sample_images = [frame for frame in grass_crop_generator]\n",
    "initialize_background_removal(sample_images)\n",
    "refresh_transform_instances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a8d7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_of_annotated_crops_dict = []\n",
    "for frame_id in frame_id_sample:\n",
    "    track_ids_to_process = track_ids_for_frame[frame_id]\n",
    "    track_data_to_process = [data for data in tracks_data.values() if data.track_id in track_ids_to_process]\n",
    "    augmented_crops_dict = {}\n",
    "    for data in track_data_to_process:\n",
    "        original_crops = data.crops[:5]\n",
    "        augmented_crops = augment_images(original_crops)\n",
    "        augmented_crops_dict[data.track_id] = augmented_crops\n",
    "    list_of_annotated_crops_dict.append(augmented_crops_dict.copy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d612b4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.dataset import LacrossePlayerDataset\n",
    "from config.transforms import get_transforms\n",
    "import tempfile\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "def create_augmented_dataset(\n",
    "    augmented_crops_dict,\n",
    "    transforms_config='opencv_safe_training',\n",
    "    temp_dir_prefix=\"augmented_data\",\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a LacrossePlayerDataset from augmented crops dictionary.\n",
    "    \n",
    "    Args:\n",
    "        augmented_crops_dict: Dictionary of {track_id: [augmented_crops]}\n",
    "        transforms_config: Transform configuration to use (default: 'opencv_safe_training')\n",
    "        temp_dir_prefix: Prefix for the temporary directory name\n",
    "        verbose: If True, print detailed information\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (dataset, temp_data_dir, total_images_saved)\n",
    "            - dataset: The created LacrossePlayerDataset\n",
    "            - temp_data_dir: Path to the temporary data directory\n",
    "            - total_images_saved: Number of images successfully saved\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a temporary directory structure for the dataset\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    data_dir = os.path.join(temp_dir, temp_dir_prefix)\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Creating dataset in temporary directory: {data_dir}\")\n",
    "\n",
    "    # Create subdirectories for each track (treating each track as a class)\n",
    "    track_dirs = {}\n",
    "    for track_id in augmented_crops_dict.keys():\n",
    "        track_dir = os.path.join(data_dir, f\"track_{track_id}\")\n",
    "        os.makedirs(track_dir, exist_ok=True)\n",
    "        track_dirs[track_id] = track_dir\n",
    "\n",
    "    # Save augmented crops as image files\n",
    "    total_images_saved = 0\n",
    "    failed_saves = 0\n",
    "\n",
    "    for track_id, augmented_crops in augmented_crops_dict.items():\n",
    "        track_dir = track_dirs[track_id]\n",
    "        \n",
    "        for idx, crop in enumerate(augmented_crops):\n",
    "            if crop is not None and crop.size > 0:\n",
    "                # Save as PNG file\n",
    "                image_path = os.path.join(track_dir, f\"crop_{idx:04d}.png\")\n",
    "                success = cv2.imwrite(image_path, crop)\n",
    "                if success:\n",
    "                    total_images_saved += 1\n",
    "                else:\n",
    "                    failed_saves += 1\n",
    "                    if verbose:\n",
    "                        print(f\"Failed to save image: {image_path}\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Saved {total_images_saved} augmented images across {len(track_dirs)} track directories\")\n",
    "        if failed_saves > 0:\n",
    "            print(f\"Warning: {failed_saves} images failed to save\")\n",
    "\n",
    "    # Create the LacrossePlayerDataset\n",
    "    transforms = get_transforms(transforms_config)\n",
    "\n",
    "    # Create dataset\n",
    "    augmented_dataset = LacrossePlayerDataset(\n",
    "        data_dir,\n",
    "        transform=transforms\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Created LacrossePlayerDataset with {len(augmented_dataset)} samples\")\n",
    "        print(f\"Temporary data directory: {data_dir}\")\n",
    "        print(\"Note: This directory can be cleaned up later with: import shutil; shutil.rmtree(temp_data_dir)\")\n",
    "\n",
    "    return augmented_dataset, data_dir, total_images_saved\n",
    "\n",
    "def cleanup_temp_directory(temp_data_dir, verbose=True):\n",
    "    \"\"\"\n",
    "    Clean up the temporary directory created for the dataset.\n",
    "    \n",
    "    Args:\n",
    "        temp_data_dir: Path to the temporary directory to clean up\n",
    "        verbose: If True, print cleanup information\n",
    "    \"\"\"\n",
    "    import shutil\n",
    "    \n",
    "    if os.path.exists(temp_data_dir):\n",
    "        try:\n",
    "            shutil.rmtree(temp_data_dir)\n",
    "            if verbose:\n",
    "                print(f\"✓ Cleaned up temporary directory: {temp_data_dir}\")\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"✗ Failed to clean up directory {temp_data_dir}: {e}\")\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(f\"Directory does not exist: {temp_data_dir}\")\n",
    "\n",
    "# # Example usage with the current augmented crops\n",
    "# print(\"=== Creating Augmented Dataset ===\")\n",
    "\n",
    "# # Create the dataset using the new function\n",
    "# augmented_dataset, temp_data_dir, images_saved = create_augmented_dataset(\n",
    "#     augmented_crops_dict=augmented_crops_dict,\n",
    "#     transforms_config='opencv_safe_training',\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# print(f\"\\n✓ Dataset creation completed!\")\n",
    "# print(f\"  - Dataset samples: {len(augmented_dataset)}\")\n",
    "# print(f\"  - Images saved: {images_saved}\")\n",
    "# print(f\"  - Temporary directory: {temp_data_dir}\")\n",
    "\n",
    "# The temp_data_dir variable is now available for use in subsequent cells\n",
    "# You can clean it up later using: cleanup_temp_directory(temp_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41664404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.emb_processor import EmbeddingsProcessor\n",
    "from modules.siamesenet import SiameseNet\n",
    "from modules.dataset import LacrossePlayerDataset\n",
    "from config.transforms import get_transforms\n",
    "import os\n",
    "\n",
    "def train_embeddings_model(\n",
    "    dataset,\n",
    "    temp_data_dir,\n",
    "    model_save_path=None,\n",
    "    device=None,\n",
    "    force_pretrained=False,\n",
    "    notebooks_dir=\"/Users/fernandomousinho/Documents/Learning_to_Code/LaxAI/notebooks\",\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a SiameseNet embeddings model with the given dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The dataset to train on (e.g., augmented_dataset)\n",
    "        temp_data_dir: Temporary directory containing the training data\n",
    "        model_save_path: Path to save the trained model (optional)\n",
    "        device: Training device (cuda/cpu) - will auto-detect if None\n",
    "        force_pretrained: If True, ignore saved weights and start with pre-trained ResNet18\n",
    "        notebooks_dir: Directory where notebooks are stored\n",
    "        verbose: If True, print detailed training information\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (embeddings_processor, test_processor, training_success)\n",
    "            - embeddings_processor: The trained processor\n",
    "            - test_processor: A processor instance for inference testing\n",
    "            - training_success: Boolean indicating if training succeeded\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set default values\n",
    "    if device is None:\n",
    "        import torch\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    if model_save_path is None:\n",
    "        model_save_path = os.path.join(notebooks_dir, \"augmented_embeddings_model.pth\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Training model with augmented dataset...\")\n",
    "        print(f\"Dataset samples: {len(dataset)}\")\n",
    "        print(f\"Model will be saved to: {model_save_path}\")\n",
    "        print(f\"Device: {device}\")\n",
    "        print(f\"Force pretrained: {force_pretrained}\")\n",
    "\n",
    "    # Create EmbeddingsProcessor for training\n",
    "    embeddings_processor = EmbeddingsProcessor(\n",
    "        train_dir=temp_data_dir,\n",
    "        model_save_path=model_save_path,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Setup model (will auto-load saved weights if available, unless force_pretrained=True)\n",
    "    if force_pretrained:\n",
    "        if verbose:\n",
    "            print(\"🔄 Forcing fresh start with pre-trained ResNet18 weights...\")\n",
    "        embeddings_processor.setup_model(SiameseNet, force_pretrained=True)\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"📈 Using incremental training (loads saved weights if available)...\")\n",
    "        embeddings_processor.setup_model(SiameseNet)\n",
    "\n",
    "    # Train the model\n",
    "    training_success = False\n",
    "    test_processor = None\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Starting training...\")\n",
    "    \n",
    "    try:\n",
    "        # Setup data and train\n",
    "        embeddings_processor.setup_data(LacrossePlayerDataset, get_transforms('opencv_safe_training'))\n",
    "        trained_model = embeddings_processor.train()\n",
    "        embeddings_processor.save_model()\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"✓ Training completed successfully!\")\n",
    "            print(f\"✓ Model saved to: {model_save_path}\")\n",
    "        \n",
    "        # Verify the model file was created\n",
    "        if os.path.exists(model_save_path):\n",
    "            file_size = os.path.getsize(model_save_path) / (1024 * 1024)  # Size in MB\n",
    "            if verbose:\n",
    "                print(f\"✓ Model file size: {file_size:.2f} MB\")\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"⚠ Warning: Model file was not found after training\")\n",
    "        \n",
    "        training_success = True\n",
    "            \n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"✗ Training failed with error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        return embeddings_processor, None, False\n",
    "\n",
    "    # Test the trained model by loading it\n",
    "    if verbose:\n",
    "        print(\"\\nTesting model loading...\")\n",
    "    \n",
    "    try:\n",
    "        # Create a new processor instance to test loading\n",
    "        test_processor = EmbeddingsProcessor(\n",
    "            train_dir=temp_data_dir,\n",
    "            model_save_path=model_save_path,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Setup the model (this will load the saved weights)\n",
    "        test_processor.setup_model(SiameseNet, inference_only=True)\n",
    "        if verbose:\n",
    "            print(\"✓ Model loaded successfully for inference!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"✗ Model loading test failed: {e}\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nTraining summary:\")\n",
    "        print(f\"- Dataset: {len(dataset)} augmented samples\")\n",
    "        print(f\"- Model: SiameseNet with ResNet18 backbone\")\n",
    "        print(f\"- Starting point: {'Pre-trained ResNet18' if force_pretrained or not os.path.exists(model_save_path) else 'Saved fine-tuned weights'}\")\n",
    "        print(f\"- Device: {device}\")\n",
    "        print(f\"- Saved to: {model_save_path}\")\n",
    "    \n",
    "    return embeddings_processor, test_processor, training_success\n",
    "\n",
    "# Example usage with the current dataset and configuration\n",
    "# print(\"=== Training Embeddings Model ===\")\n",
    "\n",
    "# Define paths\n",
    "notebooks_dir = \"/Users/fernandomousinho/Documents/Learning_to_Code/LaxAI/notebooks\"\n",
    "model_save_path = os.path.join(notebooks_dir, \"augmented_embeddings_model.pth\")\n",
    "\n",
    "# Training Options:\n",
    "USE_FORCE_PRETRAINED = False  # Set to True to ignore saved weights and start fresh with pre-trained ResNet18\n",
    "\n",
    "# Train the model using the new function\n",
    "# embeddings_processor, test_processor, success = train_embeddings_model(\n",
    "#     dataset=augmented_dataset,\n",
    "#     temp_data_dir=temp_data_dir,\n",
    "#     model_save_path=model_save_path,\n",
    "#     device=device,\n",
    "#     force_pretrained=USE_FORCE_PRETRAINED,\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# if success:\n",
    "#     print(\"\\n🎉 Training pipeline completed successfully!\")\n",
    "#     print(\"The trained model is ready for embedding generation and analysis.\")\n",
    "# else:\n",
    "#     print(\"\\n❌ Training pipeline failed. Check the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ae1889",
   "metadata": {},
   "outputs": [],
   "source": [
    "turn = 0\n",
    "for annotated_crops_dict in list_of_annotated_crops_dict:\n",
    "   print(\"=== Processing set {} ===\".format(turn))\n",
    "   augmented_dataset, temp_data_dir, _ = create_augmented_dataset(annotated_crops_dict)\n",
    "   processor, test_proc, success = train_embeddings_model(augmented_dataset, temp_data_dir)\n",
    "   turn += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792d08bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "from config.transforms import get_transforms\n",
    "\n",
    "# Function to generate embeddings from crops\n",
    "def generate_embeddings_from_crops(crops_dict, embeddings_processor, max_crops_per_track=10):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a subset of crops from each track.\n",
    "    \n",
    "    Args:\n",
    "        crops_dict: Dictionary of {track_id: [crops]}\n",
    "        embeddings_processor: Trained EmbeddingsProcessor instance\n",
    "        max_crops_per_track: Maximum number of crops to process per track\n",
    "    \n",
    "    Returns:\n",
    "        embeddings, labels, track_ids\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    all_track_ids = []\n",
    "    \n",
    "    transforms = get_transforms('opencv_safe')\n",
    "    \n",
    "    for track_id, crops in crops_dict.items():\n",
    "        # Limit number of crops per track for visualization\n",
    "        selected_crops = crops[:max_crops_per_track]\n",
    "        \n",
    "        if len(selected_crops) > 0:\n",
    "            # Generate embeddings using the processor\n",
    "            try:\n",
    "                track_embeddings = embeddings_processor.create_embeddings_from_crops(\n",
    "                    crops=selected_crops,\n",
    "                    transform=transforms\n",
    "                )\n",
    "                \n",
    "                # Convert to numpy if needed\n",
    "                if hasattr(track_embeddings, 'cpu'):\n",
    "                    track_embeddings = track_embeddings.cpu().numpy()\n",
    "                \n",
    "                all_embeddings.extend(track_embeddings)\n",
    "                all_labels.extend([f\"Track_{track_id}\"] * len(track_embeddings))\n",
    "                all_track_ids.extend([track_id] * len(track_embeddings))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Failed to generate embeddings for track {track_id}: {e}\")\n",
    "    \n",
    "    return np.array(all_embeddings), all_labels, all_track_ids\n",
    "\n",
    "# Function to visualize embeddings using t-SNE\n",
    "def visualize_embeddings_tsne(embeddings, labels, title=\"t-SNE Visualization of Embeddings\"):\n",
    "    \"\"\"Visualize embeddings using t-SNE dimensionality reduction.\"\"\"\n",
    "    print(f\"Running t-SNE on {len(embeddings)} embeddings...\")\n",
    "    \n",
    "    # Run t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(embeddings)-1))\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Get unique labels and assign colors\n",
    "    unique_labels = list(set(labels))\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(unique_labels)))\n",
    "    \n",
    "    for i, label in enumerate(unique_labels):\n",
    "        mask = [l == label for l in labels]\n",
    "        plt.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1], \n",
    "                   c=[colors[i]], label=label, alpha=0.7, s=50)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"t-SNE Component 1\")\n",
    "    plt.ylabel(\"t-SNE Component 2\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return embeddings_2d\n",
    "\n",
    "# Function to visualize embeddings using PCA\n",
    "def visualize_embeddings_pca(embeddings, labels, title=\"PCA Visualization of Embeddings\"):\n",
    "    \"\"\"Visualize embeddings using PCA dimensionality reduction.\"\"\"\n",
    "    print(f\"Running PCA on {len(embeddings)} embeddings...\")\n",
    "    \n",
    "    # Run PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    embeddings_2d = pca.fit_transform(embeddings)\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Get unique labels and assign colors\n",
    "    unique_labels = list(set(labels))\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(unique_labels)))\n",
    "    \n",
    "    for i, label in enumerate(unique_labels):\n",
    "        mask = [l == label for l in labels]\n",
    "        plt.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1], \n",
    "                   c=[colors[i]], label=label, alpha=0.7, s=50)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)\")\n",
    "    plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Total variance explained: {sum(pca.explained_variance_ratio_):.2%}\")\n",
    "    return embeddings_2d, pca\n",
    "\n",
    "# Function to show sample crops alongside embeddings\n",
    "def visualize_crops_with_embeddings(crops_dict, embeddings_2d, labels, track_ids, max_display=20):\n",
    "    \"\"\"Display sample crops with their 2D embedding coordinates.\"\"\"\n",
    "    \n",
    "    # Select random samples to display\n",
    "    indices = np.random.choice(len(embeddings_2d), min(max_display, len(embeddings_2d)), replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(4, 5, figsize=(15, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "            \n",
    "        track_id = track_ids[idx]\n",
    "        # Find the corresponding crop (approximate - this is a simplified approach)\n",
    "        crops = crops_dict.get(track_id, [])\n",
    "        if crops:\n",
    "            crop_idx = idx % len(crops)  # Simple mapping\n",
    "            crop = crops[crop_idx]\n",
    "            \n",
    "            # Convert BGR to RGB for display\n",
    "            crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            axes[i].imshow(crop_rgb)\n",
    "            axes[i].set_title(f\"{labels[idx]}\\n({embeddings_2d[idx, 0]:.2f}, {embeddings_2d[idx, 1]:.2f})\", \n",
    "                            fontsize=8)\n",
    "            axes[i].axis('off')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(indices), len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(\"Sample Crops with 2D Embedding Coordinates\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate embeddings for visualization\n",
    "print(\"Generating embeddings for visualization...\")\n",
    "\n",
    "# First, set up the trained model\n",
    "test_processor = EmbeddingsProcessor(\n",
    "    train_dir=temp_data_dir,\n",
    "    model_save_path=model_save_path,\n",
    "    device=device\n",
    ")\n",
    "test_processor.setup_model(SiameseNet, inference_only=True)\n",
    "\n",
    "complete_embeddings = {}\n",
    "for _ , augmented_crops_dict in enumerate(list_of_annotated_crops_dict):\n",
    "    complete_embeddings.update(augmented_crops_dict)\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings, labels, track_ids = generate_embeddings_from_crops(\n",
    "    complete_embeddings, \n",
    "    test_processor, \n",
    "    max_crops_per_track=5  # Limit for faster visualization\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(embeddings)} embeddings from {len(set(track_ids))} tracks\")\n",
    "\n",
    "# Visualize using t-SNE\n",
    "if len(embeddings) > 1:\n",
    "    tsne_coords = visualize_embeddings_tsne(embeddings, labels, \"t-SNE: Track Embeddings\")\n",
    "    \n",
    "    # Visualize using PCA\n",
    "    pca_coords, pca_model = visualize_embeddings_pca(embeddings, labels, \"PCA: Track Embeddings\")\n",
    "    \n",
    "    # Show sample crops with coordinates\n",
    "    visualize_crops_with_embeddings(augmented_crops_dict, tsne_coords, labels, track_ids)\n",
    "    \n",
    "else:\n",
    "    print(\"Not enough embeddings generated for visualization\")\n",
    "\n",
    "print(\"Embedding visualization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc93ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from typing import List, Tuple\n",
    "from config.transforms import get_transforms\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer_name='backbone.layer4'):\n",
    "        self.model = model\n",
    "        self.target_layer_name = target_layer_name\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        \n",
    "        # Register hooks\n",
    "        self.register_hooks()\n",
    "        \n",
    "    def register_hooks(self):\n",
    "        \"\"\"Register forward and backward hooks on the target layer.\"\"\"\n",
    "        # Get the target layer\n",
    "        target_layer = self.get_target_layer()\n",
    "        \n",
    "        # Register forward hook to capture activations\n",
    "        target_layer.register_forward_hook(self.forward_hook)\n",
    "        \n",
    "        # Register backward hook to capture gradients\n",
    "        target_layer.register_backward_hook(self.backward_hook)\n",
    "    \n",
    "    def get_target_layer(self):\n",
    "        \"\"\"Get the target layer from the model.\"\"\"\n",
    "        layer_names = self.target_layer_name.split('.')\n",
    "        layer = self.model\n",
    "        for name in layer_names:\n",
    "            layer = getattr(layer, name)\n",
    "        return layer\n",
    "    \n",
    "    def forward_hook(self, module, input, output):\n",
    "        \"\"\"Hook to capture forward activations.\"\"\"\n",
    "        self.activations = output\n",
    "    \n",
    "    def backward_hook(self, module, grad_input, grad_output):\n",
    "        \"\"\"Hook to capture gradients.\"\"\"\n",
    "        self.gradients = grad_output[0]\n",
    "    \n",
    "    def generate_cam(self, input_tensor, class_idx=None):\n",
    "        \"\"\"Generate CAM heatmap for given input.\"\"\"\n",
    "        # Forward pass\n",
    "        self.model.eval()\n",
    "        output = self.model(input_tensor)\n",
    "        \n",
    "        # If class_idx is None, use the predicted class\n",
    "        if class_idx is None:\n",
    "            class_idx = output.argmax(dim=1)\n",
    "        \n",
    "        # Zero gradients\n",
    "        self.model.zero_grad()\n",
    "        \n",
    "        # Backward pass\n",
    "        output[0, class_idx].backward()\n",
    "        \n",
    "        # Get gradients and activations\n",
    "        gradients = self.gradients[0]  # Shape: [C, H, W]\n",
    "        activations = self.activations[0]  # Shape: [C, H, W]\n",
    "        \n",
    "        # Global average pooling of gradients\n",
    "        weights = torch.mean(gradients, dim=(1, 2))  # Shape: [C]\n",
    "        \n",
    "        # Weighted combination of activation maps\n",
    "        cam = torch.zeros(activations.shape[1:], dtype=torch.float32)  # Shape: [H, W]\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * activations[i]\n",
    "        \n",
    "        # Apply ReLU and normalize\n",
    "        cam = F.relu(cam)\n",
    "        cam = cam / torch.max(cam) if torch.max(cam) > 0 else cam\n",
    "        \n",
    "        return cam.detach().cpu().numpy()\n",
    "\n",
    "def apply_gradcam_to_crops(model, crops, max_crops=5, target_layer='backbone.layer4'):\n",
    "    \"\"\"Apply GradCAM to a list of crops and return heatmaps.\"\"\"\n",
    "    if len(crops) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Initialize GradCAM\n",
    "    gradcam = GradCAM(model, target_layer)\n",
    "    \n",
    "    # Get transforms for inference\n",
    "    transforms = get_transforms('opencv_safe')\n",
    "    \n",
    "    heatmaps = []\n",
    "    processed_crops = []\n",
    "    \n",
    "    # Process up to max_crops\n",
    "    for i, crop in enumerate(crops[:max_crops]):\n",
    "        if crop is None or crop.size == 0:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Convert BGR to RGB\n",
    "            crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Apply transforms\n",
    "            input_tensor = transforms(crop_rgb).unsqueeze(0)  # Add batch dimension\n",
    "            \n",
    "            # Generate CAM\n",
    "            cam = gradcam.generate_cam(input_tensor)\n",
    "            \n",
    "            heatmaps.append(cam)\n",
    "            processed_crops.append(crop_rgb)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing crop {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return heatmaps, processed_crops\n",
    "\n",
    "def visualize_gradcam_results(crops, heatmaps, track_id, max_display=5):\n",
    "    \"\"\"Visualize original crops with GradCAM heatmaps.\"\"\"\n",
    "    if len(crops) == 0 or len(heatmaps) == 0:\n",
    "        print(f\"No valid crops/heatmaps for track {track_id}\")\n",
    "        return\n",
    "    \n",
    "    num_crops = min(len(crops), len(heatmaps), max_display)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, num_crops, figsize=(3*num_crops, 9))\n",
    "    if num_crops == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    for i in range(num_crops):\n",
    "        crop = crops[i]\n",
    "        heatmap = heatmaps[i]\n",
    "        \n",
    "        # Original crop\n",
    "        axes[0, i].imshow(crop)\n",
    "        axes[0, i].set_title(f\"Original {i+1}\")\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Heatmap\n",
    "        axes[1, i].imshow(heatmap, cmap='jet')\n",
    "        axes[1, i].set_title(f\"GradCAM {i+1}\")\n",
    "        axes[1, i].axis('off')\n",
    "        \n",
    "        # Overlay\n",
    "        # Resize heatmap to match crop size\n",
    "        heatmap_resized = cv2.resize(heatmap, (crop.shape[1], crop.shape[0]))\n",
    "        \n",
    "        # Normalize heatmap for overlay\n",
    "        heatmap_normalized = (heatmap_resized * 255).astype(np.uint8)\n",
    "        heatmap_colored = cv2.applyColorMap(heatmap_normalized, cv2.COLORMAP_JET)\n",
    "        heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Create overlay\n",
    "        overlay = cv2.addWeighted(crop, 0.6, heatmap_colored, 0.4, 0)\n",
    "        \n",
    "        axes[2, i].imshow(overlay)\n",
    "        axes[2, i].set_title(f\"Overlay {i+1}\")\n",
    "        axes[2, i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"GradCAM Analysis - Track {track_id}\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def generate_gradcam_for_all_tracks(augmented_crops_dict, model, max_crops_per_track=3):\n",
    "    \"\"\"Generate GradCAM visualizations for all tracks.\"\"\"\n",
    "    print(\"Generating GradCAM visualizations for all tracks...\")\n",
    "    \n",
    "    for track_id, crops in augmented_crops_dict.items():\n",
    "        print(f\"\\nProcessing Track {track_id}...\")\n",
    "        \n",
    "        # Apply GradCAM\n",
    "        heatmaps, processed_crops = apply_gradcam_to_crops(\n",
    "            model, crops, max_crops=max_crops_per_track\n",
    "        )\n",
    "        \n",
    "        if len(heatmaps) > 0:\n",
    "            # Visualize results\n",
    "            visualize_gradcam_results(processed_crops, heatmaps, track_id, max_display=max_crops_per_track)\n",
    "        else:\n",
    "            print(f\"No valid heatmaps generated for Track {track_id}\")\n",
    "\n",
    "# Apply GradCAM to our trained model and augmented crops\n",
    "print(\"Setting up GradCAM analysis...\")\n",
    "\n",
    "# Make sure the model is in evaluation mode\n",
    "test_processor.model.eval()\n",
    "\n",
    "# Generate GradCAM for all tracks\n",
    "generate_gradcam_for_all_tracks(augmented_crops_dict, test_processor.model, max_crops_per_track=3)\n",
    "\n",
    "print(\"GradCAM analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452688e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.transforms import get_global_background_detector\n",
    "background_detector = get_global_background_detector()\n",
    "if background_detector:\n",
    "            # The detector's get_stats() method provides all relevant info\n",
    "            stats = background_detector.get_stats()\n",
    "\n",
    "            print(\"\\nApplied Configuration:\")\n",
    "            print(f\"  - HSV Lower Bound (The Range): {stats.get('lower_bound')}\")\n",
    "            print(f\"  - HSV Upper Bound (The Range): {stats.get('upper_bound')}\")\n",
    "            print(f\"  - Sensitivity (Std Dev Multiplier): {stats.get('std_dev_multiplier')}\")\n",
    "            print(f\"  - Replacement Color (RGB): {stats.get('replacement_color')}\")\n",
    "            \n",
    "else:\n",
    "    # This state is unlikely if is_background_removal_enabled() is true,\n",
    "    # but it is good practice to handle it.\n",
    "    print(\"⚠️ Warning: Background removal is enabled, but detector is not available.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
